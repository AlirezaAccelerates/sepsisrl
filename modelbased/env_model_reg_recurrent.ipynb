{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import cPickle as pickle\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/rl_train_set_unscaled.csv')\n",
    "df_val = pd.read_csv('../data/rl_val_set_unscaled.csv')\n",
    "df_test = pd.read_csv('../data/rl_test_set_unscaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# let the mortality labels  be -1 and 1: -1 for survival\n",
    "df_train['died_in_hosp'][df_train['died_in_hosp'] == 0] = -1\n",
    "df_val['died_in_hosp'][df_val['died_in_hosp'] == 0] = -1\n",
    "df_test['died_in_hosp'][df_test['died_in_hosp'] == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_feat = list(np.loadtxt('../data/state_features_pred.txt', dtype=str))\n",
    "target_feat.append('died_in_hosp')\n",
    "\n",
    "cur_feat = list(np.loadtxt('../data/state_features.txt', dtype=str))\n",
    "cur_feat.append('iv_input')\n",
    "cur_feat.append('vaso_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define an action mapping - how to get an id representing the action from the (iv,vaso) tuple\n",
    "action_map = {}\n",
    "count = 0\n",
    "for iv in range(5):\n",
    "    for vaso in range(5):\n",
    "        action_map[(iv,vaso)] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_action_map = {}\n",
    "for iv in range(5):\n",
    "    for vaso in range(5):\n",
    "        inv_action_map[5*iv+vaso] = [iv,vaso]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# Change the actions: {0: -0.8, 1: -0.4, 2: 0, 3: 0.4, 4: 0.8}. This will help for the policy search, which\n",
    "# uses a continuous action space\n",
    "lookup ={0:-0.8, 1:-0.4, 2:0, 3:0.4, 4:0.8}\n",
    "for dfm in (df_train, df_val, df_test):\n",
    "    for iv_input in range(5):\n",
    "        val = lookup[iv_input]\n",
    "        dfm['iv_input'][dfm['iv_input'] == iv_input] = val\n",
    "    for vaso_input in range(5):\n",
    "        val = lookup[vaso_input]\n",
    "        dfm['vaso_input'][dfm['vaso_input'] == vaso_input] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the longest trajectory\n",
    "orig_df = pd.read_csv('../data/MKdataset07Feb17.csv')\n",
    "max_time_steps = max(orig_df['bloc'])\n",
    "del orig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(9).reshape([3,3])\n",
    "np.pad(a, [[0, 5-a.shape[0]],[0,0]], 'constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X: [[s1a1, s2a2, ... , sTaT], [...] ] ie, (s,a) tuples for every trajectory in dataset\n",
    "# Y: [[delta1, delta2, ... , deltaT], [...] ] ie, deltas for every \n",
    "# make X of shape: num_traj*max_time_steps*len(cur_feat)\n",
    "# make Y of shape: num_traj*max_time_steps*len(target_feat)\n",
    "def make_data(df_in):\n",
    "    X = []\n",
    "    Y = []\n",
    "    seq_lens = []\n",
    "    traj_ids = df_in['icustayid'].unique()\n",
    "\n",
    "    for traj_id in traj_ids:\n",
    "        rel_df = df_in.loc[df_in['icustayid'] == traj_id]\n",
    "        state = rel_df[cur_feat].values[:-1] # take everything except the final state in the trajectory\n",
    "        \n",
    "        tmp1 = rel_df[target_feat].values\n",
    "        tmp2 = tmp1[1:]\n",
    "        tmp1 = tmp1[:-1]\n",
    "        target = tmp2 - tmp1\n",
    "        # add the mortality labels in\n",
    "        target[:,-1] = rel_df['died_in_hosp'].values[:-1]\n",
    "        \n",
    "        # store the length of the seq for dynamic RNN\n",
    "        seq_lens.append(len(state))\n",
    "        \n",
    "        # do padding if necessary\n",
    "        if len(state) < max_time_steps:\n",
    "            state = np.pad(state, [[0, max_time_steps-state.shape[0]],[0,0]],'constant', constant_values=0)\n",
    "            target = np.pad(target, [[0, max_time_steps-target.shape[0]],[0,0]],'constant', constant_values=0)\n",
    "        Y.append(target)\n",
    "        X.append(state)\n",
    "\n",
    "        if count % 10000 == 0 and count > 0:\n",
    "            print count\n",
    "\n",
    "    return np.array(X),np.array(Y), np.array(seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train\n",
      "Loaded val\n",
      "Loaded test\n"
     ]
    }
   ],
   "source": [
    "dire = 'recurrent_data/'\n",
    "if not os.path.exists(dire):\n",
    "    os.makedirs(dire)\n",
    "    \n",
    "if not os.path.exists(dire + 'X_train.npy'):\n",
    "    x_train, y_train, seqlen_train = make_data(df_train)\n",
    "    np.save(dire + 'X_train.npy',x_train)\n",
    "    np.save(dire + 'Y_train.npy',y_train)\n",
    "    np.save(dire + 'seqlen_train.npy',seqlen_train)\n",
    "    print \"Saved train\"\n",
    "else:\n",
    "    x_train = np.load(dire + 'X_train.npy')\n",
    "    y_train = np.load(dire + 'Y_train.npy')\n",
    "    seqlen_train = np.load(dire + 'seqlen_train.npy')\n",
    "    print \"Loaded train\"\n",
    "\n",
    "if not os.path.exists(dire + 'X_val.npy'):\n",
    "    x_val,y_val, seqlen_val = make_data(df_val)\n",
    "    np.save(dire + 'X_val.npy',x_val)\n",
    "    np.save(dire + 'Y_val.npy',y_val)\n",
    "    np.save(dire + 'seqlen_val.npy',seqlen_val)\n",
    "    print \"Saved val\"\n",
    "else:\n",
    "    x_val = np.load(dire + 'X_val.npy')\n",
    "    y_val = np.load(dire + 'Y_val.npy')\n",
    "    seqlen_val = np.load(dire + 'seqlen_val.npy')\n",
    "    print \"Loaded val\"\n",
    "    \n",
    "if not os.path.exists(dire + 'X_test.npy'):\n",
    "    x_test, y_test, seqlen_test = make_data(df_test)\n",
    "    np.save(dire + 'X_test.npy',x_test)\n",
    "    np.save(dire +    'Y_test.npy',y_test)\n",
    "    np.save(dire + 'seqlen_test.npy',seqlen_test)\n",
    "    print \"Saved test\"\n",
    "else:\n",
    "    x_test = np.load(dire + 'X_test.npy')\n",
    "    y_test = np.load(dire + 'Y_test.npy')\n",
    "    seqlen_test = np.load(dire + 'seqlen_test.npy')\n",
    "    print \"Loaded test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12528, 20, 50) (12528, 20, 42)\n",
      "(1789, 20, 50) (1789, 20, 42)\n",
      "(3581, 20, 50) (3581, 20, 42)\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape, y_train.shape\n",
    "print x_val.shape, y_val.shape\n",
    "print x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sequence LSTM -- predicts change in key variables\n",
    "hidden1 = 25\n",
    "hidden2 = 32\n",
    "# target_size = 1  #just predict sofa\n",
    "target_size = y_train.shape[2]\n",
    "class EnvModel():\n",
    "    def __init__(self):\n",
    "        self.input = tf.placeholder(tf.float32, shape=[None, max_time_steps, len(cur_feat)])\n",
    "        self.targets = tf.placeholder(tf.float32, shape=[None, max_time_steps, target_size])\n",
    "        self.seq_len = tf.placeholder(tf.int32, shape = None)\n",
    "        self.rnn_layers = [tf.contrib.rnn.LSTMCell(size) for size in [hidden1, target_size]]\n",
    "        #self.rnn_layers = [tf.contrib.rnn.LSTMCell(size) for size in [hidden1, hidden2, target_size]]\n",
    "        self.multi_rnn_cell = tf.contrib.rnn.MultiRNNCell(self.rnn_layers)\n",
    "        self.outputs, self.state = tf.nn.dynamic_rnn(cell=self.multi_rnn_cell,\n",
    "                                       inputs=self.input,\n",
    "                                       sequence_length =self.seq_len,\n",
    "                                       dtype=tf.float32)\n",
    "        # 'outputs' is a tensor of shape [batch_size, max_time_steps, target_size]\n",
    "        # 'state' is a N-tuple where N is the number of LSTMCells containing a\n",
    "        # tf.contrib.rnn.LSTMStateTuple for each cell\n",
    "        self.loss = tf.reduce_mean(tf.square(self.targets-self.outputs))\n",
    "        self.reg_lambda = 0.0001\n",
    "\n",
    "        self.vars = tf.trainable_variables() \n",
    "        self.lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in self.vars\n",
    "                    if 'bias' not in v.name ]) * self.reg_lambda    \n",
    "\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "        # Ensures that we execute the update_ops before performing the model update, so batchnorm works\n",
    "            self.update_model = self.trainer.minimize(self.loss + self.lossL2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # Don't use all GPUs \n",
    "config.allow_soft_placement = True  # Enable manual control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_batch = 64\n",
    "def run_eval(sess, env_mdl):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    net_loss = 0.0\n",
    "    while start < len(x_val):\n",
    "        end = min(len(x_val), start+eval_batch)\n",
    "        x_batch = x_val[start:end]\n",
    "        y_batch = y_val[start:end]\n",
    "        seqlen_batch = seqlen_val[start:end]\n",
    "\n",
    "        # extract out only sofa for target\n",
    "#         y_batch = y_batch[:,:, 29]\n",
    "#         y_batch = y_batch[:,:, None]\n",
    "\n",
    "        loss = sess.run(env_model.loss,\n",
    "                        feed_dict={env_model.input:x_batch,\n",
    "                                   env_model.targets:y_batch,\n",
    "                                   env_model.seq_len:seqlen_batch})\n",
    "    \n",
    "        # update net loss\n",
    "        net_loss += loss\n",
    "\n",
    "        # increment start index to inds\n",
    "        start += eval_batch\n",
    "    \n",
    "    avg_eval_loss = net_loss/(len(x_val)/float(eval_batch))\n",
    "    \n",
    "    return avg_eval_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running default init\n",
      "Init done\n",
      "Saved Model, epoch 1\n",
      "('Average training loss per batch is ', 0.11886537463272212)\n",
      "Avg val loss at epoch end is 0.111391836735\n",
      "Saved Model, epoch 2\n",
      "('Average training loss per batch is ', 0.11037431210089156)\n",
      "Avg val loss at epoch end is 0.106728300027\n",
      "Saved Model, epoch 3\n",
      "('Average training loss per batch is ', 0.10706470999985666)\n",
      "Avg val loss at epoch end is 0.104728123807\n",
      "Saved Model, epoch 4\n",
      "('Average training loss per batch is ', 0.10559951273265317)\n",
      "Avg val loss at epoch end is 0.1035458891\n",
      "Saved Model, epoch 5\n",
      "('Average training loss per batch is ', 0.10475373481273043)\n",
      "Avg val loss at epoch end is 0.102878025486\n",
      "Saved Model, epoch 6\n",
      "('Average training loss per batch is ', 0.1041547190945115)\n",
      "Avg val loss at epoch end is 0.102443620704\n",
      "Saved Model, epoch 7\n",
      "('Average training loss per batch is ', 0.1037721182879817)\n",
      "Avg val loss at epoch end is 0.102114128627\n",
      "Saved Model, epoch 8\n",
      "('Average training loss per batch is ', 0.10344979977699076)\n",
      "Avg val loss at epoch end is 0.101917781265\n",
      "Saved Model, epoch 9\n",
      "('Average training loss per batch is ', 0.10316821403734833)\n",
      "Avg val loss at epoch end is 0.101623061251\n",
      "Saved Model, epoch 10\n",
      "('Average training loss per batch is ', 0.10299950571656988)\n",
      "Avg val loss at epoch end is 0.101451973731\n",
      "Saved Model, epoch 11\n",
      "('Average training loss per batch is ', 0.10279666041505747)\n",
      "Avg val loss at epoch end is 0.101332472049\n",
      "Saved Model, epoch 12\n",
      "('Average training loss per batch is ', 0.10266429254378395)\n",
      "Avg val loss at epoch end is 0.101178961136\n",
      "Saved Model, epoch 13\n",
      "('Average training loss per batch is ', 0.10253272442525374)\n",
      "Avg val loss at epoch end is 0.101049227046\n",
      "Saved Model, epoch 14\n",
      "('Average training loss per batch is ', 0.10245490070351543)\n",
      "Avg val loss at epoch end is 0.100963684752\n",
      "Saved Model, epoch 15\n",
      "('Average training loss per batch is ', 0.10228637466028732)\n",
      "Avg val loss at epoch end is 0.100961201147\n",
      "Saved Model, epoch 16\n",
      "('Average training loss per batch is ', 0.10220852745447122)\n",
      "Avg val loss at epoch end is 0.100855870777\n",
      "Saved Model, epoch 17\n",
      "('Average training loss per batch is ', 0.10210845654647165)\n",
      "Avg val loss at epoch end is 0.100758338787\n",
      "Saved Model, epoch 18\n",
      "('Average training loss per batch is ', 0.10200443810582313)\n",
      "Avg val loss at epoch end is 0.10069288869\n",
      "Saved Model, epoch 19\n",
      "('Average training loss per batch is ', 0.101944071617772)\n",
      "Avg val loss at epoch end is 0.100645659955\n",
      "Saved Model, epoch 20\n",
      "('Average training loss per batch is ', 0.10191156063347788)\n",
      "Avg val loss at epoch end is 0.100529430031\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "load_model = False #Whether to load a saved model.\n",
    "save_dir = \"./env_model_rnn/\"\n",
    "save_path = \"./env_model_rnn/ckpt\"#The path to save our model to.\n",
    "tf.reset_default_graph()\n",
    "env_model = EnvModel()\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    if load_model == True:\n",
    "        print('Trying to load model...')\n",
    "        try:\n",
    "            restorer = tf.train.import_meta_graph(save_path + '.meta')\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "            print \"Model restored\"\n",
    "        except IOError:\n",
    "            print \"No previous model found, running default init\"\n",
    "            sess.run(init)\n",
    "    else:\n",
    "        print(\"Running default init\")\n",
    "        sess.run(init)\n",
    "    print(\"Init done\")\n",
    "    \n",
    "    for j in range(1,num_epochs+1):\n",
    "        net_loss = 0.0\n",
    "        inds = np.random.permutation(x_train.shape[0])\n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "        while start_idx < len(x_train):\n",
    "            end_idx = min(len(x_train), start_idx+batch_size)\n",
    "            batch_inds = inds[start_idx:end_idx]\n",
    "            x_batch = x_train[batch_inds]\n",
    "            y_batch = y_train[batch_inds]\n",
    "            seqlen_batch = seqlen_train[batch_inds]\n",
    "            # extract out only sofa for target\n",
    "#             y_batch = y_batch[:,:, 29]\n",
    "#             y_batch = y_batch[:,:, None]\n",
    "            \n",
    "            # train using the batch\n",
    "            _,loss = sess.run([env_model.update_model,env_model.loss], \\\n",
    "            feed_dict={env_model.input:x_batch,\n",
    "                       env_model.targets:y_batch,\n",
    "                       env_model.seq_len:seqlen_batch})\n",
    "            \n",
    "            # update net loss\n",
    "            net_loss += loss\n",
    "            \n",
    "            # increment start index to inds\n",
    "            start_idx += batch_size\n",
    "        \n",
    "        saver.save(sess,save_path)\n",
    "        print(\"Saved Model, epoch \" + str(j))\n",
    "        \n",
    "        av_loss = net_loss/(len(x_train)/float(batch_size))\n",
    "        print(\"Average training loss per batch is \", av_loss)\n",
    "        net_loss = 0.0\n",
    "        \n",
    "        eval_loss = run_eval(sess, env_model)\n",
    "        \n",
    "        print (\"Avg val loss at epoch end is \" + str(eval_loss))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load model...\n",
      "No previous model found, running default init\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'init' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-62824b0421ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"No previous model found, running default init\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Init done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init' is not defined"
     ]
    }
   ],
   "source": [
    "# with tf.Session(config=config) as sess:\n",
    "#     print('Trying to load model...')\n",
    "#     try:\n",
    "#         restorer = tf.train.import_meta_graph(save_path + '.meta')\n",
    "#         restorer.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "#         print \"Model restored\"\n",
    "#     except IOError:\n",
    "#         print \"No previous model found, running default init\"\n",
    "#         sess.run(init)\n",
    "#     print(\"Init done\")\n",
    "    \n",
    "#     bx = x_train[50:100]\n",
    "#     by = y_train[50:100, 29]\n",
    "#     by = by[:, None]\n",
    "#     op = sess.run(env_model.output, \\\n",
    "#             feed_dict={env_model.cur_state:bx,\n",
    "#                        env_model.targets:by,\n",
    "#                        env_model.phase:False})\n",
    "\n",
    "# # print op.shape, by.shape\n",
    "# # for i in zip(np.squeeze(op), np.squeeze(by)):\n",
    "# #     print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11017323824480833"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((np.squeeze(op)-np.squeeze(by))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.17911743, 0.0)\n",
      "(0.093046457, 0.0)\n",
      "(0.13156834, 0.28613931514300006)\n",
      "(0.075183168, 0.0)\n",
      "(0.15709624, 1.144557260572)\n",
      "(0.053191449, 0.28613931513999979)\n",
      "(0.007503923, -0.28613931513999979)\n",
      "(0.086839095, 0.0)\n",
      "(0.059295628, 0.0)\n",
      "(-0.0029163994, 0.0)\n",
      "(0.02531508, 0.28613931513999979)\n",
      "(-0.036283854, 0.0)\n",
      "(-0.039782193, 0.0)\n",
      "(-0.10121112, -0.57227863027999981)\n",
      "(-0.16417766, -0.28613931514799995)\n",
      "(0.13331677, 0.0)\n",
      "(0.053894948, 0.0)\n",
      "(0.21118657, 0.28613931514799995)\n",
      "(0.16026877, 0.0)\n",
      "(0.082047328, 0.28613931514000002)\n",
      "(-0.057933476, 0.0)\n",
      "(0.035227526, 0.28613931513999979)\n",
      "(-0.036803689, 0.0)\n",
      "(-0.12285101, -0.28613931513999979)\n",
      "(-0.14585981, 0.0)\n",
      "(-0.14774466, -0.28613931514000002)\n",
      "(-0.19879135, -0.28613931514799995)\n",
      "(-0.043731753, 0.0)\n",
      "(0.027128305, 0.0)\n",
      "(-0.092596322, -1.144557260572)\n",
      "(0.24658708, 0.0)\n",
      "(0.16986302, 0.0)\n",
      "(0.16923501, 0.0)\n",
      "(0.14478707, 0.0)\n",
      "(0.09138447, 0.0)\n",
      "(0.12362672, 0.0)\n",
      "(0.057311695, 0.0)\n",
      "(0.064386889, 0.5722786302859999)\n",
      "(-0.077138633, 1.144557260574)\n",
      "(-0.51417947, -0.28613931514000002)\n",
      "(-0.17385907, -0.28613931514300006)\n",
      "(-0.22510956, -0.28613931514300001)\n",
      "(-0.16911776, -0.2861393151428)\n",
      "(-0.17119114, -0.28613931514319996)\n",
      "(-0.42581651, -0.28613931514300006)\n",
      "(-0.41570267, 0.0)\n",
      "(-0.38291588, 0.0)\n",
      "(-0.32653114, -0.85841794542920002)\n",
      "(0.11460985, 0.0)\n",
      "(0.10880844, 0.0)\n"
     ]
    }
   ],
   "source": [
    "for i in zip(np.squeeze(op), np.squeeze(by)):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
