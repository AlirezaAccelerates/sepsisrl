{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from autograd import grad\n",
    "from autograd.misc.optimizers import adam\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albumin', 'Arterial_BE', 'Arterial_lactate', 'Arterial_pH', 'BUN', 'CO2_mEqL', 'Calcium', 'Chloride', 'Creatinine', 'DiaBP', 'FiO2_1', 'GCS', 'Glucose', 'HCO3', 'HR', 'Hb', 'INR', 'Ionised_Ca', 'Magnesium', 'MeanBP', 'PT', 'PTT', 'PaO2_FiO2', 'Platelets_count', 'Potassium', 'RR', 'SGOT', 'SGPT', 'SIRS', 'SOFA', 'Shock_Index', 'Sodium', 'SpO2', 'SysBP', 'Temp_C', 'Total_bili', 'WBC_count', 'Weight_kg', 'age', 'elixhauser', 'gender', 'mechvent', 'output_4hourly', 'output_total', 'paCO2', 'paO2', 're_admission', 'bloc']\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "with open('../data/state_features.txt') as f:\n",
    "    state_features = f.read().split()\n",
    "print (state_features)\n",
    "print (len(state_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/rl_train_set_unscaled.csv')\n",
    "df_val = pd.read_csv('../data/rl_val_set_unscaled.csv')\n",
    "df_test = pd.read_csv('../data/rl_test_set_unscaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/csail.mit.edu/u/a/araghu/.conda/envs/my_root/lib/python3.5/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/afs/csail.mit.edu/u/a/araghu/.conda/envs/my_root/lib/python3.5/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/afs/csail.mit.edu/u/a/araghu/.conda/envs/my_root/lib/python3.5/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# let the mortality labels  be -1 and 1: -1 for survival\n",
    "df_train['died_in_hosp'][df_train['died_in_hosp'] == 0] = -1\n",
    "df_val['died_in_hosp'][df_val['died_in_hosp'] == 0] = -1\n",
    "df_test['died_in_hosp'][df_test['died_in_hosp'] == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_feat = list(np.loadtxt('../data/state_features_pred.txt', dtype=str))\n",
    "target_feat.append('died_in_hosp')\n",
    "\n",
    "cur_feat = list(np.loadtxt('../data/state_features.txt', dtype=str))\n",
    "cur_feat.append('iv_input_norm')\n",
    "cur_feat.append('vaso_input_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define an action mapping - how to get an id representing the action from the (iv,vaso) tuple\n",
    "action_map = {}\n",
    "count = 0\n",
    "for iv in range(5):\n",
    "    for vaso in range(5):\n",
    "        action_map[(iv,vaso)] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_action_map = {}\n",
    "for iv in range(5):\n",
    "    for vaso in range(5):\n",
    "        inv_action_map[5*iv+vaso] = [iv,vaso]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iv_mean = df_train['iv_input'].mean()\n",
    "iv_std = df_train['iv_input'].std()\n",
    "\n",
    "vaso_mean = df_train['vaso_input'].mean()\n",
    "vaso_std = df_train['vaso_input'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['iv_input_norm'] = (df_train['iv_input']- iv_mean)/iv_std\n",
    "df_train['vaso_input_norm'] = (df_train['vaso_input'] - vaso_mean)/vaso_std\n",
    "\n",
    "df_val['iv_input_norm'] = (df_val['iv_input']- iv_mean)/iv_std\n",
    "df_val['vaso_input_norm'] = (df_val['vaso_input'] - vaso_mean)/vaso_std\n",
    "\n",
    "df_test['iv_input_norm'] = (df_test['iv_input']- iv_mean)/iv_std\n",
    "df_test['vaso_input_norm'] = (df_test['vaso_input'] - vaso_mean)/vaso_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  X: (states, actions)\n",
    "#  Y: (difference between next state and current state (zeros if end of trajectory), mortality)\n",
    "def make_data(df_in):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for count,i in enumerate(df_in.index):\n",
    "        cur_state = df_in.loc[i,cur_feat]\n",
    "        if i != df_in.index[-1]:\n",
    "            # if not terminal step in trajectory             \n",
    "            if df_in.loc[i, 'icustayid'] == df_in.loc[i+1, 'icustayid']:\n",
    "                target = df_in.loc[i + 1, target_feat] - df_in.loc[i, target_feat]\n",
    "                target[-1] = df_in.loc[i, 'died_in_hosp']\n",
    "                Y.append(target)\n",
    "                X.append(cur_state)\n",
    "\n",
    "        if count % 10000 == 0 and count > 0:\n",
    "            print(count)\n",
    "\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  X: (s_t-3, a_t-3, s_t-2, a_t-2,s_t-1, a_t-1, s_t, a_t )\n",
    "#  Y: (difference between next state and current state (zeros if end of trajectory), mortality)\n",
    "hist = 3\n",
    "def make_data_history(df_in):\n",
    "    df_in = df_in.reset_index()\n",
    "    X = []\n",
    "    Y = []\n",
    "    count_in_traj = 0\n",
    "    for count,i in enumerate(df_in.index):\n",
    "        if count % 10000 == 0 and count > 0:\n",
    "            print (count)\n",
    "        \n",
    "        # skip the last one; no next state\n",
    "        if i == df_in.index[-1]:\n",
    "            break\n",
    "       \n",
    "        # if not terminal step in trajectory    \n",
    "        if df_in.loc[i, 'icustayid'] == df_in.loc[i+1, 'icustayid']:\n",
    "            count_in_traj += 1\n",
    "            if count_in_traj >=(hist+1):\n",
    "\n",
    "                target = df_in.loc[i + 1, target_feat] - df_in.loc[i, target_feat]\n",
    "                target[-1] = df_in.loc[i, 'died_in_hosp']\n",
    "                Y.append(target)\n",
    "                state = df_in.loc[i-hist, cur_feat]\n",
    "                for index in range(hist-1,-1,-1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, cur_feat]])\n",
    "                #state = np.hstack([df_in.loc[i-2, cur_feat], df_in.loc[i-1, cur_feat], df_in.loc[i, cur_feat]])\n",
    "                X.append(state)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            count_in_traj = 0\n",
    "\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  X: (s_t-3, a_t-3, s_t-2, a_t-2,s_t-1, a_t-1, s_t, a_t )\n",
    "#  Y: (difference between next state and current state (zeros if end of trajectory), mortality)\n",
    "hist = 3\n",
    "def make_data_history_zeros(df_in):\n",
    "    df_in = df_in.reset_index()\n",
    "    X = []\n",
    "    Y = []\n",
    "    count_in_traj = 0\n",
    "    for count,i in enumerate(df_in.index):\n",
    "        if count % 10000 == 0 and count > 0:\n",
    "            print (count)\n",
    "        \n",
    "        # skip the last one; no next state\n",
    "        if i == df_in.index[-1]:\n",
    "            break\n",
    "       \n",
    "        # if not terminal step in trajectory    \n",
    "        if df_in.loc[i, 'icustayid'] == df_in.loc[i+1, 'icustayid']:\n",
    "            count_in_traj += 1\n",
    "            target = df_in.loc[i + 1, target_feat] - df_in.loc[i, target_feat]\n",
    "            target[-1] = df_in.loc[i, 'died_in_hosp']\n",
    "            Y.append(target)\n",
    "            if count_in_traj >=(hist+1):\n",
    "                state = df_in.loc[i-hist, cur_feat]\n",
    "                for index in range(hist-1,-1,-1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, cur_feat]])\n",
    "                X.append(state)\n",
    "            else:\n",
    "                num_zeros = (hist+1) - count_in_traj\n",
    "                num_actual = count_in_traj - 1\n",
    "                state = np.hstack([np.zeros(len(cur_feat)) for _ in range(num_zeros)])\n",
    "                for index in range(num_actual, 0, -1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, cur_feat]])\n",
    "                state = np.hstack([state, df_in.loc[i, cur_feat]])\n",
    "                X.append(state) \n",
    "        else:\n",
    "            count_in_traj = 0\n",
    "\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# dire = 'converted_data/'\n",
    "# if not os.path.exists(dire):\n",
    "#     os.makedirs(dire)\n",
    "    \n",
    "# if not os.path.exists(dire + 'X_train.txt'):\n",
    "#     x_train_nohist, y_train_nohist = make_data(df_train)\n",
    "#     np.savetxt(dire + 'X_train.txt',x_train_nohist,fmt='%5.4f')\n",
    "#     np.savetxt(dire + 'Y_train.txt',y_train_nohist,fmt='%5.4f')\n",
    "#     print(\"Saved train\")\n",
    "# else:\n",
    "#     x_train_nohist = np.loadtxt(dire + 'X_train.txt')\n",
    "#     y_train_nohist = np.loadtxt(dire + 'Y_train.txt')\n",
    "#     print (\"Loaded train\")\n",
    "\n",
    "# if not os.path.exists(dire + 'X_val.txt'):\n",
    "#     x_val_nohist,y_val_nohist = make_data(df_val)\n",
    "#     np.savetxt(dire + 'X_val.txt',x_val_nohist,fmt='%5.4f')\n",
    "#     np.savetxt(dire + 'Y_val.txt',y_val_nohist,fmt='%5.4f')\n",
    "#     print (\"Saved val\")\n",
    "# else:\n",
    "#     x_val_nohist = np.loadtxt(dire + 'X_val.txt')\n",
    "#     y_val_nohist = np.loadtxt(dire + 'Y_val.txt')\n",
    "#     print (\"Loaded val\")\n",
    "    \n",
    "# if not os.path.exists(dire + 'X_test.txt'):\n",
    "#     x_test_nohist, y_test_nohist = make_data(df_test)\n",
    "#     np.savetxt(dire + 'X_test.txt',x_test_nohist,fmt='%5.4f')\n",
    "#     np.savetxt(dire +    'Y_test.txt',y_test_nohist,fmt='%5.4f')\n",
    "#     print (\"Saved test\")\n",
    "# else:\n",
    "#     x_test_nohist = np.loadtxt(dire + 'X_test.txt')\n",
    "#     y_test_nohist = np.loadtxt(dire + 'Y_test.txt')\n",
    "#     print (\"Loaded test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dire = 'converted_data/'\n",
    "# if not os.path.exists(dire):\n",
    "#     os.makedirs(dire)\n",
    "    \n",
    "# if not os.path.exists(dire + 'X_train_hist.txt'):\n",
    "#     x_train_hist, y_train_hist = make_data_history(df_train)\n",
    "#     np.savetxt(dire + 'X_train_hist.txt',x_train_hist,fmt='%5.4f')\n",
    "#     np.savetxt(dire + 'Y_train_hist.txt',y_train_hist,fmt='%5.4f')\n",
    "#     print (\"Saved train\")\n",
    "# else:\n",
    "#     x_train_hist = np.loadtxt(dire + 'X_train_hist.txt')\n",
    "#     y_train_hist = np.loadtxt(dire + 'Y_train_hist.txt')\n",
    "#     print (\"Loaded train\")\n",
    "\n",
    "# if not os.path.exists(dire + 'X_val_hist.txt'):\n",
    "#     x_val_hist,y_val_hist = make_data_history(df_val)\n",
    "#     np.savetxt(dire + 'X_val_hist.txt',x_val_hist,fmt='%5.4f')\n",
    "#     np.savetxt(dire + 'Y_val_hist.txt',y_val_hist,fmt='%5.4f')\n",
    "#     print (\"Saved val\")\n",
    "# else:\n",
    "#     x_val_hist = np.loadtxt(dire + 'X_val_hist.txt')\n",
    "#     y_val_hist = np.loadtxt(dire + 'Y_val_hist.txt')\n",
    "#     print (\"Loaded val\")\n",
    "    \n",
    "# if not os.path.exists(dire + 'X_test_hist.txt'):\n",
    "#     x_test_hist, y_test_hist = make_data_history(df_test)\n",
    "#     np.savetxt(dire + 'X_test_hist.txt',x_test_hist,fmt='%5.4f')\n",
    "#     np.savetxt(dire + 'Y_test_hist.txt',y_test_hist,fmt='%5.4f')\n",
    "#     print (\"Saved test\")\n",
    "# else:\n",
    "#     x_test_hist = np.loadtxt(dire + 'X_test_hist.txt')\n",
    "#     y_test_hist = np.loadtxt(dire + 'Y_test_hist.txt')\n",
    "#     print (\"Loaded test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train_zeros\n",
      "Loaded val_zeros\n",
      "Loaded test_zeros\n"
     ]
    }
   ],
   "source": [
    "dire = 'converted_data/'\n",
    "if not os.path.exists(dire):\n",
    "    os.makedirs(dire)\n",
    "\n",
    "if not os.path.exists(dire + 'X_train_hist_zeros.txt'):\n",
    "    train_feat_zeros, train_labels_zeros = make_data_history_zeros(df_train)\n",
    "    np.savetxt(dire + 'X_train_hist_zeros.txt',train_feat_zeros,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_train_hist_zeros.txt',train_labels_zeros,fmt='%5.4f')\n",
    "    print (\"Saved train_zeros\")\n",
    "else:\n",
    "    train_feat_zeros = np.loadtxt(dire + 'X_train_hist_zeros.txt')\n",
    "    train_labels_zeros = np.loadtxt(dire + 'Y_train_hist_zeros.txt')\n",
    "    print (\"Loaded train_zeros\")\n",
    "\n",
    "if not os.path.exists(dire + 'X_val_hist_zeros.txt'):\n",
    "    val_feat_zeros, val_labels_zeros = make_data_history_zeros(df_val)\n",
    "    np.savetxt(dire + 'X_val_hist_zeros.txt',val_feat_zeros,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_val_hist_zeros.txt',val_labels_zeros,fmt='%5.4f')\n",
    "    print (\"Saved val_zeros\")\n",
    "else:\n",
    "    val_feat_zeros = np.loadtxt(dire + 'X_val_hist_zeros.txt')\n",
    "    val_labels_zeros = np.loadtxt(dire + 'Y_val_hist_zeros.txt')\n",
    "    print (\"Loaded val_zeros\")\n",
    "\n",
    "if not os.path.exists(dire + 'X_test_hist_zeros.txt'):\n",
    "    test_feat_zeros, test_labels_zeros = make_data_history_zeros(df_test)\n",
    "    np.savetxt(dire + 'X_test_hist_zeros.txt',test_feat_zeros,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_test_hist_zeros.txt',test_labels_zeros,fmt='%5.4f')\n",
    "    print (\"Saved test_zeros\")\n",
    "else:\n",
    "    test_feat_zeros = np.loadtxt(dire + 'X_test_hist_zeros.txt')\n",
    "    test_labels_zeros = np.loadtxt(dire + 'Y_test_hist_zeros.txt')\n",
    "    print (\"Loaded test_zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train_feat_zeros\n",
    "y_train = train_labels_zeros\n",
    "\n",
    "x_val = val_feat_zeros\n",
    "y_val = val_labels_zeros\n",
    "\n",
    "x_test = test_feat_zeros\n",
    "y_test = test_labels_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156967, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get random sample of train/test for this model\n",
    "train_idx = np.random.permutation(len(x_train))\n",
    "val_idx = np.random.permutation(len(x_val))\n",
    "\n",
    "x_train = x_train[train_idx]\n",
    "y_train = y_train[train_idx]\n",
    "\n",
    "x_val = x_val[val_idx]\n",
    "y_val = y_val[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # only predict sofa\n",
    "# y_train = np.expand_dims(y_train[:, 29], 1)\n",
    "# y_val = np.expand_dims(y_val[:,29],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def black_box_variational_inference(logprob, D, num_samples):\n",
    "    \"\"\"Implements http://arxiv.org/abs/1401.0118, and uses the\n",
    "    local reparameterization trick from http://arxiv.org/abs/1506.02557\"\"\"\n",
    "\n",
    "    def unpack_params(params):\n",
    "        # Variational dist is a diagonal Gaussian.\n",
    "        mean, log_std = params[:D], params[D:]\n",
    "        return mean, log_std\n",
    "\n",
    "    def gaussian_entropy(log_std):\n",
    "        return 0.5 * D * (1.0 + np.log(2*np.pi)) + np.sum(log_std)\n",
    "\n",
    "    rs = npr.RandomState(0)\n",
    "    def variational_objective(params, t):\n",
    "        \"\"\"Provides a stochastic estimate of the variational lower bound.\"\"\"\n",
    "        mean, log_std = unpack_params(params)\n",
    "        samples = rs.randn(num_samples, D) * np.exp(log_std) + mean\n",
    "        lower_bound = gaussian_entropy(log_std) + np.mean(logprob(samples, t))\n",
    "        return -lower_bound\n",
    "\n",
    "    gradient = grad(variational_objective)\n",
    "\n",
    "    return variational_objective, gradient, unpack_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_nn_funs(layer_sizes, L2_reg, noise_variance, nonlinearity=np.tanh):\n",
    "    \"\"\"These functions implement a standard multi-layer perceptron,\n",
    "    vectorized over both training examples and weight samples.\"\"\"\n",
    "    shapes = list(zip(layer_sizes[:-1], layer_sizes[1:]))\n",
    "    num_weights = sum((m+1)*n for m, n in shapes)\n",
    "\n",
    "    def unpack_layers(weights):\n",
    "        num_weight_sets = len(weights)\n",
    "        for m, n in shapes:\n",
    "            yield weights[:, :m*n]     .reshape((num_weight_sets, m, n)),\\\n",
    "                  weights[:, m*n:m*n+n].reshape((num_weight_sets, 1, n))\n",
    "            weights = weights[:, (m+1)*n:]\n",
    "\n",
    "    def predictions(weights, inputs):\n",
    "        \"\"\"weights is shape (num_weight_samples x num_weights)\n",
    "           inputs  is shape (num_datapoints x D)\"\"\"\n",
    "        inputs = np.expand_dims(inputs, 0)\n",
    "        for W, b in unpack_layers(weights):\n",
    "            outputs = np.einsum('mnd,mdo->mno', inputs, W) + b\n",
    "            inputs = nonlinearity(outputs)\n",
    "        outputs = 1.5*np.tanh(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def logprob(weights, inputs, targets):\n",
    "        log_prior = -L2_reg * np.sum(weights**2, axis=1)\n",
    "        preds = predictions(weights, inputs)\n",
    "        log_lik = -np.sum((preds - targets)**2, axis=1)[:, 0] / noise_variance\n",
    "        return log_prior + log_lik\n",
    "\n",
    "    return num_weights, predictions, logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing variational parameters...\n",
      "Iteration 0 lower bound -149767.85633941315\n",
      "Iteration 0: train mse: 1.5256512249407104 val mse: 1.522787818027553\n",
      "(0.0, 1.0672272432998893)\n",
      "(0.85840000000000005, -0.49900740745440597)\n",
      "(-0.28610000000000002, 1.2026697416005852)\n",
      "(0.28610000000000002, 0.97176297202875328)\n",
      "(0.0, 1.1755986909558662)\n",
      "(0.0, 1.0790898896694552)\n",
      "(0.28610000000000002, 0.92085263929580174)\n",
      "(-0.28610000000000002, 1.1016659791355972)\n",
      "(0.0, -0.83279830558293266)\n",
      "(0.28610000000000002, -0.37177154336388413)\n",
      "Iteration 1 lower bound -137056.04653507943\n",
      "Iteration 1: train mse: 1.4874541323482975 val mse: 1.4838963451260265\n",
      "(0.0, 1.1152940945350316)\n",
      "(0.85840000000000005, -0.54822525092740648)\n",
      "(-0.28610000000000002, 1.1352618407560484)\n",
      "(0.28610000000000002, -0.64054340172171109)\n",
      "(0.0, 1.0711035504841813)\n",
      "(0.0, 1.0648365238337507)\n",
      "(0.28610000000000002, 1.2355105845018837)\n",
      "(-0.28610000000000002, 1.1473342831628708)\n",
      "(0.0, -0.90474515673214762)\n",
      "(0.28610000000000002, -0.22936202047546619)\n",
      "Iteration 2 lower bound -120185.49448513368\n",
      "Iteration 2: train mse: 1.4431646924039365 val mse: 1.4423034596312894\n",
      "(0.0, 1.155245178946287)\n",
      "(0.85840000000000005, -0.57857862051235731)\n",
      "(-0.28610000000000002, 1.0629877183012884)\n",
      "(0.28610000000000002, -1.0132259713943035)\n",
      "(0.0, 1.1037710216739107)\n",
      "(0.0, 1.0974635908263173)\n",
      "(0.28610000000000002, 1.3646055347702202)\n",
      "(-0.28610000000000002, 1.1760773274267151)\n",
      "(0.0, -0.86082791127457092)\n",
      "(0.28610000000000002, -0.14185701589293634)\n",
      "Iteration 3 lower bound -108249.29370716293\n",
      "Iteration 3: train mse: 1.4066132545763634 val mse: 1.4020989249451694\n",
      "(0.0, 1.1750432224744065)\n",
      "(0.85840000000000005, -0.64780945195006567)\n",
      "(-0.28610000000000002, -0.42097972601287453)\n",
      "(0.28610000000000002, -1.1118316277589253)\n",
      "(0.0, 1.0906658540916969)\n",
      "(0.0, 1.1063201867491164)\n",
      "(0.28610000000000002, 1.3385697981853766)\n",
      "(-0.28610000000000002, 1.215707001836603)\n",
      "(0.0, -0.87320995920887901)\n",
      "(0.28610000000000002, -0.079809769971774178)\n",
      "Iteration 4 lower bound -97709.06171788633\n",
      "Iteration 4: train mse: 1.3702447482579878 val mse: 1.3646843028119369\n",
      "(0.0, 1.1933199788183317)\n",
      "(0.85840000000000005, -0.74600494398748263)\n",
      "(-0.28610000000000002, -1.076505909076753)\n",
      "(0.28610000000000002, -1.1960337068989291)\n",
      "(0.0, 1.0578084299085428)\n",
      "(0.0, 1.0936321511370755)\n",
      "(0.28610000000000002, 1.003076207862756)\n",
      "(-0.28610000000000002, 1.1952907233293326)\n",
      "(0.0, -0.8865409125825634)\n",
      "(0.28610000000000002, -0.078123099714426089)\n",
      "Iteration 5 lower bound -86668.59099327278\n",
      "Iteration 5: train mse: 1.3348726664151624 val mse: 1.3277727170738063\n",
      "(0.0, 1.2066716753476336)\n",
      "(0.85840000000000005, -0.8283621095135365)\n",
      "(-0.28610000000000002, -1.1029435483085968)\n",
      "(0.28610000000000002, -1.230016533439984)\n",
      "(0.0, 1.019616264087013)\n",
      "(0.0, 1.0652758384951926)\n",
      "(0.28610000000000002, 0.91391180335169264)\n",
      "(-0.28610000000000002, 1.1528176375050794)\n",
      "(0.0, -0.93185966417766974)\n",
      "(0.28610000000000002, -0.086604426920938241)\n",
      "Iteration 6 lower bound -71162.6001033389\n",
      "Iteration 6: train mse: 1.2978820763695205 val mse: 1.2905350253468584\n",
      "(0.0, 1.2217436222102442)\n",
      "(0.85840000000000005, -0.55152477989533899)\n",
      "(-0.28610000000000002, -1.069397773159205)\n",
      "(0.28610000000000002, -1.2560125178384325)\n",
      "(0.0, 0.98018762503531387)\n",
      "(0.0, 1.0307681061745808)\n",
      "(0.28610000000000002, 0.87476332160750014)\n",
      "(-0.28610000000000002, 1.092079337169988)\n",
      "(0.0, -1.0453385272600033)\n",
      "(0.28610000000000002, -0.10678082553138304)\n",
      "Iteration 7 lower bound -62243.26276107482\n",
      "Iteration 7: train mse: 1.2556484806472414 val mse: 1.25190430699801\n",
      "(0.0, 1.2342689550583925)\n",
      "(0.85840000000000005, -0.435182361592091)\n",
      "(-0.28610000000000002, -0.9877386135788484)\n",
      "(0.28610000000000002, -1.2251705247780769)\n",
      "(0.0, 0.94488109632284656)\n",
      "(0.0, 1.011582032549271)\n",
      "(0.28610000000000002, 0.83598063287629931)\n",
      "(-0.28610000000000002, 1.0316836689035642)\n",
      "(0.0, -1.194211426575869)\n",
      "(0.28610000000000002, -0.1198219936383578)\n",
      "Iteration 8 lower bound -55323.72694142324\n",
      "Iteration 8: train mse: 1.2167017598302055 val mse: 1.2111001253062854\n",
      "(0.0, 1.238238488613564)\n",
      "(0.85840000000000005, -0.23362185587339646)\n",
      "(-0.28610000000000002, -0.87204708767322092)\n",
      "(0.28610000000000002, -1.1590722928383204)\n",
      "(0.0, 0.92215354865834787)\n",
      "(0.0, 0.99957737034789085)\n",
      "(0.28610000000000002, 0.77527738794278367)\n",
      "(-0.28610000000000002, 0.98821099710459115)\n",
      "(0.0, -1.2642976738837857)\n",
      "(0.28610000000000002, -0.10970822706505531)\n",
      "Iteration 9 lower bound -48881.42868328779\n",
      "Iteration 9: train mse: 1.171079345360454 val mse: 1.1691042035033519\n",
      "(0.0, 1.2154158377445357)\n",
      "(0.85840000000000005, 0.73305938320133734)\n",
      "(-0.28610000000000002, -0.73000220940718186)\n",
      "(0.28610000000000002, -1.0624035420514719)\n",
      "(0.0, 0.91244138644165551)\n",
      "(0.0, 0.99580733713684277)\n",
      "(0.28610000000000002, 0.71701872728473881)\n",
      "(-0.28610000000000002, 0.96466818358359774)\n",
      "(0.0, -1.2690455389667452)\n",
      "(0.28610000000000002, -0.07732757530018107)\n",
      "Iteration 10 lower bound -45635.42318229919\n",
      "Iteration 10: train mse: 1.1256986604598678 val mse: 1.1256500988513076\n",
      "(0.0, 1.19208638025493)\n",
      "(0.85840000000000005, 0.95782279161834682)\n",
      "(-0.28610000000000002, -0.53835479829100774)\n",
      "(0.28610000000000002, -0.96464978917832789)\n",
      "(0.0, 0.91429033280372773)\n",
      "(0.0, 0.99967437083750055)\n",
      "(0.28610000000000002, 0.69199088167839096)\n",
      "(-0.28610000000000002, 0.9590600010178012)\n",
      "(0.0, -1.2534429821988819)\n",
      "(0.28610000000000002, -0.038786995734103588)\n",
      "Iteration 11 lower bound -39878.45748719442\n",
      "Iteration 11: train mse: 1.082923670591214 val mse: 1.0814191599017033\n",
      "(0.0, 1.1708247902140587)\n",
      "(0.85840000000000005, 0.84397884066652418)\n",
      "(-0.28610000000000002, -0.119033500352313)\n",
      "(0.28610000000000002, -0.90345086387149354)\n",
      "(0.0, 0.92429086115096004)\n",
      "(0.0, 1.009412775604061)\n",
      "(0.28610000000000002, 0.70504030532352435)\n",
      "(-0.28610000000000002, 0.96205769172093181)\n",
      "(0.0, -1.2243996032895208)\n",
      "(0.28610000000000002, -0.026240839515453522)\n",
      "Iteration 12 lower bound -37224.078766980696\n",
      "Iteration 12: train mse: 1.0476638573445543 val mse: 1.037457933681194\n",
      "(0.0, 1.1578346450874299)\n",
      "(0.85840000000000005, 0.52831014059232861)\n",
      "(-0.28610000000000002, 0.71085984203875818)\n",
      "(0.28610000000000002, -0.82706975885211553)\n",
      "(0.0, 0.93514154497881496)\n",
      "(0.0, 1.0191801072708682)\n",
      "(0.28610000000000002, 0.7260073860487265)\n",
      "(-0.28610000000000002, 0.96798235333064953)\n",
      "(0.0, -1.1830065198506943)\n",
      "(0.28610000000000002, -0.069352623775796901)\n",
      "Iteration 13 lower bound -35187.376241397484\n",
      "Iteration 13: train mse: 0.9994295484897123 val mse: 0.9941604555421413\n",
      "(0.0, 1.1766972170297667)\n",
      "(0.85840000000000005, 0.30515360585149409)\n",
      "(-0.28610000000000002, 0.82707171114209233)\n",
      "(0.28610000000000002, -0.70776071234935189)\n",
      "(0.0, 0.93809367618638695)\n",
      "(0.0, 1.0243275530636846)\n",
      "(0.28610000000000002, 0.73485908671987121)\n",
      "(-0.28610000000000002, 0.97356349554728094)\n",
      "(0.0, -1.1286233650771014)\n",
      "(0.28610000000000002, -0.16529040459434446)\n",
      "Iteration 14 lower bound -35386.02004149069\n",
      "Iteration 14: train mse: 0.9595071632895563 val mse: 0.9517391681351225\n",
      "(0.0, 1.2224003794724367)\n",
      "(0.85840000000000005, 0.23674748134997953)\n",
      "(-0.28610000000000002, 0.81939128571354058)\n",
      "(0.28610000000000002, -0.61763852871898584)\n",
      "(0.0, 0.95529383380849775)\n",
      "(0.0, 1.0201441050250621)\n",
      "(0.28610000000000002, 0.72935438450663015)\n",
      "(-0.28610000000000002, 0.97405046126392669)\n",
      "(0.0, -1.068647872008319)\n",
      "(0.28610000000000002, -0.25399915736900203)\n",
      "Iteration 15 lower bound -34376.801172913045\n",
      "Iteration 15: train mse: 0.9130568657752273 val mse: 0.909831340076784\n",
      "(0.0, 1.2337763914844668)\n",
      "(0.85840000000000005, 0.22392307988860921)\n",
      "(-0.28610000000000002, 0.79573592109280988)\n",
      "(0.28610000000000002, -0.59855076835382881)\n",
      "(0.0, 0.96932968977291356)\n",
      "(0.0, 1.0136363960445869)\n",
      "(0.28610000000000002, 0.71065103889507075)\n",
      "(-0.28610000000000002, 0.97124534824684916)\n",
      "(0.0, -1.0124760506135666)\n",
      "(0.28610000000000002, -0.31751155589449687)\n",
      "Iteration 16 lower bound -31223.447413532147\n",
      "Iteration 16: train mse: 0.873346974117302 val mse: 0.8689435250588311\n",
      "(0.0, 1.2130455061625771)\n",
      "(0.85840000000000005, 0.21637664257823444)\n",
      "(-0.28610000000000002, 0.75709078074096714)\n",
      "(0.28610000000000002, -0.58652981115398084)\n",
      "(0.0, 0.9770540657840916)\n",
      "(0.0, 1.0097179903685134)\n",
      "(0.28610000000000002, 0.67878817273242231)\n",
      "(-0.28610000000000002, 0.96214675796801985)\n",
      "(0.0, -0.97046396312370276)\n",
      "(0.28610000000000002, -0.37282730154139943)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17 lower bound -30513.983744649235\n",
      "Iteration 17: train mse: 0.835444857889795 val mse: 0.8293873625548834\n",
      "(0.0, 1.187308667135653)\n",
      "(0.85840000000000005, 0.23209441028666133)\n",
      "(-0.28610000000000002, 0.70933314012499205)\n",
      "(0.28610000000000002, -0.56918210165468308)\n",
      "(0.0, 0.98403171242066656)\n",
      "(0.0, 1.0605998107234529)\n",
      "(0.28610000000000002, 0.63915041802763917)\n",
      "(-0.28610000000000002, 0.94938929643065839)\n",
      "(0.0, -0.93606551331143917)\n",
      "(0.28610000000000002, -0.41198395973215873)\n",
      "Iteration 18 lower bound -28155.473528947376\n",
      "Iteration 18: train mse: 0.7966539932635911 val mse: 0.791795031678807\n",
      "(0.0, 1.1576070139183572)\n",
      "(0.85840000000000005, 0.25821756810266727)\n",
      "(-0.28610000000000002, 0.65350962653008104)\n",
      "(0.28610000000000002, -0.55521359681683324)\n",
      "(0.0, 0.96736118488606659)\n",
      "(0.0, 1.1351869856640189)\n",
      "(0.28610000000000002, 0.59378544682510648)\n",
      "(-0.28610000000000002, 0.93418434858203026)\n",
      "(0.0, -0.90233461890016586)\n",
      "(0.28610000000000002, -0.44143822963515406)\n",
      "Iteration 19 lower bound -25326.22855202782\n",
      "Iteration 19: train mse: 0.7589253689953864 val mse: 0.7560877763170133\n",
      "(0.0, 1.1327157065446651)\n",
      "(0.85840000000000005, 0.30931437768491049)\n",
      "(-0.28610000000000002, 0.59348481506717632)\n",
      "(0.28610000000000002, -0.54056460374300708)\n",
      "(0.0, 0.95012175265605081)\n",
      "(0.0, 1.1123855060050241)\n",
      "(0.28610000000000002, 0.54566998856199822)\n",
      "(-0.28610000000000002, 0.91777321073672946)\n",
      "(0.0, -0.87139681978366512)\n",
      "(0.28610000000000002, -0.46105286940524282)\n",
      "Iteration 20 lower bound -24003.49546569053\n",
      "Iteration 20: train mse: 0.7253400517653859 val mse: 0.721864747269245\n",
      "(0.0, 1.1015895379457195)\n",
      "(0.85840000000000005, 0.36994303568462278)\n",
      "(-0.28610000000000002, 0.53249859448943238)\n",
      "(0.28610000000000002, -0.52755072143163972)\n",
      "(0.0, 0.94049014823387334)\n",
      "(0.0, 1.0755991075868714)\n",
      "(0.28610000000000002, 0.50043679875322167)\n",
      "(-0.28610000000000002, 0.90094958689927451)\n",
      "(0.0, -0.83992885282737806)\n",
      "(0.28610000000000002, -0.47260176314489399)\n",
      "Iteration 21 lower bound -22892.626735142385\n",
      "Iteration 21: train mse: 0.694269935757075 val mse: 0.6892324340007134\n",
      "(0.0, 1.0727786754239315)\n",
      "(0.85840000000000005, 0.4229489908309807)\n",
      "(-0.28610000000000002, 0.47433666927962637)\n",
      "(0.28610000000000002, -0.51182016865007851)\n",
      "(0.0, 0.93067958425140174)\n",
      "(0.0, 1.0350787580477321)\n",
      "(0.28610000000000002, 0.46332756964237864)\n",
      "(-0.28610000000000002, 0.8854056538695142)\n",
      "(0.0, -0.79687599496263939)\n",
      "(0.28610000000000002, -0.47431300281777977)\n",
      "Iteration 22 lower bound -20131.752726297855\n",
      "Iteration 22: train mse: 0.6642853549230535 val mse: 0.6580236739563902\n",
      "(0.0, 1.0402082927081626)\n",
      "(0.85840000000000005, 0.46609271616125658)\n",
      "(-0.28610000000000002, 0.4234192111565081)\n",
      "(0.28610000000000002, -0.49398912996885347)\n",
      "(0.0, 0.92685002852904863)\n",
      "(0.0, 0.99366820152422264)\n",
      "(0.28610000000000002, 0.43679392749193047)\n",
      "(-0.28610000000000002, 0.87021414268900765)\n",
      "(0.0, -0.74606980474149376)\n",
      "(0.28610000000000002, -0.46713269858906503)\n",
      "Iteration 23 lower bound -18140.93813835923\n",
      "Iteration 23: train mse: 0.6289234726089734 val mse: 0.6283471969531056\n",
      "(0.0, 1.0084144686417473)\n",
      "(0.85840000000000005, 0.50087133118283877)\n",
      "(-0.28610000000000002, 0.3824836781884442)\n",
      "(0.28610000000000002, -0.47280199749248597)\n",
      "(0.0, 0.92218083874565993)\n",
      "(0.0, 0.95080761547337822)\n",
      "(0.28610000000000002, 0.42140653629988922)\n",
      "(-0.28610000000000002, 0.85467373649336909)\n",
      "(0.0, -0.69273161941988914)\n",
      "(0.28610000000000002, -0.45138288119541842)\n",
      "Iteration 24 lower bound -16770.495225965115\n",
      "Iteration 24: train mse: 0.6031550123757664 val mse: 0.6002310892467042\n",
      "(0.0, 0.97815912806157823)\n",
      "(0.85840000000000005, 0.5319816203119353)\n",
      "(-0.28610000000000002, 0.3544392305676749)\n",
      "(0.28610000000000002, -0.45028165493458056)\n",
      "(0.0, 0.91713280336100289)\n",
      "(0.0, 0.90578294974817353)\n",
      "(0.28610000000000002, 0.41789108612115511)\n",
      "(-0.28610000000000002, 0.84101608819492701)\n",
      "(0.0, -0.63333154900088806)\n",
      "(0.28610000000000002, -0.42481945953931871)\n",
      "Iteration 25 lower bound -16642.58680148704\n",
      "Iteration 25: train mse: 0.5811883973441099 val mse: 0.5735856709710553\n",
      "(0.0, 0.94917966834213063)\n",
      "(0.85840000000000005, 0.55731126761461214)\n",
      "(-0.28610000000000002, 0.33635811126285164)\n",
      "(0.28610000000000002, -0.42966055171158823)\n",
      "(0.0, 0.90988202694740838)\n",
      "(0.0, 0.86426765623520951)\n",
      "(0.28610000000000002, 0.42403403342486234)\n",
      "(-0.28610000000000002, 0.82620979515116955)\n",
      "(0.0, -0.57602807336177997)\n",
      "(0.28610000000000002, -0.3926358759979432)\n",
      "Iteration 26 lower bound -15110.695056724042\n",
      "Iteration 26: train mse: 0.5553609698251988 val mse: 0.5483264294904376\n",
      "(0.0, 0.91947593079684187)\n",
      "(0.85840000000000005, 0.57611599687064863)\n",
      "(-0.28610000000000002, 0.32416250857009005)\n",
      "(0.28610000000000002, -0.41017785004854834)\n",
      "(0.0, 0.89834986574944775)\n",
      "(0.0, 0.82473686069787111)\n",
      "(0.28610000000000002, 0.43403145279115363)\n",
      "(-0.28610000000000002, 0.80804872185390908)\n",
      "(0.0, -0.52260350336413419)\n",
      "(0.28610000000000002, -0.35951670616014869)\n",
      "Iteration 27 lower bound -14341.368849800634\n",
      "Iteration 27: train mse: 0.531746619807496 val mse: 0.5246521352948473\n",
      "(0.0, 0.88649465305648045)\n",
      "(0.85840000000000005, 0.58597411082594264)\n",
      "(-0.28610000000000002, 0.31406429564136612)\n",
      "(0.28610000000000002, -0.39765092558777282)\n",
      "(0.0, 0.88119621927083036)\n",
      "(0.0, 0.78118249711739673)\n",
      "(0.28610000000000002, 0.44372056719054143)\n",
      "(-0.28610000000000002, 0.78480886274366346)\n",
      "(0.0, -0.47624453414249829)\n",
      "(0.28610000000000002, -0.32900433897432496)\n",
      "Iteration 28 lower bound -14195.279778274504\n",
      "Iteration 28: train mse: 0.5054254674548682 val mse: 0.5024648126357003\n",
      "(0.0, 0.848974534621877)\n",
      "(0.85840000000000005, 0.58624821812061867)\n",
      "(-0.28610000000000002, 0.30255360193032271)\n",
      "(0.28610000000000002, -0.38463753740434881)\n",
      "(0.0, 0.85832489734049133)\n",
      "(0.0, 0.73219423169538245)\n",
      "(0.28610000000000002, 0.4495259801671268)\n",
      "(-0.28610000000000002, 0.75661012614750167)\n",
      "(0.0, -0.43770182381139583)\n",
      "(0.28610000000000002, -0.30255875009044048)\n",
      "Iteration 29 lower bound -14188.400526948011\n",
      "Iteration 29: train mse: 0.48218812131436817 val mse: 0.4815909074527654\n",
      "(0.0, 0.8062442464070998)\n",
      "(0.85840000000000005, 0.57765816344615217)\n",
      "(-0.28610000000000002, 0.28757705408135947)\n",
      "(0.28610000000000002, -0.37339756276456515)\n",
      "(0.0, 0.82934279868232541)\n",
      "(0.0, 0.68378795026421746)\n",
      "(0.28610000000000002, 0.44968684661816871)\n",
      "(-0.28610000000000002, 0.72341623225279794)\n",
      "(0.0, -0.4068287761290848)\n",
      "(0.28610000000000002, -0.28096698181442104)\n",
      "Iteration 30 lower bound -12353.75119666331\n",
      "Iteration 30: train mse: 0.46338647240963515 val mse: 0.46192748489331675\n",
      "(0.0, 0.75948748473536676)\n",
      "(0.85840000000000005, 0.56259909606059866)\n",
      "(-0.28610000000000002, 0.27334838045579818)\n",
      "(0.28610000000000002, -0.35869652227713972)\n",
      "(0.0, 0.79673606012701359)\n",
      "(0.0, 0.63414083092999662)\n",
      "(0.28610000000000002, 0.45006654834114457)\n",
      "(-0.28610000000000002, 0.6880515093415519)\n",
      "(0.0, -0.37960435303751155)\n",
      "(0.28610000000000002, -0.26132428089095028)\n",
      "Iteration 31 lower bound -11999.07917299574\n",
      "Iteration 31: train mse: 0.4489381297640013 val mse: 0.44335515911107776\n",
      "(0.0, 0.70868703197145455)\n",
      "(0.85840000000000005, 0.54278311798018875)\n",
      "(-0.28610000000000002, 0.25902468982007393)\n",
      "(0.28610000000000002, -0.34003112152813098)\n",
      "(0.0, 0.76096110064546618)\n",
      "(0.0, 0.5835435178774987)\n",
      "(0.28610000000000002, 0.44572400599627232)\n",
      "(-0.28610000000000002, 0.65103993213705669)\n",
      "(0.0, -0.35536113487783927)\n",
      "(0.28610000000000002, -0.24430103891825045)\n",
      "Iteration 32 lower bound -12322.040570601413\n",
      "Iteration 32: train mse: 0.428028511626628 val mse: 0.4257617736235112\n",
      "(0.0, 0.65450635810145552)\n",
      "(0.85840000000000005, 0.52028085699876603)\n",
      "(-0.28610000000000002, 0.24640528243481935)\n",
      "(0.28610000000000002, -0.32160796486428372)\n",
      "(0.0, 0.72371986730072646)\n",
      "(0.0, 0.53696954456185841)\n",
      "(0.28610000000000002, 0.43886998887550888)\n",
      "(-0.28610000000000002, 0.61434346225133241)\n",
      "(0.0, -0.33237341336094506)\n",
      "(0.28610000000000002, -0.22550724095884486)\n",
      "Iteration 33 lower bound -11553.959584460456\n",
      "Iteration 33: train mse: 0.41729455159938317 val mse: 0.4091678024562759\n",
      "(0.0, 0.59604017374073681)\n",
      "(0.85840000000000005, 0.49643191335412906)\n",
      "(-0.28610000000000002, 0.23277564233795434)\n",
      "(0.28610000000000002, -0.29669106695704006)\n",
      "(0.0, 0.6847269438431921)\n",
      "(0.0, 0.49208625680046436)\n",
      "(0.28610000000000002, 0.42722638056878559)\n",
      "(-0.28610000000000002, 0.57801009063558217)\n",
      "(0.0, -0.31121656382927065)\n",
      "(0.28610000000000002, -0.20560655488486113)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34 lower bound -11040.849750006593\n",
      "Iteration 34: train mse: 0.399539221975332 val mse: 0.39346862231194163\n",
      "(0.0, 0.53858682990069884)\n",
      "(0.85840000000000005, 0.47854105115579471)\n",
      "(-0.28610000000000002, 0.22232547803669359)\n",
      "(0.28610000000000002, -0.26433227891549049)\n",
      "(0.0, 0.64867580172936712)\n",
      "(0.0, 0.45303901757458276)\n",
      "(0.28610000000000002, 0.41662090035101301)\n",
      "(-0.28610000000000002, 0.54793120002441376)\n",
      "(0.0, -0.28625589056886797)\n",
      "(0.28610000000000002, -0.17931716945523851)\n",
      "Iteration 35 lower bound -11137.384410325238\n",
      "Iteration 35: train mse: 0.3782641480008545 val mse: 0.37875705678596694\n",
      "(0.0, 0.48476306215367321)\n",
      "(0.85840000000000005, 0.46813312929598949)\n",
      "(-0.28610000000000002, 0.21617127932724989)\n",
      "(0.28610000000000002, -0.2227259486340869)\n",
      "(0.0, 0.61828793891104805)\n",
      "(0.0, 0.42078367186222987)\n",
      "(0.28610000000000002, 0.40888448278627243)\n",
      "(-0.28610000000000002, 0.52548093520338779)\n",
      "(0.0, -0.25442253042673812)\n",
      "(0.28610000000000002, -0.14468664428743891)\n",
      "Iteration 36 lower bound -10735.694231619424\n",
      "Iteration 36: train mse: 0.3681899565578966 val mse: 0.364899538399275\n",
      "(0.0, 0.43150008256067379)\n",
      "(0.85840000000000005, 0.46021226637976981)\n",
      "(-0.28610000000000002, 0.20962773018163389)\n",
      "(0.28610000000000002, -0.17821286139306733)\n",
      "(0.0, 0.58921983775170261)\n",
      "(0.0, 0.39133850814006255)\n",
      "(0.28610000000000002, 0.39963043078371419)\n",
      "(-0.28610000000000002, 0.50607898812682572)\n",
      "(0.0, -0.22083987761705443)\n",
      "(0.28610000000000002, -0.10337268473578587)\n",
      "Iteration 37 lower bound -10062.4995926806\n",
      "Iteration 37: train mse: 0.35487792507390176 val mse: 0.351917025673954\n",
      "(0.0, 0.37935146614821785)\n",
      "(0.85840000000000005, 0.45339711907058183)\n",
      "(-0.28610000000000002, 0.20279795349105267)\n",
      "(0.28610000000000002, -0.13127997302691258)\n",
      "(0.0, 0.56067317619904411)\n",
      "(0.0, 0.36433830721471022)\n",
      "(0.28610000000000002, 0.38915659984517992)\n",
      "(-0.28610000000000002, 0.48816985740028362)\n",
      "(0.0, -0.18653064264651556)\n",
      "(0.28610000000000002, -0.059443753695852627)\n",
      "Iteration 38 lower bound -9887.552033536485\n",
      "Iteration 38: train mse: 0.3451702824704061 val mse: 0.3395745422754789\n",
      "(0.0, 0.33022053876587204)\n",
      "(0.85840000000000005, 0.44511061184102435)\n",
      "(-0.28610000000000002, 0.19735892646149561)\n",
      "(0.28610000000000002, -0.081367530746905989)\n",
      "(0.0, 0.53177363428885127)\n",
      "(0.0, 0.33962393974501648)\n",
      "(0.28610000000000002, 0.37897421411841625)\n",
      "(-0.28610000000000002, 0.46937389919961248)\n",
      "(0.0, -0.15079953188409073)\n",
      "(0.28610000000000002, -0.014147980046764564)\n",
      "Iteration 39 lower bound -10124.66123421686\n",
      "Iteration 39: train mse: 0.3304611004915526 val mse: 0.3277837043886714\n",
      "(0.0, 0.27995452111003261)\n",
      "(0.85840000000000005, 0.43543202294682004)\n",
      "(-0.28610000000000002, 0.18895592152980997)\n",
      "(0.28610000000000002, -0.030287944045707615)\n",
      "(0.0, 0.50154424320193969)\n",
      "(0.0, 0.31427108280887361)\n",
      "(0.28610000000000002, 0.365250822713027)\n",
      "(-0.28610000000000002, 0.45024600301488932)\n",
      "(0.0, -0.11692297547707646)\n",
      "(0.28610000000000002, 0.029290510974918388)\n",
      "Iteration 40 lower bound -9594.126783457865\n",
      "Iteration 40: train mse: 0.32254912743016345 val mse: 0.31646260841041485\n",
      "(0.0, 0.22791195324628238)\n",
      "(0.85840000000000005, 0.4204435481451313)\n",
      "(-0.28610000000000002, 0.17671920581473716)\n",
      "(0.28610000000000002, 0.014763288300951836)\n",
      "(0.0, 0.46732092179888418)\n",
      "(0.0, 0.28734841151158347)\n",
      "(0.28610000000000002, 0.34712807784854338)\n",
      "(-0.28610000000000002, 0.42646466508497216)\n",
      "(0.0, -0.088074734536193258)\n",
      "(0.28610000000000002, 0.066192487997329283)\n",
      "Iteration 41 lower bound -10047.787468853021\n",
      "Iteration 41: train mse: 0.3120212866739698 val mse: 0.3055322906447403\n",
      "(0.0, 0.17336585763904133)\n",
      "(0.85840000000000005, 0.40121880958784517)\n",
      "(-0.28610000000000002, 0.15981117511722931)\n",
      "(0.28610000000000002, 0.054199484378730442)\n",
      "(0.0, 0.42964599818251992)\n",
      "(0.0, 0.257484737871777)\n",
      "(0.28610000000000002, 0.32445515466375685)\n",
      "(-0.28610000000000002, 0.39882915525305063)\n",
      "(0.0, -0.063662363804019878)\n",
      "(0.28610000000000002, 0.099059875834955569)\n",
      "Iteration 42 lower bound -10420.170549948714\n",
      "Iteration 42: train mse: 0.2953895767745859 val mse: 0.2951843382311653\n",
      "(0.0, 0.12195388454658003)\n",
      "(0.85840000000000005, 0.38321511529465152)\n",
      "(-0.28610000000000002, 0.14372648600828533)\n",
      "(0.28610000000000002, 0.091422031164956688)\n",
      "(0.0, 0.39377160724477622)\n",
      "(0.0, 0.22941730493022278)\n",
      "(0.28610000000000002, 0.30178220844007825)\n",
      "(-0.28610000000000002, 0.37282444810728349)\n",
      "(0.0, -0.039355438883170703)\n",
      "(0.28610000000000002, 0.13103556467696217)\n",
      "Iteration 43 lower bound -9716.640196444887\n",
      "Iteration 43: train mse: 0.2892847733633195 val mse: 0.2853136828463274\n",
      "(0.0, 0.074376740462604091)\n",
      "(0.85840000000000005, 0.36610498164106514)\n",
      "(-0.28610000000000002, 0.12877221176504822)\n",
      "(0.28610000000000002, 0.12670221799133635)\n",
      "(0.0, 0.35975022957260905)\n",
      "(0.0, 0.20339259630552595)\n",
      "(0.28610000000000002, 0.28003825415405381)\n",
      "(-0.28610000000000002, 0.34887775764231277)\n",
      "(0.0, -0.015563786651479153)\n",
      "(0.28610000000000002, 0.16030164705485056)\n",
      "Iteration 44 lower bound -8855.01315236334\n",
      "Iteration 44: train mse: 0.2790280212482246 val mse: 0.2758575228705108\n",
      "(0.0, 0.026915158593971332)\n",
      "(0.85840000000000005, 0.34576863996246748)\n",
      "(-0.28610000000000002, 0.11077255547958162)\n",
      "(0.28610000000000002, 0.1556157464882986)\n",
      "(0.0, 0.3238261014985801)\n",
      "(0.0, 0.17532482949603834)\n",
      "(0.28610000000000002, 0.25517507177318349)\n",
      "(-0.28610000000000002, 0.32242480678131941)\n",
      "(0.0, 0.003531702604519976)\n",
      "(0.28610000000000002, 0.18287572385062709)\n",
      "Iteration 45 lower bound -9338.756650142925\n",
      "Iteration 45: train mse: 0.2780521686412438 val mse: 0.2669092897185132\n",
      "(0.0, -0.014374558957819292)\n",
      "(0.85840000000000005, 0.32705640005285358)\n",
      "(-0.28610000000000002, 0.095385141225220155)\n",
      "(0.28610000000000002, 0.18213550990839716)\n",
      "(0.0, 0.29072475447151674)\n",
      "(0.0, 0.1503809660036439)\n",
      "(0.28610000000000002, 0.23222714254008142)\n",
      "(-0.28610000000000002, 0.29896612085802182)\n",
      "(0.0, 0.022326045273422395)\n",
      "(0.28610000000000002, 0.20351687459350948)\n",
      "Iteration 46 lower bound -8924.213731421722\n",
      "Iteration 46: train mse: 0.26382813421715323 val mse: 0.2589070130351784\n",
      "(0.0, -0.048272234796538013)\n",
      "(0.85840000000000005, 0.31044091254958039)\n",
      "(-0.28610000000000002, 0.083976887090041752)\n",
      "(0.28610000000000002, 0.20869703503541107)\n",
      "(0.0, 0.26086776487474345)\n",
      "(0.0, 0.13005125811509607)\n",
      "(0.28610000000000002, 0.2125492423283335)\n",
      "(-0.28610000000000002, 0.27753686245663195)\n",
      "(0.0, 0.043187290043923597)\n",
      "(0.28610000000000002, 0.22449200488475646)\n",
      "Iteration 47 lower bound -9192.901682400538\n",
      "Iteration 47: train mse: 0.2557734291638103 val mse: 0.2516457085569365\n",
      "(0.0, -0.077359726379614635)\n",
      "(0.85840000000000005, 0.29284645482913113)\n",
      "(-0.28610000000000002, 0.07314961528291429)\n",
      "(0.28610000000000002, 0.22899129808012336)\n",
      "(0.0, 0.23231778998115277)\n",
      "(0.0, 0.11076474101154329)\n",
      "(0.28610000000000002, 0.19264910801018739)\n",
      "(-0.28610000000000002, 0.25761556879802117)\n",
      "(0.0, 0.060255684504458541)\n",
      "(0.28610000000000002, 0.23955760999798961)\n",
      "Iteration 48 lower bound -9351.798868629721\n",
      "Iteration 48: train mse: 0.24516037301840551 val mse: 0.24524609540185327\n",
      "(0.0, -0.098844393815940748)\n",
      "(0.85840000000000005, 0.27840721624686215)\n",
      "(-0.28610000000000002, 0.066174638531890581)\n",
      "(0.28610000000000002, 0.2489690485837428)\n",
      "(0.0, 0.20885796260194142)\n",
      "(0.0, 0.096163794130674038)\n",
      "(0.28610000000000002, 0.17637058907003184)\n",
      "(-0.28610000000000002, 0.24190383912606692)\n",
      "(0.0, 0.079034331959250081)\n",
      "(0.28610000000000002, 0.25446471530624015)\n",
      "Iteration 49 lower bound -8464.561028199092\n",
      "Iteration 49: train mse: 0.24260020128015583 val mse: 0.23960211906536322\n",
      "(0.0, -0.10909434501910358)\n",
      "(0.85840000000000005, 0.26990623657060048)\n",
      "(-0.28610000000000002, 0.066983869258971374)\n",
      "(0.28610000000000002, 0.27047957754963969)\n",
      "(0.0, 0.1940444183628654)\n",
      "(0.0, 0.090195287213686165)\n",
      "(0.28610000000000002, 0.16755980025728703)\n",
      "(-0.28610000000000002, 0.23398962892414957)\n",
      "(0.0, 0.10181439566074391)\n",
      "(0.28610000000000002, 0.27129030591897768)\n",
      "Iteration 50 lower bound -8986.939427976238\n",
      "Iteration 50: train mse: 0.23634843502286046 val mse: 0.23458684339016292\n",
      "(0.0, -0.11403333321322565)\n",
      "(0.85840000000000005, 0.26129472862407949)\n",
      "(-0.28610000000000002, 0.070150638086060424)\n",
      "(0.28610000000000002, 0.2884016214897957)\n",
      "(0.0, 0.18163133710802035)\n",
      "(0.0, 0.087260848447836395)\n",
      "(0.28610000000000002, 0.16067409524260454)\n",
      "(-0.28610000000000002, 0.22645661390818944)\n",
      "(0.0, 0.12363947593491945)\n",
      "(0.28610000000000002, 0.28538151897793795)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51 lower bound -9176.130386572291\n",
      "Iteration 51: train mse: 0.23453534460161457 val mse: 0.23031843322387552\n",
      "(0.0, -0.11758938171809277)\n",
      "(0.85840000000000005, 0.24728786911871542)\n",
      "(-0.28610000000000002, 0.071511702355456355)\n",
      "(0.28610000000000002, 0.29705783083671766)\n",
      "(0.0, 0.16679332395671098)\n",
      "(0.0, 0.083294594864312121)\n",
      "(0.28610000000000002, 0.15161119058702716)\n",
      "(-0.28610000000000002, 0.21370722951495505)\n",
      "(0.0, 0.1392742777428076)\n",
      "(0.28610000000000002, 0.29135495164657688)\n",
      "Iteration 52 lower bound -7918.556255304618\n",
      "Iteration 52: train mse: 0.2236774969467112 val mse: 0.22668383971181036\n",
      "(0.0, -0.11508439413885074)\n",
      "(0.85840000000000005, 0.23355940857362331)\n",
      "(-0.28610000000000002, 0.075871825796179751)\n",
      "(0.28610000000000002, 0.30359619522050885)\n",
      "(0.0, 0.15461330403767728)\n",
      "(0.0, 0.08332092728753078)\n",
      "(0.28610000000000002, 0.14572856626865102)\n",
      "(-0.28610000000000002, 0.19981139816183821)\n",
      "(0.0, 0.15565828792241049)\n",
      "(0.28610000000000002, 0.29655359512868679)\n",
      "Iteration 53 lower bound -8430.723301856444\n",
      "Iteration 53: train mse: 0.22452832524570807 val mse: 0.22352359096671653\n",
      "(0.0, -0.10732160160125837)\n",
      "(0.85840000000000005, 0.22451515731318974)\n",
      "(-0.28610000000000002, 0.081775820654668374)\n",
      "(0.28610000000000002, 0.30987937689533501)\n",
      "(0.0, 0.14875120745022388)\n",
      "(0.0, 0.086500643391559606)\n",
      "(0.28610000000000002, 0.14267222272139024)\n",
      "(-0.28610000000000002, 0.19296558472246733)\n",
      "(0.0, 0.17317030264416272)\n",
      "(0.28610000000000002, 0.30153982425744064)\n",
      "Iteration 54 lower bound -8040.606071075132\n",
      "Iteration 54: train mse: 0.2271848649798706 val mse: 0.2207051438560122\n",
      "(0.0, -0.092587456564595966)\n",
      "(0.85840000000000005, 0.2159215531127146)\n",
      "(-0.28610000000000002, 0.091531770602138213)\n",
      "(0.28610000000000002, 0.31248070993350641)\n",
      "(0.0, 0.14601754291131372)\n",
      "(0.0, 0.094165806646386685)\n",
      "(0.28610000000000002, 0.14381229757198227)\n",
      "(-0.28610000000000002, 0.18805629718975608)\n",
      "(0.0, 0.18891201285753903)\n",
      "(0.28610000000000002, 0.3035781767951356)\n",
      "Iteration 55 lower bound -7321.116846682016\n",
      "Iteration 55: train mse: 0.2206781702321757 val mse: 0.21802804148033939\n",
      "(0.0, -0.086515244290611901)\n",
      "(0.85840000000000005, 0.1976913006386225)\n",
      "(-0.28610000000000002, 0.090351263000146315)\n",
      "(0.28610000000000002, 0.30368219381626915)\n",
      "(0.0, 0.13421880009555076)\n",
      "(0.0, 0.092854030903170653)\n",
      "(0.28610000000000002, 0.13636304328013515)\n",
      "(-0.28610000000000002, 0.1712135663905375)\n",
      "(0.0, 0.19446056826250199)\n",
      "(0.28610000000000002, 0.29491404193080195)\n",
      "Iteration 56 lower bound -7875.358125153287\n",
      "Iteration 56: train mse: 0.21754926367413052 val mse: 0.21563271824224653\n",
      "(0.0, -0.10485459925359093)\n",
      "(0.85840000000000005, 0.16157834881400554)\n",
      "(-0.28610000000000002, 0.063159381212896729)\n",
      "(0.28610000000000002, 0.27353834132199401)\n",
      "(0.0, 0.10378853560860997)\n",
      "(0.0, 0.067615846595674142)\n",
      "(0.28610000000000002, 0.10588382967100214)\n",
      "(-0.28610000000000002, 0.13813069509719317)\n",
      "(0.0, 0.17809633872988079)\n",
      "(0.28610000000000002, 0.26429771966210708)\n",
      "Iteration 57 lower bound -7375.814601218918\n",
      "Iteration 57: train mse: 0.21993768315495923 val mse: 0.21396204908588237\n",
      "(0.0, -0.113323188590713)\n",
      "(0.85840000000000005, 0.13112854717602912)\n",
      "(-0.28610000000000002, 0.044285812781651043)\n",
      "(0.28610000000000002, 0.24796810478669334)\n",
      "(0.0, 0.080888156847594692)\n",
      "(0.0, 0.051456361142520411)\n",
      "(0.28610000000000002, 0.0844018906189411)\n",
      "(-0.28610000000000002, 0.10924087061666865)\n",
      "(0.0, 0.16846174057736726)\n",
      "(0.28610000000000002, 0.23926604487263048)\n",
      "Iteration 58 lower bound -7052.399438565378\n",
      "Iteration 58: train mse: 0.21532567309535258 val mse: 0.21274029017656737\n",
      "(0.0, -0.11694600126060953)\n",
      "(0.85840000000000005, 0.10473614469691805)\n",
      "(-0.28610000000000002, 0.028168601261639357)\n",
      "(0.28610000000000002, 0.22476163263213794)\n",
      "(0.0, 0.062464167349246383)\n",
      "(0.0, 0.039638767248178303)\n",
      "(0.28610000000000002, 0.067064926293626759)\n",
      "(-0.28610000000000002, 0.083179131812148116)\n",
      "(0.0, 0.16202070088705639)\n",
      "(0.28610000000000002, 0.21704335054974505)\n",
      "Iteration 59 lower bound -8715.144020425347\n",
      "Iteration 59: train mse: 0.21568817333907983 val mse: 0.21190840548548961\n",
      "(0.0, -0.10644517119133845)\n",
      "(0.85840000000000005, 0.095190939366929933)\n",
      "(-0.28610000000000002, 0.025363476824459597)\n",
      "(0.28610000000000002, 0.21750340116555381)\n",
      "(0.0, 0.060622654227697602)\n",
      "(0.0, 0.04307929709546271)\n",
      "(0.28610000000000002, 0.064818636211915789)\n",
      "(-0.28610000000000002, 0.073417494490499211)\n",
      "(0.0, 0.17104795962555777)\n",
      "(0.28610000000000002, 0.21047199509195308)\n",
      "Iteration 60 lower bound -6787.967833747943\n",
      "Iteration 60: train mse: 0.22040752162737545 val mse: 0.21125666036133178\n",
      "(0.0, -0.082610548286014079)\n",
      "(0.85840000000000005, 0.099075779994368396)\n",
      "(-0.28610000000000002, 0.035454162218702595)\n",
      "(0.28610000000000002, 0.22286316954117988)\n",
      "(0.0, 0.073014509920685478)\n",
      "(0.0, 0.0608031183539815)\n",
      "(0.28610000000000002, 0.076743890144062979)\n",
      "(-0.28610000000000002, 0.07656108730511238)\n",
      "(0.0, 0.1925568209497982)\n",
      "(0.28610000000000002, 0.2163675165976473)\n",
      "Iteration 61 lower bound -7966.2118828898665\n",
      "Iteration 61: train mse: 0.21565015963883635 val mse: 0.21102380305576812\n",
      "(0.0, -0.045554898352240338)\n",
      "(0.85840000000000005, 0.11563982913497151)\n",
      "(-0.28610000000000002, 0.058268174069318723)\n",
      "(0.28610000000000002, 0.23933958017571114)\n",
      "(0.0, 0.099417604678900484)\n",
      "(0.0, 0.091722033464445685)\n",
      "(0.28610000000000002, 0.10186778622420319)\n",
      "(-0.28610000000000002, 0.094704466247256508)\n",
      "(0.0, 0.22438897537037464)\n",
      "(0.28610000000000002, 0.23283878299332769)\n"
     ]
    }
   ],
   "source": [
    "# Specify inference problem by its unnormalized log-posterior.\n",
    "from IPython import display\n",
    "\n",
    "import pylab as pl\n",
    "\n",
    "inputs, targets = (x_train, y_train)\n",
    "\n",
    "rbf = lambda x: 0.6*np.exp(-x**2)\n",
    "relu = lambda x: np.maximum(x, 0.01*x)\n",
    "tanh = lambda x: 0.6*np.tanh(x)\n",
    "num_weights, predictions, logprob = \\\n",
    "    make_nn_funs(layer_sizes=[x_train.shape[1], 10, 10, y_train.shape[1]], L2_reg=1.5,\n",
    "                 noise_variance=0.01, nonlinearity=tanh)\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "n_epoch = 5\n",
    "\n",
    "num_batches = int(np.ceil(len(inputs) / batch_size))\n",
    "\n",
    "glob_params = None\n",
    "\n",
    "def batch_indices(iter):\n",
    "    batchidx = iter % num_batches\n",
    "    return slice(batchidx * batch_size, (batchidx+1) * batch_size)\n",
    "\n",
    "\n",
    "def log_posterior(weights, t):\n",
    "    cur_batch_idx = batch_indices(t)\n",
    "    cur_inp = inputs[cur_batch_idx]\n",
    "    cur_tar = targets[cur_batch_idx]\n",
    "    return logprob(weights, cur_inp, cur_tar)\n",
    "\n",
    "# Build variational objective.\n",
    "objective, gradient, unpack_params = \\\n",
    "    black_box_variational_inference(log_posterior, num_weights,\n",
    "                                    num_samples=50)\n",
    "\n",
    "\n",
    "def callback(params, t, g):\n",
    "    global glob_params\n",
    "    print(\"Iteration {} lower bound {}\".format(t, -objective(params, t)))\n",
    "    \n",
    "    glob_params = params\n",
    "\n",
    "    # Sample functions from posterior.\n",
    "    rs = npr.RandomState(0)\n",
    "    mean, log_std = unpack_params(params)\n",
    "    #rs = npr.RandomState(0)\n",
    "    sample_weights = rs.randn(10, num_weights) * np.exp(log_std) + mean\n",
    "\n",
    "    val_outputs = np.mean(predictions(sample_weights, x_val), axis=0)\n",
    "#     print(val_outputs.shape)\n",
    "#     import sys\n",
    "#     sys.exit()\n",
    "    \n",
    "    mse = np.mean( (val_outputs - y_val)**2)\n",
    "    \n",
    "    train_idx_samp = np.random.choice(train_idx, 10000, replace=False)\n",
    "    \n",
    "    xt = x_train[train_idx_samp]\n",
    "    yt = y_train[train_idx_samp]\n",
    "    \n",
    "    train_outputs = np.mean(predictions(sample_weights, xt), axis=0)\n",
    "    \n",
    "    mse_train = np.mean((train_outputs - yt)**2)\n",
    "    \n",
    "    print(\"Iteration {}: train mse: {} val mse: {}\".format(t, mse_train, mse))\n",
    "    for tup in zip(y_val[:10, 29],val_outputs[:10, 29]):\n",
    "        print(tup)\n",
    "\n",
    "#     # Plot data and functions.\n",
    "#     if (t+1) % 10 == 0:\n",
    "#         pl.plot(inputs.ravel(), targets.ravel(), 'bx')\n",
    "#         pl.plot(plot_inputs, outputs[:, :, 0].T)\n",
    "#     #     display.clear_output(wait=True)\n",
    "#         pl.show()\n",
    "#         print(np.squeeze(outputs).shape)\n",
    "\n",
    "# Initialize variational parameters\n",
    "rs = npr.RandomState(0)\n",
    "init_mean    = rs.randn(num_weights)\n",
    "init_log_std = -5 * np.ones(num_weights)\n",
    "# init_mean, init_log_std = unpack_params(glob_params)\n",
    "init_var_params = np.concatenate([init_mean, init_log_std])\n",
    "\n",
    "print(\"Optimizing variational parameters...\")\n",
    "variational_params = adam(gradient, init_var_params,\n",
    "                          step_size=0.05, num_iters=62, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('glob_params.npy', glob_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean, log_std = unpack_params(glob_params)\n",
    "sample_weights = rs.randn(10, num_weights) * np.exp(log_std) + mean\n",
    "val_outputs = np.mean(predictions(sample_weights, x_val), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:my_root]",
   "language": "python",
   "name": "conda-env-my_root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
