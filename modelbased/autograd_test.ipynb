{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from autograd import grad\n",
    "from autograd.misc.optimizers import adam\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albumin', 'Arterial_BE', 'Arterial_lactate', 'Arterial_pH', 'BUN', 'CO2_mEqL', 'Calcium', 'Chloride', 'Creatinine', 'DiaBP', 'FiO2_1', 'GCS', 'Glucose', 'HCO3', 'HR', 'Hb', 'INR', 'Ionised_Ca', 'Magnesium', 'MeanBP', 'PT', 'PTT', 'PaO2_FiO2', 'Platelets_count', 'Potassium', 'RR', 'SGOT', 'SGPT', 'SIRS', 'SOFA', 'Shock_Index', 'Sodium', 'SpO2', 'SysBP', 'Temp_C', 'Total_bili', 'WBC_count', 'Weight_kg', 'age', 'elixhauser', 'gender', 'mechvent', 'output_4hourly', 'output_total', 'paCO2', 'paO2', 're_admission', 'bloc']\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "with open('../data/state_features.txt') as f:\n",
    "    state_features = f.read().split()\n",
    "print (state_features)\n",
    "print (len(state_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/rl_train_set_unscaled.csv')\n",
    "df_val = pd.read_csv('../data/rl_val_set_unscaled.csv')\n",
    "df_test = pd.read_csv('../data/rl_test_set_unscaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# let the mortality labels  be -1 and 1: -1 for survival\n",
    "df_train['died_in_hosp'][df_train['died_in_hosp'] == 0] = -1\n",
    "df_val['died_in_hosp'][df_val['died_in_hosp'] == 0] = -1\n",
    "df_test['died_in_hosp'][df_test['died_in_hosp'] == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_feat = list(np.loadtxt('../data/state_features_pred.txt', dtype=str))\n",
    "target_feat.append('died_in_hosp')\n",
    "\n",
    "cur_feat = list(np.loadtxt('../data/state_features.txt', dtype=str))\n",
    "cur_feat.append('iv_input')\n",
    "cur_feat.append('vaso_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define an action mapping - how to get an id representing the action from the (iv,vaso) tuple\n",
    "action_map = {}\n",
    "count = 0\n",
    "for iv in range(5):\n",
    "    for vaso in range(5):\n",
    "        action_map[(iv,vaso)] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_action_map = {}\n",
    "for iv in range(5):\n",
    "    for vaso in range(5):\n",
    "        inv_action_map[5*iv+vaso] = [iv,vaso]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# Change the actions: {0: -0.8, 1: -0.4, 2: 0, 3: 0.4, 4: 0.8}. This will help for the policy search, which\n",
    "# uses a continuous action space\n",
    "lookup ={0:-0.8, 1:-0.4, 2:0, 3:0.4, 4:0.8}\n",
    "for dfm in (df_train, df_val, df_test):\n",
    "    for iv_input in range(5):\n",
    "        val = lookup[iv_input]\n",
    "        dfm['iv_input'][dfm['iv_input'] == iv_input] = val\n",
    "    for vaso_input in range(5):\n",
    "        val = lookup[vaso_input]\n",
    "        dfm['vaso_input'][dfm['vaso_input'] == vaso_input] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  X: (states, actions)\n",
    "#  Y: (difference between next state and current state (zeros if end of trajectory), mortality)\n",
    "def make_data(df_in):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for count,i in enumerate(df_in.index):\n",
    "        cur_state = df_in.loc[i,cur_feat]\n",
    "        if i != df_in.index[-1]:\n",
    "            # if not terminal step in trajectory             \n",
    "            if df_in.loc[i, 'icustayid'] == df_in.loc[i+1, 'icustayid']:\n",
    "                target = df_in.loc[i + 1, target_feat] - df_in.loc[i, target_feat]\n",
    "                target[-1] = df_in.loc[i, 'died_in_hosp']\n",
    "                Y.append(target)\n",
    "                X.append(cur_state)\n",
    "\n",
    "        if count % 10000 == 0 and count > 0:\n",
    "            print(count)\n",
    "\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train\n",
      "Loaded val\n",
      "Loaded test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dire = 'converted_data/'\n",
    "if not os.path.exists(dire):\n",
    "    os.makedirs(dire)\n",
    "    \n",
    "if not os.path.exists(dire + 'X_train.txt'):\n",
    "    x_train_nohist, y_train_nohist = make_data(df_train)\n",
    "    np.savetxt(dire + 'X_train.txt',x_train_nohist,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_train.txt',y_train_nohist,fmt='%5.4f')\n",
    "    print(\"Saved train\")\n",
    "else:\n",
    "    x_train_nohist = np.loadtxt(dire + 'X_train.txt')\n",
    "    y_train_nohist = np.loadtxt(dire + 'Y_train.txt')\n",
    "    print (\"Loaded train\")\n",
    "\n",
    "if not os.path.exists(dire + 'X_val.txt'):\n",
    "    x_val_nohist,y_val_nohist = make_data(df_val)\n",
    "    np.savetxt(dire + 'X_val.txt',x_val_nohist,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_val.txt',y_val_nohist,fmt='%5.4f')\n",
    "    print (\"Saved val\")\n",
    "else:\n",
    "    x_val_nohist = np.loadtxt(dire + 'X_val.txt')\n",
    "    y_val_nohist = np.loadtxt(dire + 'Y_val.txt')\n",
    "    print (\"Loaded val\")\n",
    "    \n",
    "if not os.path.exists(dire + 'X_test.txt'):\n",
    "    x_test_nohist, y_test_nohist = make_data(df_test)\n",
    "    np.savetxt(dire + 'X_test.txt',x_test_nohist,fmt='%5.4f')\n",
    "    np.savetxt(dire +    'Y_test.txt',y_test_nohist,fmt='%5.4f')\n",
    "    print (\"Saved test\")\n",
    "else:\n",
    "    x_test_nohist = np.loadtxt(dire + 'X_test.txt')\n",
    "    y_test_nohist = np.loadtxt(dire + 'Y_test.txt')\n",
    "    print (\"Loaded test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train\n",
      "Loaded val\n",
      "Loaded test\n"
     ]
    }
   ],
   "source": [
    "dire = 'converted_data/'\n",
    "if not os.path.exists(dire):\n",
    "    os.makedirs(dire)\n",
    "    \n",
    "if not os.path.exists(dire + 'X_train_hist.txt'):\n",
    "    x_train_hist, y_train_hist = make_data_history(df_train)\n",
    "    np.savetxt(dire + 'X_train_hist.txt',x_train_hist,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_train_hist.txt',y_train_hist,fmt='%5.4f')\n",
    "    print (\"Saved train\")\n",
    "else:\n",
    "    x_train_hist = np.loadtxt(dire + 'X_train_hist.txt')\n",
    "    y_train_hist = np.loadtxt(dire + 'Y_train_hist.txt')\n",
    "    print (\"Loaded train\")\n",
    "\n",
    "if not os.path.exists(dire + 'X_val_hist.txt'):\n",
    "    x_val_hist,y_val_hist = make_data_history(df_val)\n",
    "    np.savetxt(dire + 'X_val_hist.txt',x_val_hist,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_val_hist.txt',y_val_hist,fmt='%5.4f')\n",
    "    print (\"Saved val\")\n",
    "else:\n",
    "    x_val_hist = np.loadtxt(dire + 'X_val_hist.txt')\n",
    "    y_val_hist = np.loadtxt(dire + 'Y_val_hist.txt')\n",
    "    print (\"Loaded val\")\n",
    "    \n",
    "if not os.path.exists(dire + 'X_test_hist.txt'):\n",
    "    x_test_hist, y_test_hist = make_data_history(df_test)\n",
    "    np.savetxt(dire + 'X_test_hist.txt',x_test_hist,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_test_hist.txt',y_test_hist,fmt='%5.4f')\n",
    "    print (\"Saved test\")\n",
    "else:\n",
    "    x_test_hist = np.loadtxt(dire + 'X_test_hist.txt')\n",
    "    y_test_hist = np.loadtxt(dire + 'Y_test_hist.txt')\n",
    "    print (\"Loaded test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train_hist\n",
    "y_train = y_train_hist\n",
    "\n",
    "x_val = x_val_hist\n",
    "y_val = y_val_hist\n",
    "\n",
    "x_test = x_test_hist\n",
    "y_test = y_test_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119555, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get random sample of train/test for this model\n",
    "train_idx = np.random.permutation(len(x_train))\n",
    "val_idx = np.random.permutation(len(x_val))\n",
    "\n",
    "x_train = x_train[train_idx]\n",
    "y_train = y_train[train_idx]\n",
    "\n",
    "x_val = x_val[val_idx]\n",
    "y_val = y_val[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # only predict sofa\n",
    "# y_train = np.expand_dims(y_train[:, 29], 1)\n",
    "# y_val = np.expand_dims(y_val[:,29],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def black_box_variational_inference(logprob, D, num_samples):\n",
    "    \"\"\"Implements http://arxiv.org/abs/1401.0118, and uses the\n",
    "    local reparameterization trick from http://arxiv.org/abs/1506.02557\"\"\"\n",
    "\n",
    "    def unpack_params(params):\n",
    "        # Variational dist is a diagonal Gaussian.\n",
    "        mean, log_std = params[:D], params[D:]\n",
    "        return mean, log_std\n",
    "\n",
    "    def gaussian_entropy(log_std):\n",
    "        return 0.5 * D * (1.0 + np.log(2*np.pi)) + np.sum(log_std)\n",
    "\n",
    "    rs = npr.RandomState(0)\n",
    "    def variational_objective(params, t):\n",
    "        \"\"\"Provides a stochastic estimate of the variational lower bound.\"\"\"\n",
    "        mean, log_std = unpack_params(params)\n",
    "        samples = rs.randn(num_samples, D) * np.exp(log_std) + mean\n",
    "        lower_bound = gaussian_entropy(log_std) + np.mean(logprob(samples, t))\n",
    "        return -lower_bound\n",
    "\n",
    "    gradient = grad(variational_objective)\n",
    "\n",
    "    return variational_objective, gradient, unpack_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_nn_funs(layer_sizes, L2_reg, noise_variance, nonlinearity=np.tanh):\n",
    "    \"\"\"These functions implement a standard multi-layer perceptron,\n",
    "    vectorized over both training examples and weight samples.\"\"\"\n",
    "    shapes = list(zip(layer_sizes[:-1], layer_sizes[1:]))\n",
    "    num_weights = sum((m+1)*n for m, n in shapes)\n",
    "\n",
    "    def unpack_layers(weights):\n",
    "        num_weight_sets = len(weights)\n",
    "        for m, n in shapes:\n",
    "            yield weights[:, :m*n]     .reshape((num_weight_sets, m, n)),\\\n",
    "                  weights[:, m*n:m*n+n].reshape((num_weight_sets, 1, n))\n",
    "            weights = weights[:, (m+1)*n:]\n",
    "\n",
    "    def predictions(weights, inputs):\n",
    "        \"\"\"weights is shape (num_weight_samples x num_weights)\n",
    "           inputs  is shape (num_datapoints x D)\"\"\"\n",
    "        inputs = np.expand_dims(inputs, 0)\n",
    "        for W, b in unpack_layers(weights):\n",
    "            outputs = np.einsum('mnd,mdo->mno', inputs, W) + b\n",
    "            inputs = nonlinearity(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def logprob(weights, inputs, targets):\n",
    "        log_prior = -L2_reg * np.sum(weights**2, axis=1)\n",
    "        preds = predictions(weights, inputs)\n",
    "        log_lik = -np.sum((preds - targets)**2, axis=1)[:, 0] / noise_variance\n",
    "        return log_prior + log_lik\n",
    "\n",
    "    return num_weights, predictions, logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing variational parameters...\n",
      "Iteration 0 lower bound -8415512.43597\n",
      "Iteration 0 val mse 16.2800800863\n",
      "Iteration 1 lower bound -5657292.01343\n",
      "Iteration 1 val mse 15.06208813\n",
      "Iteration 2 lower bound -5446365.52002\n",
      "Iteration 2 val mse 14.0220880141\n",
      "Iteration 3 lower bound -4283670.81052\n",
      "Iteration 3 val mse 12.8579109893\n",
      "Iteration 4 lower bound -3470105.28335\n",
      "Iteration 4 val mse 11.7164344725\n",
      "Iteration 5 lower bound -3336549.19719\n",
      "Iteration 5 val mse 10.7373255251\n",
      "Iteration 6 lower bound -3441150.81132\n",
      "Iteration 6 val mse 9.87003918964\n",
      "Iteration 7 lower bound -3176898.3926\n",
      "Iteration 7 val mse 9.06920879358\n",
      "Iteration 8 lower bound -2685209.49385\n",
      "Iteration 8 val mse 8.32821369825\n",
      "Iteration 9 lower bound -2462161.11643\n",
      "Iteration 9 val mse 7.6730106855\n",
      "Iteration 10 lower bound -2416967.53738\n",
      "Iteration 10 val mse 7.07703474228\n",
      "Iteration 11 lower bound -2260372.31278\n",
      "Iteration 11 val mse 6.5027117881\n",
      "Iteration 12 lower bound -2015882.30246\n",
      "Iteration 12 val mse 5.95548754902\n",
      "Iteration 13 lower bound -1788644.99204\n",
      "Iteration 13 val mse 5.43885856483\n",
      "Iteration 14 lower bound -1655213.91457\n",
      "Iteration 14 val mse 4.97806606417\n",
      "Iteration 15 lower bound -1518172.87388\n",
      "Iteration 15 val mse 4.53862566872\n",
      "Iteration 16 lower bound -1536715.2942\n",
      "Iteration 16 val mse 4.14657154758\n",
      "Iteration 17 lower bound -1405280.56766\n",
      "Iteration 17 val mse 3.78821328517\n",
      "Iteration 18 lower bound -1255381.89266\n",
      "Iteration 18 val mse 3.46007812922\n",
      "Iteration 19 lower bound -1199194.12692\n",
      "Iteration 19 val mse 3.16102098251\n",
      "Iteration 20 lower bound -1122169.57907\n",
      "Iteration 20 val mse 2.8860093479\n",
      "Iteration 21 lower bound -1139603.34181\n",
      "Iteration 21 val mse 2.63980565039\n",
      "Iteration 22 lower bound -1025707.10065\n",
      "Iteration 22 val mse 2.42095439579\n",
      "Iteration 23 lower bound -959881.379743\n",
      "Iteration 23 val mse 2.22513200452\n",
      "Iteration 24 lower bound -897150.483432\n",
      "Iteration 24 val mse 2.05118978197\n",
      "Iteration 25 lower bound -877078.598026\n",
      "Iteration 25 val mse 1.89578411566\n",
      "Iteration 26 lower bound -819227.646036\n",
      "Iteration 26 val mse 1.7562620892\n",
      "Iteration 27 lower bound -786448.348727\n",
      "Iteration 27 val mse 1.62792523318\n",
      "Iteration 28 lower bound -726500.026618\n",
      "Iteration 28 val mse 1.50849367489\n",
      "Iteration 29 lower bound -676316.940598\n",
      "Iteration 29 val mse 1.39804503341\n",
      "Iteration 30 lower bound -682061.504034\n",
      "Iteration 30 val mse 1.29933677909\n",
      "Iteration 31 lower bound -668588.363476\n",
      "Iteration 31 val mse 1.20915582513\n",
      "Iteration 32 lower bound -594289.166021\n",
      "Iteration 32 val mse 1.12846537872\n",
      "Iteration 33 lower bound -562831.378931\n",
      "Iteration 33 val mse 1.05365367715\n",
      "Iteration 34 lower bound -551017.369694\n",
      "Iteration 34 val mse 0.985054421944\n",
      "Iteration 35 lower bound -526266.72351\n",
      "Iteration 35 val mse 0.921750411925\n",
      "Iteration 36 lower bound -549555.207617\n",
      "Iteration 36 val mse 0.861826431715\n",
      "Iteration 37 lower bound -510012.834899\n",
      "Iteration 37 val mse 0.804234432122\n",
      "Iteration 38 lower bound -483514.346553\n",
      "Iteration 38 val mse 0.74913664262\n",
      "Iteration 39 lower bound -474992.41066\n",
      "Iteration 39 val mse 0.696989120625\n",
      "Iteration 40 lower bound -454027.640489\n",
      "Iteration 40 val mse 0.647571033265\n",
      "Iteration 41 lower bound -440447.808307\n",
      "Iteration 41 val mse 0.603262376706\n",
      "Iteration 42 lower bound -413875.481729\n",
      "Iteration 42 val mse 0.56442266201\n",
      "Iteration 43 lower bound -404573.321423\n",
      "Iteration 43 val mse 0.529582951583\n"
     ]
    }
   ],
   "source": [
    "# Specify inference problem by its unnormalized log-posterior.\n",
    "from IPython import display\n",
    "\n",
    "import pylab as pl\n",
    "\n",
    "inputs, targets = (x_train, y_train)\n",
    "\n",
    "rbf = lambda x: 1.5*np.exp(-x**2)\n",
    "relu = lambda x: np.maximum(x, 0.01*x)\n",
    "tanh = lambda x: 2*np.tanh(x)\n",
    "num_weights, predictions, logprob = \\\n",
    "    make_nn_funs(layer_sizes=[x_train.shape[1], 32, 32, y_train.shape[1]], L2_reg=1.5,\n",
    "                 noise_variance=0.01, nonlinearity=rbf)\n",
    "\n",
    "batch_size = 10000\n",
    "\n",
    "n_epoch = 5\n",
    "\n",
    "num_batches = int(np.ceil(len(inputs) / batch_size))\n",
    "\n",
    "glob_params = None\n",
    "\n",
    "def batch_indices(iter):\n",
    "    batchidx = iter % num_batches\n",
    "    return slice(batchidx * batch_size, (batchidx+1) * batch_size)\n",
    "\n",
    "\n",
    "def log_posterior(weights, t):\n",
    "    cur_batch_idx = batch_indices(t)\n",
    "    cur_inp = inputs[cur_batch_idx]\n",
    "    cur_tar = targets[cur_batch_idx]\n",
    "    return logprob(weights, cur_inp, cur_tar)\n",
    "\n",
    "# Build variational objective.\n",
    "objective, gradient, unpack_params = \\\n",
    "    black_box_variational_inference(log_posterior, num_weights,\n",
    "                                    num_samples=20)\n",
    "\n",
    "\n",
    "def callback(params, t, g):\n",
    "    global glob_params\n",
    "    print(\"Iteration {} lower bound {}\".format(t, -objective(params, t)))\n",
    "    \n",
    "    glob_params = params\n",
    "\n",
    "    # Sample functions from posterior.\n",
    "    rs = npr.RandomState(0)\n",
    "    mean, log_std = unpack_params(params)\n",
    "    #rs = npr.RandomState(0)\n",
    "    sample_weights = rs.randn(10, num_weights) * np.exp(log_std) + mean\n",
    "\n",
    "    val_outputs = np.mean(predictions(sample_weights, x_val), axis=0)\n",
    "#     print(val_outputs.shape)\n",
    "#     import sys\n",
    "#     sys.exit()\n",
    "    \n",
    "    mse = np.mean( (val_outputs - y_val)**2)\n",
    "    \n",
    "    print(\"Iteration {} val mse {}\".format(t, mse))\n",
    "\n",
    "#     # Plot data and functions.\n",
    "#     if (t+1) % 10 == 0:\n",
    "#         pl.plot(inputs.ravel(), targets.ravel(), 'bx')\n",
    "#         pl.plot(plot_inputs, outputs[:, :, 0].T)\n",
    "#     #     display.clear_output(wait=True)\n",
    "#         pl.show()\n",
    "#         print(np.squeeze(outputs).shape)\n",
    "\n",
    "# Initialize variational parameters\n",
    "rs = npr.RandomState(0)\n",
    "init_mean    = rs.randn(num_weights)\n",
    "init_log_std = -5 * np.ones(num_weights)\n",
    "# init_mean, init_log_std = unpack_params(glob_params)\n",
    "init_var_params = np.concatenate([init_mean, init_log_std])\n",
    "\n",
    "print(\"Optimizing variational parameters...\")\n",
    "variational_params = adam(gradient, init_var_params,\n",
    "                          step_size=0.05, num_iters=50 * num_batches, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('glob_params.npy', glob_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
