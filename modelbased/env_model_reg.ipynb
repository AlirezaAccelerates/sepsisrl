{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import _pickle as pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/rl_train_set_unscaled.csv')\n",
    "df_val = pd.read_csv('../data/rl_val_set_unscaled.csv')\n",
    "df_test = pd.read_csv('../data/rl_test_set_unscaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/csail.mit.edu/u/a/araghu/.conda/envs/my_root/lib/python3.5/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/afs/csail.mit.edu/u/a/araghu/.conda/envs/my_root/lib/python3.5/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/afs/csail.mit.edu/u/a/araghu/.conda/envs/my_root/lib/python3.5/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# let the mortality labels  be -1 and 1: -1 for survival\n",
    "df_train['died_in_hosp'][df_train['died_in_hosp'] == 0] = -1\n",
    "df_val['died_in_hosp'][df_val['died_in_hosp'] == 0] = -1\n",
    "df_test['died_in_hosp'][df_test['died_in_hosp'] == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_feat = list(np.loadtxt('../data/state_features_pred.txt', dtype=str))\n",
    "target_feat.append('died_in_hosp')\n",
    "\n",
    "cur_feat = list(np.loadtxt('../data/state_features.txt', dtype=str))\n",
    "cur_feat.append('iv_input_norm')\n",
    "cur_feat.append('vaso_input_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define an action mapping - how to get an id representing the action from the (iv,vaso) tuple\n",
    "action_map = {}\n",
    "count = 0\n",
    "for iv in range(5):\n",
    "    for vaso in range(5):\n",
    "        action_map[(iv,vaso)] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_action_map = {}\n",
    "for iv in range(5):\n",
    "    for vaso in range(5):\n",
    "        inv_action_map[5*iv+vaso] = [iv,vaso]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iv_mean = df_train['iv_input'].mean()\n",
    "iv_std = df_train['iv_input'].std()\n",
    "\n",
    "vaso_mean = df_train['vaso_input'].mean()\n",
    "vaso_std = df_train['vaso_input'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['iv_input_norm'] = (df_train['iv_input']- iv_mean)/iv_std\n",
    "df_train['vaso_input_norm'] = (df_train['vaso_input'] - vaso_mean)/vaso_std\n",
    "\n",
    "df_val['iv_input_norm'] = (df_val['iv_input']- iv_mean)/iv_std\n",
    "df_val['vaso_input_norm'] = (df_val['vaso_input'] - vaso_mean)/vaso_std\n",
    "\n",
    "df_test['iv_input_norm'] = (df_test['iv_input']- iv_mean)/iv_std\n",
    "df_test['vaso_input_norm'] = (df_test['vaso_input'] - vaso_mean)/vaso_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  X: (s_t-3, a_t-3, s_t-2, a_t-2,s_t-1, a_t-1, s_t, a_t )\n",
    "#  Y: (difference between next state and current state (zeros if end of trajectory), mortality)\n",
    "hist = 3\n",
    "def make_data_history_zeros(df_in):\n",
    "    df_in = df_in.reset_index()\n",
    "    X = []\n",
    "    Y = []\n",
    "    count_in_traj = 0\n",
    "    for count,i in enumerate(df_in.index):\n",
    "        if count % 10000 == 0 and count > 0:\n",
    "            print (count)\n",
    "        \n",
    "        # skip the last one; no next state\n",
    "        if i == df_in.index[-1]:\n",
    "            break\n",
    "       \n",
    "        # if not terminal step in trajectory    \n",
    "        if df_in.loc[i, 'icustayid'] == df_in.loc[i+1, 'icustayid']:\n",
    "            count_in_traj += 1\n",
    "            target = df_in.loc[i + 1, target_feat] - df_in.loc[i, target_feat]\n",
    "            target[-1] = df_in.loc[i, 'died_in_hosp']\n",
    "            Y.append(target)\n",
    "            if count_in_traj >=(hist+1):\n",
    "                state = df_in.loc[i-hist, cur_feat]\n",
    "                for index in range(hist-1,-1,-1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, cur_feat]])\n",
    "                X.append(state)\n",
    "            else:\n",
    "                num_zeros = (hist+1) - count_in_traj\n",
    "                num_actual = count_in_traj - 1\n",
    "                state = np.hstack([np.zeros(len(cur_feat)) for _ in range(num_zeros)])\n",
    "                for index in range(num_actual, 0, -1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, cur_feat]])\n",
    "                state = np.hstack([state, df_in.loc[i, cur_feat]])\n",
    "                X.append(state) \n",
    "        else:\n",
    "            count_in_traj = 0\n",
    "\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  X: (s_t-3, a_t-3, s_t-2, a_t-2,s_t-1, a_t-1, s_t, a_t )\n",
    "#  Y: (difference between next state and current state (zeros if end of trajectory), mortality)\n",
    "hist = 3\n",
    "def make_data_history(df_in):\n",
    "    df_in = df_in.reset_index()\n",
    "    X = []\n",
    "    Y = []\n",
    "    count_in_traj = 0\n",
    "    for count,i in enumerate(df_in.index):\n",
    "        if count % 10000 == 0 and count > 0:\n",
    "            print (count)\n",
    "        \n",
    "        # skip the last one; no next state\n",
    "        if i == df_in.index[-1]:\n",
    "            break\n",
    "       \n",
    "        # if not terminal step in trajectory    \n",
    "        if df_in.loc[i, 'icustayid'] == df_in.loc[i+1, 'icustayid']:\n",
    "            count_in_traj += 1\n",
    "            if count_in_traj >=(hist+1):\n",
    "\n",
    "                target = df_in.loc[i + 1, target_feat] - df_in.loc[i, target_feat]\n",
    "                target[-1] = df_in.loc[i, 'died_in_hosp']\n",
    "                Y.append(target)\n",
    "                state = df_in.loc[i-hist, cur_feat]\n",
    "                for index in range(hist-1,-1,-1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, cur_feat]])\n",
    "                #state = np.hstack([df_in.loc[i-2, cur_feat], df_in.loc[i-1, cur_feat], df_in.loc[i, cur_feat]])\n",
    "                X.append(state)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            count_in_traj = 0\n",
    "\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  X: (states, actions)\n",
    "#  Y: (difference between next state and current state (zeros if end of trajectory), mortality)\n",
    "def make_data(df_in):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for count,i in enumerate(df_in.index):\n",
    "        cur_state = df_in.loc[i,cur_feat]\n",
    "        if i != df_in.index[-1]:\n",
    "            # if not terminal step in trajectory             \n",
    "            if df_in.loc[i, 'icustayid'] == df_in.loc[i+1, 'icustayid']:\n",
    "                target = df_in.loc[i + 1, target_feat] - df_in.loc[i, target_feat]\n",
    "                target[-1] = df_in.loc[i, 'died_in_hosp']\n",
    "                Y.append(target)\n",
    "                X.append(cur_state)\n",
    "        if count % 10000 == 0 and count > 0:\n",
    "            print (count)\n",
    "\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dire = 'converted_data/'\n",
    "# if not os.path.exists(dire):\n",
    "#     os.makedirs(dire)\n",
    "    \n",
    "# if not os.path.exists(dire + 'X_train_hist.txt'):\n",
    "#     x_train_hist, y_train_hist = make_data_history(df_train)\n",
    "#     np.savetxt(dire + 'X_train_hist.txt',x_train_hist,fmt='%5.4f')\n",
    "#     np.savetxt(dire + 'Y_train_hist.txt',y_train_hist,fmt='%5.4f')\n",
    "#     print \"Saved train\"\n",
    "# else:\n",
    "#     x_train_hist = np.loadtxt(dire + 'X_train_hist.txt')\n",
    "#     y_train_hist = np.loadtxt(dire + 'Y_train_hist.txt')\n",
    "#     print \"Loaded train\"\n",
    "\n",
    "# if not os.path.exists(dire + 'X_val_hist.txt'):\n",
    "#     x_val_hist,y_val_hist = make_data_history(df_val)\n",
    "#     np.savetxt(dire + 'X_val_hist.txt',x_val_hist,fmt='%5.4f')\n",
    "#     np.savetxt(dire + 'Y_val_hist.txt',y_val_hist,fmt='%5.4f')\n",
    "#     print \"Saved val\"\n",
    "# else:\n",
    "#     x_val_hist = np.loadtxt(dire + 'X_val_hist.txt')\n",
    "#     y_val_hist = np.loadtxt(dire + 'Y_val_hist.txt')\n",
    "#     print \"Loaded val\"\n",
    "    \n",
    "# if not os.path.exists(dire + 'X_test_hist.txt'):\n",
    "#     x_test_hist, y_test_hist = make_data_history(df_test)\n",
    "#     np.savetxt(dire + 'X_test_hist.txt',x_test_hist,fmt='%5.4f')\n",
    "#     np.savetxt(dire + 'Y_test_hist.txt',y_test_hist,fmt='%5.4f')\n",
    "#     print \"Saved test\"\n",
    "# else:\n",
    "#     x_test_hist = np.loadtxt(dire + 'X_test_hist.txt')\n",
    "#     y_test_hist = np.loadtxt(dire + 'Y_test_hist.txt')\n",
    "#     print \"Loaded test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dire = 'converted_data/'\n",
    "# if not os.path.exists(dire):\n",
    "#     os.makedirs(dire)\n",
    "    \n",
    "# if not os.path.exists(dire + 'X_train.txt'):\n",
    "#     x_train_nohist, y_train_nohist = make_data(df_train)\n",
    "#     np.savetxt(dire + 'X_train.txt',x_train_nohist,fmt='%5.4f')\n",
    "#     np.savetxt(dire + 'Y_train.txt',y_train_nohist,fmt='%5.4f')\n",
    "#     print \"Saved train\"\n",
    "# else:\n",
    "#     x_train_nohist = np.loadtxt(dire + 'X_train.txt')\n",
    "#     y_train_nohist = np.loadtxt(dire + 'Y_train.txt')\n",
    "#     print \"Loaded train\"\n",
    "\n",
    "# if not os.path.exists(dire + 'X_val.txt'):\n",
    "#     x_val_nohist,y_val_nohist = make_data(df_val)\n",
    "#     np.savetxt(dire + 'X_val.txt',x_val_nohist,fmt='%5.4f')\n",
    "#     np.savetxt(dire + 'Y_val.txt',y_val_nohist,fmt='%5.4f')\n",
    "#     print \"Saved val\"\n",
    "# else:\n",
    "#     x_val_nohist = np.loadtxt(dire + 'X_val.txt')\n",
    "#     y_val_nohist = np.loadtxt(dire + 'Y_val.txt')\n",
    "#     print \"Loaded val\"\n",
    "    \n",
    "# if not os.path.exists(dire + 'X_test.txt'):\n",
    "#     x_test_nohist, y_test_nohist = make_data(df_test)\n",
    "#     np.savetxt(dire + 'X_test.txt',x_test_nohist,fmt='%5.4f')\n",
    "#     np.savetxt(dire +    'Y_test.txt',y_test_nohist,fmt='%5.4f')\n",
    "#     print \"Saved test\"\n",
    "# else:\n",
    "#     x_test_nohist = np.loadtxt(dire + 'X_test.txt')\n",
    "#     y_test_nohist = np.loadtxt(dire + 'Y_test.txt')\n",
    "#     print \"Loaded test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "Saved train_zeros\n",
      "10000\n",
      "20000\n",
      "Saved val_zeros\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "Saved test_zeros\n"
     ]
    }
   ],
   "source": [
    "dire = 'converted_data/'\n",
    "if not os.path.exists(dire):\n",
    "    os.makedirs(dire)\n",
    "\n",
    "if not os.path.exists(dire + 'X_train_hist_zeros.txt'):\n",
    "    train_feat_zeros, train_labels_zeros = make_data_history_zeros(df_train)\n",
    "    np.savetxt(dire + 'X_train_hist_zeros.txt',train_feat_zeros,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_train_hist_zeros.txt',train_labels_zeros,fmt='%5.4f')\n",
    "    print (\"Saved train_zeros\")\n",
    "else:\n",
    "    train_feat_zeros = np.loadtxt(dire + 'X_train_hist_zeros.txt')\n",
    "    train_labels_zeros = np.loadtxt(dire + 'Y_train_hist_zeros.txt')\n",
    "    print (\"Loaded train_zeros\")\n",
    "\n",
    "if not os.path.exists(dire + 'X_val_hist_zeros.txt'):\n",
    "    val_feat_zeros, val_labels_zeros = make_data_history_zeros(df_val)\n",
    "    np.savetxt(dire + 'X_val_hist_zeros.txt',val_feat_zeros,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_val_hist_zeros.txt',val_labels_zeros,fmt='%5.4f')\n",
    "    print (\"Saved val_zeros\")\n",
    "else:\n",
    "    val_feat_zeros = np.loadtxt(dire + 'X_val_hist_zeros.txt')\n",
    "    val_labels_zeros = np.loadtxt(dire + 'Y_val_hist_zeros.txt')\n",
    "    print (\"Loaded val_zeros\")\n",
    "\n",
    "if not os.path.exists(dire + 'X_test_hist_zeros.txt'):\n",
    "    test_feat_zeros, test_labels_zeros = make_data_history_zeros(df_test)\n",
    "    np.savetxt(dire + 'X_test_hist_zeros.txt',test_feat_zeros,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_test_hist_zeros.txt',test_labels_zeros,fmt='%5.4f')\n",
    "    print (\"Saved test_zeros\")\n",
    "else:\n",
    "    test_feat_zeros = np.loadtxt(dire + 'X_test_hist_zeros.txt')\n",
    "    test_labels_zeros = np.loadtxt(dire + 'Y_test_hist_zeros.txt')\n",
    "    print (\"Loaded test_zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Gaussian noise to the state features. Commented out for now -- might need to put back in.\n",
    "# gaussian_shape = df.loc[:, state_features].values.shape\n",
    "# noise = np.random.normal(0, 0.03, gaussian_shape)\n",
    "# df.loc[:, state_features] += noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train_feat_zeros\n",
    "y_train = train_labels_zeros\n",
    "\n",
    "x_val = val_feat_zeros\n",
    "y_val = val_labels_zeros\n",
    "\n",
    "x_test = test_feat_zeros\n",
    "y_test = test_labels_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156967, 200) (156967, 43)\n",
      "(22549, 200) (22549, 43)\n",
      "(45036, 200) (45036, 43)\n"
     ]
    }
   ],
   "source": [
    "print (x_train.shape, y_train.shape)\n",
    "print (x_val.shape, y_val.shape)\n",
    "print (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_1_size = 128\n",
    "hidden_2_size = 32\n",
    "class EnvModel():\n",
    "    def __init__(self):\n",
    "        self.phase = tf.placeholder(tf.bool)\n",
    "        \n",
    "        self.input_size = x_train.shape[1]\n",
    "        \n",
    "#         self.target_size = 1 # for now, just predict sofa\n",
    "\n",
    "        self.target_size = y_train.shape[1]\n",
    "\n",
    "        self.cur_state = tf.placeholder(tf.float32, shape=[None, self.input_size],name=\"cur_state\")\n",
    "        self.targets = tf.placeholder(tf.float32, shape=[None, self.target_size],name=\"target\")\n",
    "        \n",
    "        self.fc_1 = tf.contrib.layers.fully_connected(self.cur_state, hidden_1_size, activation_fn=tf.nn.relu)\n",
    "        self.fc_1_bn = tf.contrib.layers.batch_norm(self.fc_1, center=True, scale=True, is_training=self.phase)\n",
    "        self.fc_2 = tf.contrib.layers.fully_connected(self.fc_1_bn, hidden_2_size, activation_fn=tf.nn.relu)\n",
    "        self.fc_2_bn = tf.contrib.layers.batch_norm(self.fc_2, center=True, scale=True, is_training=self.phase)\n",
    "        \n",
    "        self.output = tf.contrib.layers.fully_connected(self.fc_2_bn, self.target_size, activation_fn = None)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.targets-self.output))\n",
    "        \n",
    "        #self.est_next_state = self.output + self.cur_state\n",
    "        \n",
    "        self.reg_lambda = 0.0000\n",
    "        \n",
    "        self.vars = tf.trainable_variables() \n",
    "        self.lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in self.vars\n",
    "                    if 'bias' not in v.name ]) * self.reg_lambda    \n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "        # Ensures that we execute the update_ops before performing the model update, so batchnorm works\n",
    "            self.update_model = self.trainer.minimize(self.loss + self.lossL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # Don't use all GPUs \n",
    "config.allow_soft_placement = True  # Enable manual control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_batch = 64\n",
    "def run_eval(sess, env_mdl):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    net_loss = 0.0\n",
    "    while start < len(x_val):\n",
    "        end = min(len(x_val), start+eval_batch)\n",
    "        x_batch = x_val[start:end]\n",
    "        y_batch = y_val[start:end]\n",
    "\n",
    "        # extract out only sofa for target\n",
    "#         y_batch = y_batch[:, 29]\n",
    "#         y_batch = y_batch[:, None]\n",
    "\n",
    "        # train using the batch\n",
    "        loss = sess.run(env_model.loss,\n",
    "                        feed_dict={env_model.cur_state:x_batch,\n",
    "                                   env_model.targets:y_batch,\n",
    "                                   env_model.phase:False})\n",
    "        # update net loss\n",
    "        net_loss += loss\n",
    "\n",
    "        # increment start index to inds\n",
    "        start += eval_batch\n",
    "    \n",
    "    avg_eval_loss = net_loss/(len(x_val)/float(eval_batch))\n",
    "    \n",
    "    return avg_eval_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running default init\n",
      "Init done\n",
      "Saved Model, epoch 1\n",
      "Average training loss per batch is  0.184884197271\n",
      "Avg val loss at epoch end is 0.15933036045\n",
      "Saved Model, epoch 2\n",
      "Average training loss per batch is  0.159051720528\n",
      "Avg val loss at epoch end is 0.153515678239\n",
      "Saved Model, epoch 3\n",
      "Average training loss per batch is  0.155490289914\n",
      "Avg val loss at epoch end is 0.151085885783\n",
      "Saved Model, epoch 4\n",
      "Average training loss per batch is  0.153782491525\n",
      "Avg val loss at epoch end is 0.150342578068\n",
      "Saved Model, epoch 5\n",
      "Average training loss per batch is  0.152663462748\n",
      "Avg val loss at epoch end is 0.14904760643\n",
      "Saved Model, epoch 6\n",
      "Average training loss per batch is  0.151809316994\n",
      "Avg val loss at epoch end is 0.148571506277\n",
      "Saved Model, epoch 7\n",
      "Average training loss per batch is  0.15113408074\n",
      "Avg val loss at epoch end is 0.148279145677\n",
      "Saved Model, epoch 8\n",
      "Average training loss per batch is  0.150748146016\n",
      "Avg val loss at epoch end is 0.14781003284\n",
      "Saved Model, epoch 9\n",
      "Average training loss per batch is  0.150238409421\n",
      "Avg val loss at epoch end is 0.147645594947\n",
      "Saved Model, epoch 10\n",
      "Average training loss per batch is  0.149874965319\n",
      "Avg val loss at epoch end is 0.147160790322\n",
      "Saved Model, epoch 11\n",
      "Average training loss per batch is  0.149552495177\n",
      "Avg val loss at epoch end is 0.14722631078\n",
      "Saved Model, epoch 12\n",
      "Average training loss per batch is  0.149273047541\n",
      "Avg val loss at epoch end is 0.147190171645\n",
      "Saved Model, epoch 13\n",
      "Average training loss per batch is  0.148978470403\n",
      "Avg val loss at epoch end is 0.147051616732\n",
      "Saved Model, epoch 14\n",
      "Average training loss per batch is  0.148720869355\n",
      "Avg val loss at epoch end is 0.146824619216\n",
      "Saved Model, epoch 15\n",
      "Average training loss per batch is  0.148478859204\n",
      "Avg val loss at epoch end is 0.146805052229\n",
      "Saved Model, epoch 16\n",
      "Average training loss per batch is  0.148344442361\n",
      "Avg val loss at epoch end is 0.1471944359\n",
      "Saved Model, epoch 17\n",
      "Average training loss per batch is  0.148116921619\n",
      "Avg val loss at epoch end is 0.146865033243\n",
      "Saved Model, epoch 18\n",
      "Average training loss per batch is  0.147984688558\n",
      "Avg val loss at epoch end is 0.147214070831\n",
      "Saved Model, epoch 19\n",
      "Average training loss per batch is  0.147795225407\n",
      "Avg val loss at epoch end is 0.146751392157\n",
      "Saved Model, epoch 20\n",
      "Average training loss per batch is  0.147660861613\n",
      "Avg val loss at epoch end is 0.146525391243\n",
      "Saved Model, epoch 21\n",
      "Average training loss per batch is  0.147486779387\n",
      "Avg val loss at epoch end is 0.146573826238\n",
      "Saved Model, epoch 22\n",
      "Average training loss per batch is  0.147266977422\n",
      "Avg val loss at epoch end is 0.146323408572\n",
      "Saved Model, epoch 23\n",
      "Average training loss per batch is  0.147205709052\n",
      "Avg val loss at epoch end is 0.146764931147\n",
      "Saved Model, epoch 24\n",
      "Average training loss per batch is  0.147054517012\n",
      "Avg val loss at epoch end is 0.146690035126\n",
      "Saved Model, epoch 25\n",
      "Average training loss per batch is  0.146900685047\n",
      "Avg val loss at epoch end is 0.147227022028\n",
      "Saved Model, epoch 26\n",
      "Average training loss per batch is  0.14683710765\n",
      "Avg val loss at epoch end is 0.146714674321\n",
      "Saved Model, epoch 27\n",
      "Average training loss per batch is  0.146798810679\n",
      "Avg val loss at epoch end is 0.146610213846\n",
      "Saved Model, epoch 28\n",
      "Average training loss per batch is  0.146524724075\n",
      "Avg val loss at epoch end is 0.146755643914\n",
      "Saved Model, epoch 29\n",
      "Average training loss per batch is  0.14654478189\n",
      "Avg val loss at epoch end is 0.146475310315\n",
      "Saved Model, epoch 30\n",
      "Average training loss per batch is  0.146379197209\n",
      "Avg val loss at epoch end is 0.146763454029\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 30\n",
    "load_model = False #Whether to load a saved model.\n",
    "save_dir = \"./env_model_reg/\"\n",
    "save_path = \"./env_model_reg/ckpt\"#The path to save our model to.\n",
    "tf.reset_default_graph()\n",
    "env_model = EnvModel()\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    if load_model == True:\n",
    "        print('Trying to load model...')\n",
    "        try:\n",
    "            restorer = tf.train.import_meta_graph(save_path + '.meta')\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "            print (\"Model restored\")\n",
    "        except IOError:\n",
    "            print (\"No previous model found, running default init\")\n",
    "            sess.run(init)\n",
    "    else:\n",
    "        print(\"Running default init\")\n",
    "        sess.run(init)\n",
    "    print(\"Init done\")\n",
    "    \n",
    "    for j in range(1,num_epochs+1):\n",
    "        net_loss = 0.0\n",
    "        inds = np.random.permutation(x_train.shape[0])\n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "        while start_idx < len(x_train):\n",
    "            end_idx = min(len(x_train), start_idx+batch_size)\n",
    "            batch_inds = inds[start_idx:end_idx]\n",
    "            x_batch = x_train[batch_inds]\n",
    "            y_batch = y_train[batch_inds]\n",
    "            \n",
    "            # extract out only sofa for target\n",
    "#             y_batch = y_batch[:, 29]\n",
    "#             y_batch = y_batch[:, None]\n",
    "            \n",
    "            # train using the batch\n",
    "            _,loss = sess.run([env_model.update_model,env_model.loss], \\\n",
    "            feed_dict={env_model.cur_state:x_batch,\n",
    "                       env_model.targets:y_batch,\n",
    "                       env_model.phase:True})\n",
    "            \n",
    "            # update net loss\n",
    "            net_loss += loss\n",
    "            \n",
    "            # increment start index to inds\n",
    "            start_idx += batch_size\n",
    "        \n",
    "        saver.save(sess,save_path)\n",
    "        print(\"Saved Model, epoch \" + str(j))\n",
    "        \n",
    "        av_loss = net_loss/(len(x_train)/float(batch_size))\n",
    "        print(\"Average training loss per batch is \", av_loss)\n",
    "        net_loss = 0.0\n",
    "        \n",
    "        eval_loss = run_eval(sess, env_model)\n",
    "        \n",
    "        print (\"Avg val loss at epoch end is \" + str(eval_loss))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load model...\n",
      "Model restored\n",
      "Init done\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    print('Trying to load model...')\n",
    "    try:\n",
    "        restorer = tf.train.import_meta_graph(save_path + '.meta')\n",
    "        restorer.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "        print \"Model restored\"\n",
    "    except IOError:\n",
    "        print \"No previous model found, running default init\"\n",
    "        sess.run(init)\n",
    "    print(\"Init done\")\n",
    "    \n",
    "    bx = x_train[50:100]\n",
    "    by = y_train[50:100, 29]\n",
    "    by = by[:, None]\n",
    "    op = sess.run(env_model.output, \\\n",
    "            feed_dict={env_model.cur_state:bx,\n",
    "                       env_model.targets:by,\n",
    "                       env_model.phase:False})\n",
    "\n",
    "# print op.shape, by.shape\n",
    "# for i in zip(np.squeeze(op), np.squeeze(by)):\n",
    "#     print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11017323824480833"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((np.squeeze(op)-np.squeeze(by))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.17911743, 0.0)\n",
      "(0.093046457, 0.0)\n",
      "(0.13156834, 0.28613931514300006)\n",
      "(0.075183168, 0.0)\n",
      "(0.15709624, 1.144557260572)\n",
      "(0.053191449, 0.28613931513999979)\n",
      "(0.007503923, -0.28613931513999979)\n",
      "(0.086839095, 0.0)\n",
      "(0.059295628, 0.0)\n",
      "(-0.0029163994, 0.0)\n",
      "(0.02531508, 0.28613931513999979)\n",
      "(-0.036283854, 0.0)\n",
      "(-0.039782193, 0.0)\n",
      "(-0.10121112, -0.57227863027999981)\n",
      "(-0.16417766, -0.28613931514799995)\n",
      "(0.13331677, 0.0)\n",
      "(0.053894948, 0.0)\n",
      "(0.21118657, 0.28613931514799995)\n",
      "(0.16026877, 0.0)\n",
      "(0.082047328, 0.28613931514000002)\n",
      "(-0.057933476, 0.0)\n",
      "(0.035227526, 0.28613931513999979)\n",
      "(-0.036803689, 0.0)\n",
      "(-0.12285101, -0.28613931513999979)\n",
      "(-0.14585981, 0.0)\n",
      "(-0.14774466, -0.28613931514000002)\n",
      "(-0.19879135, -0.28613931514799995)\n",
      "(-0.043731753, 0.0)\n",
      "(0.027128305, 0.0)\n",
      "(-0.092596322, -1.144557260572)\n",
      "(0.24658708, 0.0)\n",
      "(0.16986302, 0.0)\n",
      "(0.16923501, 0.0)\n",
      "(0.14478707, 0.0)\n",
      "(0.09138447, 0.0)\n",
      "(0.12362672, 0.0)\n",
      "(0.057311695, 0.0)\n",
      "(0.064386889, 0.5722786302859999)\n",
      "(-0.077138633, 1.144557260574)\n",
      "(-0.51417947, -0.28613931514000002)\n",
      "(-0.17385907, -0.28613931514300006)\n",
      "(-0.22510956, -0.28613931514300001)\n",
      "(-0.16911776, -0.2861393151428)\n",
      "(-0.17119114, -0.28613931514319996)\n",
      "(-0.42581651, -0.28613931514300006)\n",
      "(-0.41570267, 0.0)\n",
      "(-0.38291588, 0.0)\n",
      "(-0.32653114, -0.85841794542920002)\n",
      "(0.11460985, 0.0)\n",
      "(0.10880844, 0.0)\n"
     ]
    }
   ],
   "source": [
    "for i in zip(np.squeeze(op), np.squeeze(by)):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:my_root]",
   "language": "python",
   "name": "conda-env-my_root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
