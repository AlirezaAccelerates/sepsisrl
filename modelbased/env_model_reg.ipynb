{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import cPickle as pickle\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/rl_train_set_unscaled.csv')\n",
    "df_val = pd.read_csv('../data/rl_val_set_unscaled.csv')\n",
    "df_test = pd.read_csv('../data/rl_test_set_unscaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# let the mortality labels  be -1 and 1: -1 for survival\n",
    "df_train['died_in_hosp'][df_train['died_in_hosp'] == 0] = -1\n",
    "df_val['died_in_hosp'][df_val['died_in_hosp'] == 0] = -1\n",
    "df_test['died_in_hosp'][df_test['died_in_hosp'] == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_feat = list(np.loadtxt('../data/state_features_pred.txt', dtype=str))\n",
    "target_feat.append('died_in_hosp')\n",
    "\n",
    "cur_feat = list(np.loadtxt('../data/state_features.txt', dtype=str))\n",
    "cur_feat.append('iv_input')\n",
    "cur_feat.append('vaso_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define an action mapping - how to get an id representing the action from the (iv,vaso) tuple\n",
    "action_map = {}\n",
    "count = 0\n",
    "for iv in range(5):\n",
    "    for vaso in range(5):\n",
    "        action_map[(iv,vaso)] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_action_map = {}\n",
    "for iv in range(5):\n",
    "    for vaso in range(5):\n",
    "        inv_action_map[5*iv+vaso] = [iv,vaso]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# Change the actions: {0: -0.8, 1: -0.4, 2: 0, 3: 0.4, 4: 0.8}. This will help for the policy search, which\n",
    "# uses a continuous action space\n",
    "lookup ={0:-0.8, 1:-0.4, 2:0, 3:0.4, 4:0.8}\n",
    "for dfm in (df_train, df_val, df_test):\n",
    "    for iv_input in range(5):\n",
    "        val = lookup[iv_input]\n",
    "        dfm['iv_input'][dfm['iv_input'] == iv_input] = val\n",
    "    for vaso_input in range(5):\n",
    "        val = lookup[vaso_input]\n",
    "        dfm['vaso_input'][dfm['vaso_input'] == vaso_input] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 0, 1, 2, 3])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(5)\n",
    "b =  np.arange(4)\n",
    "np.hstack([a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  X: (s_t-2, a_t-2,s_t-1, a_t-1, s_t, a_t )\n",
    "#  Y: (difference between next state and current state (zeros if end of trajectory), mortality)\n",
    "def make_data_history(df_in):\n",
    "    df_in = df_in.reset_index()\n",
    "    X = []\n",
    "    Y = []\n",
    "    count_in_traj = 0\n",
    "    for count,i in enumerate(df_in.index):\n",
    "        \n",
    "        # skip the last one; no next state\n",
    "        if i == df_in.index[-1]:\n",
    "            break\n",
    "       \n",
    "        # if not terminal step in trajectory    \n",
    "        if df_in.loc[i, 'icustayid'] == df_in.loc[i+1, 'icustayid']:\n",
    "            count_in_traj += 1\n",
    "            if count_in_traj >=3:\n",
    "\n",
    "                target = df_in.loc[i + 1, target_feat] - df_in.loc[i, target_feat]\n",
    "                target[-1] = df_in.loc[i, 'died_in_hosp']\n",
    "                Y.append(target)\n",
    "                state = np.hstack([df_in.loc[i-2, cur_feat], df_in.loc[i-1, cur_feat], df_in.loc[i, cur_feat]])\n",
    "                X.append(state)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            count_in_traj = 0\n",
    "\n",
    "        if count % 10000 == 0 and count > 0:\n",
    "            print count\n",
    "\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  X: (states, actions)\n",
    "#  Y: (difference between next state and current state (zeros if end of trajectory), mortality)\n",
    "def make_data(df_in):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for count,i in enumerate(df_in.index):\n",
    "        cur_state = df_in.loc[i,cur_feat]\n",
    "        if i != df_in.index[-1]:\n",
    "            # if not terminal step in trajectory             \n",
    "            if df_in.loc[i, 'icustayid'] == df_in.loc[i+1, 'icustayid']:\n",
    "                target = df_in.loc[i + 1, target_feat] - df_in.loc[i, target_feat]\n",
    "                target[-1] = df_in.loc[i, 'died_in_hosp']\n",
    "                Y.append(target)\n",
    "                X.append(cur_state)\n",
    "#   #NOTE: Commented out the lines below because this adds weird training examples to the dataset. We don't\n",
    "#   # want to predicting zero change at the end of trajectories... Include all pts up to T-1\n",
    "#             else:\n",
    "#                 # trajectory is finished\n",
    "#                 target = np.zeros(len(target_feat))\n",
    "#                 target[-1] = df_in.loc[i, 'died_in_hosp']\n",
    "#         else:\n",
    "#             # last entry in df is the final state of that trajectory\n",
    "#             target = np.zeros(len(target_feat))\n",
    "#             target[-1] = df_in.loc[i, 'died_in_hosp']\n",
    "\n",
    "        if count % 10000 == 0 and count > 0:\n",
    "            print count\n",
    "\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "Saved train\n",
      "10000\n",
      "20000\n",
      "Saved val\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "Saved test\n"
     ]
    }
   ],
   "source": [
    "dire = 'converted_data/'\n",
    "if not os.path.exists(dire):\n",
    "    os.makedirs(dire)\n",
    "    \n",
    "if not os.path.exists(dire + 'X_train_hist.txt'):\n",
    "    x_train_hist, y_train_hist = make_data_history(df_train)\n",
    "    np.savetxt(dire + 'X_train_hist.txt',x_train_hist,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_train_hist.txt',y_train_hist,fmt='%5.4f')\n",
    "    print \"Saved train\"\n",
    "\n",
    "if not os.path.exists(dire + 'X_val_hist.txt'):\n",
    "    x_val_hist,y_val_hist = make_data_history(df_val)\n",
    "    np.savetxt(dire + 'X_val_hist.txt',x_val_hist,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_val_hist.txt',y_val_hist,fmt='%5.4f')\n",
    "    print \"Saved val\"\n",
    "    \n",
    "if not os.path.exists(dire + 'X_test_hist.txt'):\n",
    "    x_test_hist, y_test_hist = make_data_history(df_test)\n",
    "    np.savetxt(dire + 'X_test_hist.txt',x_test_hist,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_test_hist.txt',y_test_hist,fmt='%5.4f')\n",
    "    print \"Saved test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "Saved val\n"
     ]
    }
   ],
   "source": [
    "dire = 'converted_data/'\n",
    "if not os.path.exists(dire):\n",
    "    os.makedirs(dire)\n",
    "    \n",
    "if not os.path.exists(dire + 'X_train.txt'):\n",
    "    x_train_nohist, y_train_nohist = make_data(df_train)\n",
    "    np.savetxt(dire + 'X_train.txt',x_train_nohist,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_train.txt',y_train_nohist,fmt='%5.4f')\n",
    "    print \"Saved train\"\n",
    "\n",
    "if not os.path.exists(dire + 'X_val.txt'):\n",
    "    x_val_nohist,y_val_nohist = make_data(df_val)\n",
    "    np.savetxt(dire + 'X_val.txt',x_val_nohist,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_val.txt',y_val_nohist,fmt='%5.4f')\n",
    "    print \"Saved val\"\n",
    "    \n",
    "if not os.path.exists(dire + 'X_test.txt'):\n",
    "    x_test_nohist, y_test_nohist = make_data(df_test)\n",
    "    np.savetxt(dire + 'X_test.txt',x_test_nohist,fmt='%5.4f')\n",
    "    np.savetxt(dire +    'Y_test.txt',y_test_nohist,fmt='%5.4f')\n",
    "    print \"Saved test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Gaussian noise to the state features. Commented out for now -- might need to put back in.\n",
    "# gaussian_shape = df.loc[:, state_features].values.shape\n",
    "# noise = np.random.normal(0, 0.03, gaussian_shape)\n",
    "# df.loc[:, state_features] += noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train_hist\n",
    "y_train = y_train_hist\n",
    "\n",
    "x_val = x_val_hist\n",
    "y_val = y_val_hist\n",
    "\n",
    "x_test = x_test_hist\n",
    "y_test = y_test_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_1_size = 32\n",
    "hidden_2_size = 32\n",
    "class EnvModel():\n",
    "    def __init__(self):\n",
    "        self.phase = tf.placeholder(tf.bool)\n",
    "        \n",
    "        self.input_size = x_train.shape[1]\n",
    "        \n",
    "        self.target_size = 1 # for now, just predict sofa\n",
    "\n",
    "        self.cur_state = tf.placeholder(tf.float32, shape=[None, self.input_size],name=\"cur_state\")\n",
    "        self.targets = tf.placeholder(tf.float32, shape=[None, self.target_size],name=\"target\")\n",
    "        \n",
    "        self.fc_1 = tf.contrib.layers.fully_connected(self.cur_state, hidden_1_size, activation_fn=tf.nn.relu)\n",
    "        self.fc_1_bn = tf.contrib.layers.batch_norm(self.fc_1, center=True, scale=True, is_training=self.phase)\n",
    "        self.fc_2 = tf.contrib.layers.fully_connected(self.fc_1_bn, hidden_2_size, activation_fn=tf.nn.relu)\n",
    "        self.fc_2_bn = tf.contrib.layers.batch_norm(self.fc_2, center=True, scale=True, is_training=self.phase)\n",
    "        \n",
    "        self.output = tf.contrib.layers.fully_connected(self.fc_2_bn, self.target_size, activation_fn = None)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.targets-self.output)) \n",
    "        \n",
    "        self.est_next_state = self.output + self.cur_state\n",
    "        \n",
    "        self.reg_lambda = 0.000\n",
    "        \n",
    "        self.vars = tf.trainable_variables() \n",
    "        self.lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in self.vars\n",
    "                    if 'bias' not in v.name ]) * self.reg_lambda    \n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "        # Ensures that we execute the update_ops before performing the model update, so batchnorm works\n",
    "            self.update_model = self.trainer.minimize(self.loss + self.lossL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # Don't use all GPUs \n",
    "config.allow_soft_placement = True  # Enable manual control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_batch = 64\n",
    "def run_eval(sess, env_mdl):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    net_loss = 0.0\n",
    "    while start < len(x_val):\n",
    "        end = min(len(x_val), start+eval_batch)\n",
    "        x_batch = x_val[start:end]\n",
    "        y_batch = y_val[start:end]\n",
    "\n",
    "        # extract out only sofa for target\n",
    "        y_batch = y_batch[:, 29]\n",
    "        y_batch = y_batch[:, None]\n",
    "\n",
    "        # train using the batch\n",
    "        loss = sess.run(env_model.loss,\n",
    "                        feed_dict={env_model.cur_state:x_batch,\n",
    "                                   env_model.targets:y_batch,\n",
    "                                   env_model.phase:False})\n",
    "        # update net loss\n",
    "        net_loss += loss\n",
    "\n",
    "        # increment start index to inds\n",
    "        start += eval_batch\n",
    "    \n",
    "    avg_eval_loss = net_loss/(len(x_val)/float(eval_batch))\n",
    "    \n",
    "    return avg_eval_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running default init\n",
      "Init done\n",
      "Saved Model, epoch 1\n",
      "('Average training loss per batch is ', 0.26523895037921069)\n",
      "Avg val loss at epoch end is 0.215350268387\n",
      "Saved Model, epoch 2\n",
      "('Average training loss per batch is ', 0.20340633354757026)\n",
      "Avg val loss at epoch end is 0.198151301308\n",
      "Saved Model, epoch 3\n",
      "('Average training loss per batch is ', 0.19611293475820771)\n",
      "Avg val loss at epoch end is 0.196184077906\n",
      "Saved Model, epoch 4\n",
      "('Average training loss per batch is ', 0.19248868642049743)\n",
      "Avg val loss at epoch end is 0.193708232205\n",
      "Saved Model, epoch 5\n",
      "('Average training loss per batch is ', 0.1899638535510817)\n",
      "Avg val loss at epoch end is 0.189579004633\n",
      "Saved Model, epoch 6\n",
      "('Average training loss per batch is ', 0.18778610130310694)\n",
      "Avg val loss at epoch end is 0.189958125447\n",
      "Saved Model, epoch 7\n",
      "('Average training loss per batch is ', 0.1861304297970536)\n",
      "Avg val loss at epoch end is 0.187830319143\n",
      "Saved Model, epoch 8\n",
      "('Average training loss per batch is ', 0.18496513319186045)\n",
      "Avg val loss at epoch end is 0.187554683273\n",
      "Saved Model, epoch 9\n",
      "('Average training loss per batch is ', 0.18380428071606356)\n",
      "Avg val loss at epoch end is 0.185607345463\n",
      "Saved Model, epoch 10\n",
      "('Average training loss per batch is ', 0.18268138696139077)\n",
      "Avg val loss at epoch end is 0.187010818412\n",
      "Saved Model, epoch 11\n",
      "('Average training loss per batch is ', 0.18166028993140498)\n",
      "Avg val loss at epoch end is 0.185845743718\n",
      "Saved Model, epoch 12\n",
      "('Average training loss per batch is ', 0.18101653187958472)\n",
      "Avg val loss at epoch end is 0.184346607965\n",
      "Saved Model, epoch 13\n",
      "('Average training loss per batch is ', 0.18079796952028893)\n",
      "Avg val loss at epoch end is 0.184347934492\n",
      "Saved Model, epoch 14\n",
      "('Average training loss per batch is ', 0.18003939314053022)\n",
      "Avg val loss at epoch end is 0.18346580522\n",
      "Saved Model, epoch 15\n",
      "('Average training loss per batch is ', 0.1797262222343485)\n",
      "Avg val loss at epoch end is 0.184339426064\n",
      "Saved Model, epoch 16\n",
      "('Average training loss per batch is ', 0.17904304140707378)\n",
      "Avg val loss at epoch end is 0.18336035836\n",
      "Saved Model, epoch 17\n",
      "('Average training loss per batch is ', 0.17864321422313867)\n",
      "Avg val loss at epoch end is 0.184306508698\n",
      "Saved Model, epoch 18\n",
      "('Average training loss per batch is ', 0.17836000093696761)\n",
      "Avg val loss at epoch end is 0.186250354116\n",
      "Saved Model, epoch 19\n",
      "('Average training loss per batch is ', 0.17787600792618116)\n",
      "Avg val loss at epoch end is 0.183486508494\n",
      "Saved Model, epoch 20\n",
      "('Average training loss per batch is ', 0.17769185587588998)\n",
      "Avg val loss at epoch end is 0.183415788243\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "load_model = False #Whether to load a saved model.\n",
    "save_dir = \"./env_model_reg/\"\n",
    "save_path = \"./env_model_reg/ckpt\"#The path to save our model to.\n",
    "tf.reset_default_graph()\n",
    "env_model = EnvModel()\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    if load_model == True:\n",
    "        print('Trying to load model...')\n",
    "        try:\n",
    "            restorer = tf.train.import_meta_graph(save_path + '.meta')\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "            print \"Model restored\"\n",
    "        except IOError:\n",
    "            print \"No previous model found, running default init\"\n",
    "            sess.run(init)\n",
    "    else:\n",
    "        print(\"Running default init\")\n",
    "        sess.run(init)\n",
    "    print(\"Init done\")\n",
    "    \n",
    "    for j in range(1,num_epochs+1):\n",
    "        net_loss = 0.0\n",
    "        inds = np.random.permutation(x_train.shape[0])\n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "        while start_idx < len(x_train):\n",
    "            end_idx = min(len(x_train), start_idx+batch_size)\n",
    "            batch_inds = inds[start_idx:end_idx]\n",
    "            x_batch = x_train[batch_inds]\n",
    "            y_batch = y_train[batch_inds]\n",
    "            \n",
    "            # extract out only sofa for target\n",
    "            y_batch = y_batch[:, 29]\n",
    "            y_batch = y_batch[:, None]\n",
    "            \n",
    "            # train using the batch\n",
    "            _,loss = sess.run([env_model.update_model,env_model.loss], \\\n",
    "            feed_dict={env_model.cur_state:x_batch,\n",
    "                       env_model.targets:y_batch,\n",
    "                       env_model.phase:True})\n",
    "            \n",
    "            # update net loss\n",
    "            net_loss += loss\n",
    "            \n",
    "            # increment start index to inds\n",
    "            start_idx += batch_size\n",
    "        \n",
    "        saver.save(sess,save_path)\n",
    "        print(\"Saved Model, epoch \" + str(j))\n",
    "        \n",
    "        av_loss = net_loss/(len(x_train)/float(batch_size))\n",
    "        print(\"Average training loss per batch is \", av_loss)\n",
    "        net_loss = 0.0\n",
    "        \n",
    "        eval_loss = run_eval(sess, env_model)\n",
    "        \n",
    "        print (\"Avg val loss at epoch end is \" + str(eval_loss))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load model...\n",
      "Model restored\n",
      "Init done\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    print('Trying to load model...')\n",
    "    try:\n",
    "        restorer = tf.train.import_meta_graph(save_path + '.meta')\n",
    "        restorer.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "        print \"Model restored\"\n",
    "    except IOError:\n",
    "        print \"No previous model found, running default init\"\n",
    "        sess.run(init)\n",
    "    print(\"Init done\")\n",
    "    \n",
    "    bx = x_train[50:100]\n",
    "    by = y_train[50:100, 29]\n",
    "    by = by[:, None]\n",
    "    op = sess.run(env_model.output, \\\n",
    "            feed_dict={env_model.cur_state:bx,\n",
    "                       env_model.targets:by,\n",
    "                       env_model.phase:False})\n",
    "\n",
    "# print op.shape, by.shape\n",
    "# for i in zip(np.squeeze(op), np.squeeze(by)):\n",
    "#     print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11017323824480833"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((np.squeeze(op)-np.squeeze(by))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.17911743, 0.0)\n",
      "(0.093046457, 0.0)\n",
      "(0.13156834, 0.28613931514300006)\n",
      "(0.075183168, 0.0)\n",
      "(0.15709624, 1.144557260572)\n",
      "(0.053191449, 0.28613931513999979)\n",
      "(0.007503923, -0.28613931513999979)\n",
      "(0.086839095, 0.0)\n",
      "(0.059295628, 0.0)\n",
      "(-0.0029163994, 0.0)\n",
      "(0.02531508, 0.28613931513999979)\n",
      "(-0.036283854, 0.0)\n",
      "(-0.039782193, 0.0)\n",
      "(-0.10121112, -0.57227863027999981)\n",
      "(-0.16417766, -0.28613931514799995)\n",
      "(0.13331677, 0.0)\n",
      "(0.053894948, 0.0)\n",
      "(0.21118657, 0.28613931514799995)\n",
      "(0.16026877, 0.0)\n",
      "(0.082047328, 0.28613931514000002)\n",
      "(-0.057933476, 0.0)\n",
      "(0.035227526, 0.28613931513999979)\n",
      "(-0.036803689, 0.0)\n",
      "(-0.12285101, -0.28613931513999979)\n",
      "(-0.14585981, 0.0)\n",
      "(-0.14774466, -0.28613931514000002)\n",
      "(-0.19879135, -0.28613931514799995)\n",
      "(-0.043731753, 0.0)\n",
      "(0.027128305, 0.0)\n",
      "(-0.092596322, -1.144557260572)\n",
      "(0.24658708, 0.0)\n",
      "(0.16986302, 0.0)\n",
      "(0.16923501, 0.0)\n",
      "(0.14478707, 0.0)\n",
      "(0.09138447, 0.0)\n",
      "(0.12362672, 0.0)\n",
      "(0.057311695, 0.0)\n",
      "(0.064386889, 0.5722786302859999)\n",
      "(-0.077138633, 1.144557260574)\n",
      "(-0.51417947, -0.28613931514000002)\n",
      "(-0.17385907, -0.28613931514300006)\n",
      "(-0.22510956, -0.28613931514300001)\n",
      "(-0.16911776, -0.2861393151428)\n",
      "(-0.17119114, -0.28613931514319996)\n",
      "(-0.42581651, -0.28613931514300006)\n",
      "(-0.41570267, 0.0)\n",
      "(-0.38291588, 0.0)\n",
      "(-0.32653114, -0.85841794542920002)\n",
      "(0.11460985, 0.0)\n",
      "(0.10880844, 0.0)\n"
     ]
    }
   ],
   "source": [
    "for i in zip(np.squeeze(op), np.squeeze(by)):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
