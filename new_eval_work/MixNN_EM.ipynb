{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import cPickle as pickle\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albumin', 'Arterial_BE', 'Arterial_lactate', 'Arterial_pH', 'BUN', 'CO2_mEqL', 'Calcium', 'Chloride', 'Creatinine', 'DiaBP', 'FiO2_1', 'GCS', 'Glucose', 'HCO3', 'HR', 'Hb', 'INR', 'Ionised_Ca', 'Magnesium', 'MeanBP', 'PT', 'PTT', 'PaO2_FiO2', 'Platelets_count', 'Potassium', 'RR', 'SGOT', 'SGPT', 'SIRS', 'SOFA', 'Shock_Index', 'Sodium', 'SpO2', 'SysBP', 'Temp_C', 'Total_bili', 'WBC_count', 'Weight_kg', 'age', 'elixhauser', 'gender', 'mechvent', 'output_4hourly', 'output_total', 'paCO2', 'paO2', 're_admission', 'bloc']\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "with open('../data/state_features.txt') as f:\n",
    "    state_features = f.read().split()\n",
    "print (state_features)\n",
    "print len(state_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/rl_train_data_final_cont.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bloc</th>\n",
       "      <th>icustayid</th>\n",
       "      <th>charttime</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>elixhauser</th>\n",
       "      <th>re_admission</th>\n",
       "      <th>died_in_hosp</th>\n",
       "      <th>mortality_90d</th>\n",
       "      <th>Weight_kg</th>\n",
       "      <th>...</th>\n",
       "      <th>median_dose_vaso</th>\n",
       "      <th>max_dose_vaso</th>\n",
       "      <th>input_total_tev</th>\n",
       "      <th>input_4hourly_tev</th>\n",
       "      <th>output_total</th>\n",
       "      <th>output_4hourly</th>\n",
       "      <th>cumulated_balance_tev</th>\n",
       "      <th>vaso_input</th>\n",
       "      <th>iv_input</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>7245052800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.797351</td>\n",
       "      <td>0.939195</td>\n",
       "      <td>0.589916</td>\n",
       "      <td>0.750908</td>\n",
       "      <td>0.554500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.222560</td>\n",
       "      <td>3</td>\n",
       "      <td>7245067200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.831780</td>\n",
       "      <td>0.934543</td>\n",
       "      <td>0.674384</td>\n",
       "      <td>0.819589</td>\n",
       "      <td>0.580033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.657321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.356608</td>\n",
       "      <td>3</td>\n",
       "      <td>7245081600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833222</td>\n",
       "      <td>0.656575</td>\n",
       "      <td>0.765423</td>\n",
       "      <td>0.939329</td>\n",
       "      <td>0.555033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.367788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.452837</td>\n",
       "      <td>3</td>\n",
       "      <td>7245096000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.834033</td>\n",
       "      <td>0.603831</td>\n",
       "      <td>0.783597</td>\n",
       "      <td>0.847073</td>\n",
       "      <td>0.545700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.199099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.527957</td>\n",
       "      <td>3</td>\n",
       "      <td>7245110400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.834836</td>\n",
       "      <td>0.603831</td>\n",
       "      <td>0.794059</td>\n",
       "      <td>0.811583</td>\n",
       "      <td>0.539533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.057596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bloc  icustayid   charttime  gender       age  elixhauser  \\\n",
       "0  0.000000          3  7245052800     0.0  0.412568         0.0   \n",
       "1  0.222560          3  7245067200     0.0  0.412568         0.0   \n",
       "2  0.356608          3  7245081600     0.0  0.412568         0.0   \n",
       "3  0.452837          3  7245096000     0.0  0.412568         0.0   \n",
       "4  0.527957          3  7245110400     0.0  0.412568         0.0   \n",
       "\n",
       "   re_admission  died_in_hosp  mortality_90d  Weight_kg    ...     \\\n",
       "0           0.0             0              1   0.262712    ...      \n",
       "1           0.0             0              1   0.262712    ...      \n",
       "2           0.0             0              1   0.262712    ...      \n",
       "3           0.0             0              1   0.262712    ...      \n",
       "4           0.0             0              1   0.262712    ...      \n",
       "\n",
       "   median_dose_vaso  max_dose_vaso  input_total_tev  input_4hourly_tev  \\\n",
       "0               0.0            0.0         0.797351           0.939195   \n",
       "1               0.0            0.0         0.831780           0.934543   \n",
       "2               0.0            0.0         0.833222           0.656575   \n",
       "3               0.0            0.0         0.834033           0.603831   \n",
       "4               0.0            0.0         0.834836           0.603831   \n",
       "\n",
       "   output_total  output_4hourly  cumulated_balance_tev  vaso_input  iv_input  \\\n",
       "0      0.589916        0.750908               0.554500         0.0       4.0   \n",
       "1      0.674384        0.819589               0.580033         0.0       4.0   \n",
       "2      0.765423        0.939329               0.555033         0.0       2.0   \n",
       "3      0.783597        0.847073               0.545700         0.0       2.0   \n",
       "4      0.794059        0.811583               0.539533         0.0       2.0   \n",
       "\n",
       "     reward  \n",
       "0  0.125000  \n",
       "1  0.657321  \n",
       "2  1.367788  \n",
       "3  1.199099  \n",
       "4  1.057596  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('../data/rl_val_data_final_cont.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../data/rl_test_data_final_cont.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define an action mapping - how to get an id representing the action from the (iv,vaso) tuple\n",
    "action_map = {}\n",
    "count = 0\n",
    "for iv in range(5):\n",
    "    for vaso in range(5):\n",
    "        action_map[(iv,vaso)] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_action_map = {}\n",
    "for iv in range(5):\n",
    "    for vaso in range(5):\n",
    "        inv_action_map[5*iv+vaso] = [iv,vaso]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist_feat = list(np.loadtxt('../data/state_features.txt', dtype=str))\n",
    "hist_feat.append('iv_input')\n",
    "hist_feat.append('vaso_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  X: (s_t-3, a_t-3, s_t-2, a_t-2,s_t-1, a_t-1, s_t, a_t )\n",
    "#  Y: (difference between next state and current state (zeros if end of trajectory), mortality)\n",
    "hist = 3\n",
    "action_bins = 5\n",
    "def make_data_history(df_in):\n",
    "    df_in = df_in.reset_index()\n",
    "    X = []\n",
    "    Y = []\n",
    "    selected_indices = []\n",
    "    count_in_traj = 0\n",
    "    for count,i in enumerate(df_in.index):\n",
    "        if count % 10000 == 0 and count > 0:\n",
    "            print count\n",
    "        \n",
    "        # skip the last one; no next state\n",
    "        if i == df_in.index[-1]:\n",
    "            if count_in_traj >=(hist+1):\n",
    "                # the target is the action taken at this timestep\n",
    "                target_arr = df_in.loc[i, ['iv_input', 'vaso_input']].values\n",
    "                target = int(action_bins*target_arr[0] + target_arr[1])\n",
    "                \n",
    "                Y.append(target)\n",
    "                \n",
    "                # use hist_feat for old ones\n",
    "                state = df_in.loc[i-hist, hist_feat]\n",
    "                for index in range(hist-1,0,-1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, hist_feat]])\n",
    "                \n",
    "                # for current state, use state_features because we don't want to pass in the action!\n",
    "                state = np.hstack([state, df_in.loc[i, state_features]])\n",
    "                \n",
    "                X.append(state)\n",
    "                selected_indices.append(count)\n",
    "            break\n",
    "       \n",
    "        # if not terminal step in trajectory    \n",
    "        if df_in.loc[i, 'icustayid'] == df_in.loc[i+1, 'icustayid']:\n",
    "            count_in_traj += 1\n",
    "            if count_in_traj >=(hist+1):\n",
    "\n",
    "                # the target is the action taken at this timestep\n",
    "                target_arr = df_in.loc[i, ['iv_input', 'vaso_input']].values\n",
    "                target = int(action_bins*target_arr[0] + target_arr[1])\n",
    "                \n",
    "                Y.append(target)\n",
    "                \n",
    "                # use hist_feat for old ones\n",
    "                state = df_in.loc[i-hist, hist_feat]\n",
    "                for index in range(hist-1,0,-1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, hist_feat]])\n",
    "                \n",
    "                # for current state, use state_features because we don't want to pass in the action!\n",
    "                state = np.hstack([state, df_in.loc[i, state_features]])\n",
    "                \n",
    "                X.append(state)\n",
    "                selected_indices.append(count)\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        else:\n",
    "            if count_in_traj >=(hist+1):\n",
    "                # the target is the action taken at this timestep\n",
    "                target_arr = df_in.loc[i, ['iv_input', 'vaso_input']].values\n",
    "                target = int(action_bins*target_arr[0] + target_arr[1])\n",
    "                \n",
    "                Y.append(target)\n",
    "                \n",
    "                # use hist_feat for old ones\n",
    "                state = df_in.loc[i-hist, hist_feat]\n",
    "                for index in range(hist-1,0,-1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, hist_feat]])\n",
    "                \n",
    "                # for current state, use state_features because we don't want to pass in the action!\n",
    "                state = np.hstack([state, df_in.loc[i, state_features]])\n",
    "                \n",
    "                X.append(state)\n",
    "                selected_indices.append(count)\n",
    "                \n",
    "            count_in_traj = 0\n",
    "\n",
    "    return np.array(X),pd.get_dummies(np.array(Y)).values, np.array(selected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  X: (s_t-3, a_t-3, s_t-2, a_t-2,s_t-1, a_t-1, s_t )\n",
    "#  Y: (difference between next state and current state (zeros if end of trajectory), mortality)\n",
    "\n",
    "# this function pads with zeros (ie the mean) so that we still predict actions for all timesteps,\n",
    "# not just those with only 3 steps of history or more.\n",
    "hist = 3\n",
    "action_bins = 5\n",
    "def make_data_history_zeros(df_in):\n",
    "    df_in = df_in.reset_index()\n",
    "    X = []\n",
    "    Y = []\n",
    "    count_in_traj = 0\n",
    "    for count,i in enumerate(df_in.index):\n",
    "        if count % 10000 == 0 and count > 0:\n",
    "            print count\n",
    "        count_in_traj += 1\n",
    "        \n",
    "        # skip the last one; no next state\n",
    "        if i == df_in.index[-1]:\n",
    "            # the target is the action taken at this timestep\n",
    "            target_arr = df_in.loc[i, ['iv_input', 'vaso_input']].values\n",
    "            target = int(action_bins*target_arr[0] + target_arr[1])\n",
    "\n",
    "            Y.append(target)\n",
    "            \n",
    "            if count_in_traj >=(hist+1):                \n",
    "                # use hist_feat for old ones\n",
    "                state = df_in.loc[i-hist, hist_feat]\n",
    "                for index in range(hist-1,0,-1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, hist_feat]])\n",
    "                \n",
    "                # for current state, use state_features because we don't want to pass in the action!\n",
    "                state = np.hstack([state, df_in.loc[i, state_features]])\n",
    "                \n",
    "                X.append(state)\n",
    "            else:\n",
    "                num_zeros = (hist+1) - count_in_traj\n",
    "                num_actual = count_in_traj - 1\n",
    "                state = np.hstack([np.zeros(len(hist_feat)) for _ in range(num_zeros)])\n",
    "                for index in range(num_actual, 0, -1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, hist_feat]])\n",
    "                state = np.hstack([state, df_in.loc[i, state_features]])\n",
    "                X.append(state)             \n",
    "            # finish to avoid index error\n",
    "            break\n",
    "       \n",
    "        # if not terminal step in trajectory    \n",
    "        if df_in.loc[i, 'icustayid'] == df_in.loc[i+1, 'icustayid']:\n",
    "            # the target is the action taken at this timestep\n",
    "            target_arr = df_in.loc[i, ['iv_input', 'vaso_input']].values\n",
    "            target = int(action_bins*target_arr[0] + target_arr[1])\n",
    "\n",
    "            Y.append(target)\n",
    "            \n",
    "            if count_in_traj >=(hist+1):                \n",
    "                # use hist_feat for old ones\n",
    "                state = df_in.loc[i-hist, hist_feat]\n",
    "                for index in range(hist-1,0,-1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, hist_feat]])\n",
    "                \n",
    "                # for current state, use state_features because we don't want to pass in the action!\n",
    "                state = np.hstack([state, df_in.loc[i, state_features]])\n",
    "                \n",
    "                X.append(state)\n",
    "            else:\n",
    "                num_zeros = (hist+1) - count_in_traj\n",
    "                num_actual = count_in_traj - 1\n",
    "                state = np.hstack([np.zeros(len(hist_feat)) for _ in range(num_zeros)])\n",
    "                for index in range(num_actual, 0, -1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, hist_feat]])\n",
    "                state = np.hstack([state, df_in.loc[i, state_features]])\n",
    "                X.append(state)   \n",
    "        \n",
    "        else:\n",
    "            # the target is the action taken at this timestep\n",
    "            target_arr = df_in.loc[i, ['iv_input', 'vaso_input']].values\n",
    "            target = int(action_bins*target_arr[0] + target_arr[1])\n",
    "\n",
    "            Y.append(target)\n",
    "            \n",
    "            if count_in_traj >=(hist+1):    \n",
    "                # use hist_feat for old ones\n",
    "                state = df_in.loc[i-hist, hist_feat]\n",
    "                for index in range(hist-1,0,-1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, hist_feat]])\n",
    "                \n",
    "                # for current state, use state_features because we don't want to pass in the action!\n",
    "                state = np.hstack([state, df_in.loc[i, state_features]])\n",
    "                \n",
    "                X.append(state)\n",
    "            else:\n",
    "                num_zeros = (hist+1) - count_in_traj\n",
    "                num_actual = count_in_traj - 1\n",
    "                state = np.hstack([np.zeros(len(hist_feat)) for _ in range(num_zeros)])\n",
    "                for index in range(num_actual, 0, -1):\n",
    "                    state = np.hstack([state,df_in.loc[i-index, hist_feat]])\n",
    "                state = np.hstack([state, df_in.loc[i, state_features]])\n",
    "                X.append(state)\n",
    "                \n",
    "            #always reset the count\n",
    "            count_in_traj = 0\n",
    "\n",
    "    return np.array(X),pd.get_dummies(np.array(Y)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features (state vector) and labels (action taken) out of the dataframe for train \n",
    "# and val sets\n",
    "def preproc(df_in, iv_bins = 5):\n",
    "    df = df_in.copy()\n",
    "    actions_raw = df[['iv_input', 'vaso_input']].values\n",
    "    keep_arr = np.loadtxt('../data/state_features.txt', dtype=str)\n",
    "    df = df[keep_arr]\n",
    "    actions_proc = (iv_bins*actions_raw[:, 0] + actions_raw[:, 1]).astype(int)\n",
    "    hist = np.histogram(actions_proc, 25)\n",
    "    actions_proc = pd.get_dummies(actions_proc).values\n",
    "    #print(hist) just to check\n",
    "    return df.values, actions_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_sample(batch_size, features, labels):\n",
    "    idx = np.random.choice(np.arange(len(features)), batch_size)\n",
    "    return (np.vstack(features[idx]), np.vstack(labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "Saved train_zeros\n",
      "10000\n",
      "20000\n",
      "Saved val_zeros\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "Saved test_zeros\n"
     ]
    }
   ],
   "source": [
    "dire = 'mixnn_em_data/'\n",
    "if not os.path.exists(dire):\n",
    "    os.makedirs(dire)\n",
    "\n",
    "if not os.path.exists(dire + 'X_train_hist_zeros.txt'):\n",
    "    train_feat_zeros, train_labels_zeros = make_data_history_zeros(df)\n",
    "    np.savetxt(dire + 'X_train_hist_zeros.txt',train_feat_zeros,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_train_hist_zeros.txt',train_labels_zeros,fmt='%5.4f')\n",
    "    print \"Saved train_zeros\"\n",
    "else:\n",
    "    train_feat_zeros = np.loadtxt(dire + 'X_train_hist_zeros.txt')\n",
    "    train_labels_zeros = np.loadtxt(dire + 'Y_train_hist_zeros.txt')\n",
    "    print \"Loaded train_zeros\"\n",
    "\n",
    "if not os.path.exists(dire + 'X_val_hist_zeros.txt'):\n",
    "    val_feat_zeros, val_labels_zeros = make_data_history_zeros(val_df)\n",
    "    np.savetxt(dire + 'X_val_hist_zeros.txt',val_feat_zeros,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_val_hist_zeros.txt',val_labels_zeros,fmt='%5.4f')\n",
    "    print \"Saved val_zeros\"\n",
    "else:\n",
    "    val_feat_zeros = np.loadtxt(dire + 'X_val_hist_zeros.txt')\n",
    "    val_labels_zeros = np.loadtxt(dire + 'Y_val_hist_zeros.txt')\n",
    "    print \"Loaded val_zeros\"\n",
    "\n",
    "if not os.path.exists(dire + 'X_test_hist_zeros.txt'):\n",
    "    test_feat_zeros, test_labels_zeros = make_data_history_zeros(test_df)\n",
    "    np.savetxt(dire + 'X_test_hist_zeros.txt',test_feat_zeros,fmt='%5.4f')\n",
    "    np.savetxt(dire + 'Y_test_hist_zeros.txt',test_labels_zeros,fmt='%5.4f')\n",
    "    print \"Saved test_zeros\"\n",
    "else:\n",
    "    test_feat_zeros = np.loadtxt(dire + 'X_test_hist_zeros.txt')\n",
    "    test_labels_zeros = np.loadtxt(dire + 'Y_test_hist_zeros.txt')\n",
    "    print \"Loaded test_zeros\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train\n",
      "Loaded val\n",
      "Loaded test\n"
     ]
    }
   ],
   "source": [
    "use_history = True\n",
    "dire = 'mixnn_em_data/'\n",
    "if not os.path.exists(dire):\n",
    "    os.makedirs(dire)\n",
    "    \n",
    "if use_history:\n",
    "    if not os.path.exists(dire + 'X_train_hist.txt'):\n",
    "        train_feat, train_labels, train_indices = make_data_history(df)\n",
    "        np.savetxt(dire + 'X_train_hist.txt',train_feat,fmt='%5.4f')\n",
    "        np.savetxt(dire + 'Y_train_hist.txt',train_labels,fmt='%5.4f')\n",
    "        np.savetxt(dire + 'train_indices.txt', train_indices, fmt='%5.4f')\n",
    "        print \"Saved train\"\n",
    "    else:\n",
    "        train_feat = np.loadtxt(dire + 'X_train_hist.txt')\n",
    "        train_labels = np.loadtxt(dire + 'Y_train_hist.txt')\n",
    "        train_indices = np.loadtxt(dire + 'train_indices.txt')\n",
    "        print \"Loaded train\"\n",
    "\n",
    "    if not os.path.exists(dire + 'X_val_hist.txt'):\n",
    "        val_feat, val_labels, val_indices = make_data_history(val_df)\n",
    "        np.savetxt(dire + 'X_val_hist.txt',val_feat,fmt='%5.4f')\n",
    "        np.savetxt(dire + 'Y_val_hist.txt',val_labels,fmt='%5.4f')\n",
    "        np.savetxt(dire + 'val_indices.txt', val_indices, fmt='%5.4f')\n",
    "        print \"Saved val\"\n",
    "    else:\n",
    "        val_feat = np.loadtxt(dire + 'X_val_hist.txt')\n",
    "        val_labels = np.loadtxt(dire + 'Y_val_hist.txt')\n",
    "        val_indices = np.loadtxt(dire + 'val_indices.txt')\n",
    "        print \"Loaded val\"\n",
    "\n",
    "    if not os.path.exists(dire + 'X_test_hist.txt'):\n",
    "        test_feat, test_labels, test_indices = make_data_history(test_df)\n",
    "        np.savetxt(dire + 'X_test_hist.txt',test_feat,fmt='%5.4f')\n",
    "        np.savetxt(dire + 'Y_test_hist.txt',test_labels,fmt='%5.4f')\n",
    "        np.savetxt(dire + 'test_indices.txt', test_indices, fmt='%5.4f')\n",
    "        print \"Saved test\"\n",
    "    else:\n",
    "        test_feat = np.loadtxt(dire + 'X_test_hist.txt')\n",
    "        test_labels = np.loadtxt(dire + 'Y_test_hist.txt')\n",
    "        test_indices = np.loadtxt(dire + 'test_indices.txt')\n",
    "        print \"Loaded test\"\n",
    "else:\n",
    "    train_feat, train_labels = preproc(df)\n",
    "    val_feat, val_labels = preproc(val_df)\n",
    "    test_feat, test_labels = preproc(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feat = train_feat_zeros\n",
    "train_labels = train_labels_zeros\n",
    "\n",
    "val_feat = val_feat_zeros\n",
    "val_labels = val_labels_zeros\n",
    "\n",
    "test_feat = test_feat_zeros\n",
    "test_labels = test_labels_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169495, 198) (169495, 25)\n",
      "(24338, 198) (24338, 25)\n",
      "(48617, 198) (48617, 25)\n"
     ]
    }
   ],
   "source": [
    "print train_feat.shape, train_labels.shape\n",
    "print val_feat.shape, val_labels.shape\n",
    "print test_feat.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_length = train_feat.shape[1]\n",
    "batch_size = 64\n",
    "num_actions = 25\n",
    "num_mix_comp = 5\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MixNN():\n",
    "    def __init__(self):\n",
    "        self.states = tf.placeholder(tf.float32, shape = [None, feature_length])\n",
    "        \n",
    "        self.actions = tf.placeholder(tf.float32, shape = [None, num_actions])\n",
    "        \n",
    "        self.old_posterior = tf.placeholder(tf.float32, shape = [None, num_mix_comp])\n",
    "        \n",
    "        self.phase = tf.placeholder(tf.bool)\n",
    "        \n",
    "        self.states3d = tf.tile(tf.expand_dims(self.states, axis=2), [1, 1, num_mix_comp])\n",
    "        \n",
    "        self.actions3d = tf.tile(tf.expand_dims(self.actions, axis=2), [1, 1, num_mix_comp])\n",
    "        \n",
    "        # this first part computes p(z|s), of dimension (bs, num_mix_comps)\n",
    "        self.mixcomp_prob = self.mixcomp_net(self.states)\n",
    "\n",
    "        # this next part computes p(a|s,z)\n",
    "        self.mix_1 = tf.nn.relu(self.nn_mix_layer(self.states3d,feature_length,hidden_size, \"mix_layer1\"))\n",
    "        self.mix_2 = tf.nn.relu(self.nn_mix_layer(self.mix_1,hidden_size,hidden_size, \"mix_layer2\"))\n",
    "        self.mix_unnorm = self.nn_mix_layer(self.mix_1,hidden_size,num_actions, \"mix_layer3\")\n",
    "        self.mix_cond_prob = tf.nn.softmax(self.mix_unnorm, dim=1) # this is of dimension (bs,num_act, mix_comp)\n",
    "\n",
    "        \n",
    "        # generate p(a, z | s) for analysis, where a is ALL actions\n",
    "        #Firstly tile the mix probabilites. Currently it's of shape (bs, num_mix_comps). \n",
    "        #Convert to (bs, num_act, num_mix_comp)\n",
    "        self.expanded_mix_prob = tf.tile(tf.expand_dims(self.mixcomp_prob, axis=1), [1, num_actions,1])\n",
    "        \n",
    "        self.joint_prob = self.expanded_mix_prob * self.mix_cond_prob\n",
    "        \n",
    "        # generate p(a_vec|s) by summing out 2 dimension. a_vec is a vector of ALL actions.\n",
    "        # should be of dimension (bs,num_actions)\n",
    "        self.lik_as_vec = tf.reduce_sum(self.joint_prob, axis=2)\n",
    "        \n",
    "        # select the actions actually taken\n",
    "        self.vec = tf.tile(tf.expand_dims(self.actions, axis=2), [1,1,num_mix_comp])           \n",
    "        \n",
    "        # this is batchsize*num_actions*num_mix_comp. Sum out the num_actions dimension, which is zeroed out\n",
    "        # due to multiplication by one hot self.vec, to get tensor of shape bs*num_mix_comp\n",
    "        self.prob_actions_taken = tf.reduce_sum(self.vec*self.mix_cond_prob, axis=1) \n",
    "               \n",
    "        # form the joint prob term of components and actions actually taken\n",
    "        self.aux_joint = self.prob_actions_taken*self.mixcomp_prob\n",
    "        \n",
    "        # get the posterior for the current param setting: p(z|a,s), which is the E step of EM\n",
    "        self.post_cur_params = tf.divide(self.aux_joint, tf.expand_dims(tf.reduce_sum(self.aux_joint, axis=1),1))\n",
    "        \n",
    "        # FORM AUX FUNC FOR M STEP\n",
    "        # firstly get log aux joint\n",
    "        self.log_aux_joint = tf.log(self.aux_joint+ 1e-10)\n",
    "        \n",
    "        # multiply by OLD posterior, passed in as placeholder, and then reduce_sum\n",
    "        self.aux = tf.reduce_sum(self.log_aux_joint * self.old_posterior) /num_mix_comp\n",
    "        self.loss = -1*self.aux\n",
    "        \n",
    "        # also create one using current posterior as an example\n",
    "        self.loss_cur_post = -1 * tf.reduce_sum(self.log_aux_joint * self.post_cur_params) / num_mix_comp \n",
    "        \n",
    "        self.likelihood = tf.reduce_sum(self.aux_joint, axis=1)\n",
    "    \n",
    "        self.log_lik =  tf.reduce_sum(tf.log(self.likelihood))\n",
    "\n",
    "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "            self.train_step = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "    \n",
    "    def mixcomp_net(self, inp):\n",
    "        fc_1_mixcomp = tf.contrib.layers.fully_connected(inp, 128, activation_fn=tf.nn.relu)\n",
    "        bn_1_mixcomp = tf.contrib.layers.batch_norm(fc_1_mixcomp, center=True, scale=True, is_training=self.phase)\n",
    "        fc_2_mixcomp = tf.contrib.layers.fully_connected(bn_1_mixcomp , 128, activation_fn=tf.nn.relu)\n",
    "        bn_2_mixcomp = tf.contrib.layers.batch_norm(fc_2_mixcomp, center=True, scale=True, is_training=self.phase)\n",
    "        mixcomp_logit = tf.contrib.layers.fully_connected(bn_2_mixcomp , num_mix_comp, activation_fn=None)\n",
    "        return tf.nn.softmax(mixcomp_logit)\n",
    "\n",
    "    def nn_mix_layer(self, inp, d_in, d_out, scope):\n",
    "        # inp is of size (batchsize,d_in,num_mix)\n",
    "        # output produced is of size (batchsize, d_out, num_mix).\n",
    "        # no activation added, so this works for last layer too\n",
    "        W = tf.get_variable(str(scope) + \"Wmix\",shape = [d_in, d_out, num_mix_comp], \n",
    "                            initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable(str(scope)+\"bmix\", shape = [d_out, num_mix_comp])\n",
    "        # might need a tf.tile here for biases\n",
    "        processed_inp = tf.einsum(\"ilk,ljk->ijk\", inp, W) + b\n",
    "        return processed_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # Don't use all GPUs \n",
    "config.allow_soft_placement = True  # Enable manual control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_probs_for_analysis(dataset, sess, mdl):\n",
    "    # get p(a_vec| s) for each entry in dataset -- this sums over the mixture components\n",
    "    # get p(a_vec| s, z^(k)) for each entry in the dataset for each mix component -- 3D tensor\n",
    "    if dataset == 'train':\n",
    "        features, labels = train_feat,train_labels\n",
    "    elif dataset == 'val':\n",
    "        features, labels = val_feat,val_labels\n",
    "    elif dataset == 'test':\n",
    "        features, labels = test_feat,test_labels\n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    marginal_lik_vec = []\n",
    "    conditional_lik_vec = []\n",
    "    mixcomp_prob = []\n",
    "    \n",
    "    while (j < len(features)):\n",
    "        feat = None\n",
    "        lbls = None\n",
    "        if len(features) - j < batch_size:\n",
    "            feat = features[j:]\n",
    "            lbls = labels[j:]\n",
    "        else:\n",
    "            feat = features[j:j+batch_size]\n",
    "            lbls = labels[j:j+batch_size]\n",
    "        feat = feat.reshape(len(feat), feature_length)\n",
    "        acs = lbls.reshape(len(lbls), num_actions)\n",
    "        if j % 10000 == 0: \n",
    "            print('Processing indx: ', j )\n",
    "        \n",
    "        batch_marg_lik_vec, batch_cond_lik_vec, batch_mixcomp_prob = \\\n",
    "                                sess.run([mdl.lik_as_vec, mdl.mix_cond_prob, mdl.mixcomp_prob], \n",
    "                                          feed_dict={mdl.states : feat, mdl.actions: acs, mdl.phase: 0})\n",
    "        \n",
    "        marginal_lik_vec.append(batch_marg_lik_vec)\n",
    "        conditional_lik_vec.append(batch_cond_lik_vec)\n",
    "        mixcomp_prob.append(batch_mixcomp_prob)\n",
    "        \n",
    "        if len(features) - j < batch_size:\n",
    "            j = len(features)\n",
    "        else:\n",
    "            j += batch_size\n",
    "    \n",
    "    marginal_lik_vec = np.vstack([arr for arr in marginal_lik_vec])\n",
    "    conditional_lik_vec = np.vstack([arr for arr in conditional_lik_vec])\n",
    "    mixcomp_prob = np.vstack([arr for arr in mixcomp_prob])\n",
    "    return marginal_lik_vec, conditional_lik_vec, mixcomp_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_posterior(dataset, sess, mdl):\n",
    "    if dataset == 'train':\n",
    "        features, labels = train_feat,train_labels\n",
    "    elif dataset == 'val':\n",
    "        features, labels = val_feat,val_labels\n",
    "    elif dataset == 'test':\n",
    "        features, labels = test_feat,test_labels\n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    posterior = []\n",
    "    \n",
    "    while (j < len(features)):\n",
    "        feat = None\n",
    "        lbls = None\n",
    "        if len(features) - j < batch_size:\n",
    "            feat = features[j:]\n",
    "            lbls = labels[j:]\n",
    "        else:\n",
    "            feat = features[j:j+batch_size]\n",
    "            lbls = labels[j:j+batch_size]\n",
    "        feat = feat.reshape(len(feat), feature_length)\n",
    "        acs = lbls.reshape(len(lbls), num_actions)\n",
    "        if j % 10000 == 0: \n",
    "            print('Processing indx: ', j )\n",
    "        \n",
    "        post = sess.run(mdl.post_cur_params, \n",
    "                          feed_dict={mdl.states : feat, mdl.actions: acs, mdl.phase: 0})\n",
    "        \n",
    "        posterior.append(post)\n",
    "\n",
    "        if len(features) - j < batch_size:\n",
    "            j = len(features)\n",
    "        else:\n",
    "            j += batch_size\n",
    "    \n",
    "    posterior = np.vstack([arr for arr in posterior])\n",
    "\n",
    "    return posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Runs eval step.\n",
    "# Do we want to use current posterior or old posterior? cur is probably better...\n",
    "def run_eval(dataset,sess, mdl):\n",
    "    if dataset == 'train':\n",
    "        features, labels = train_feat,train_labels\n",
    "    elif dataset == 'val':\n",
    "        features, labels = val_feat,val_labels\n",
    "    elif dataset == 'test':\n",
    "        features, labels = test_feat,test_labels\n",
    "\n",
    "    total_loss = 0\n",
    "    total_log_lik = 0\n",
    "    j = 0\n",
    "    while (j < len(features)):\n",
    "        feat = None\n",
    "        lbls = None\n",
    "        if len(features) - j < batch_size:\n",
    "            feat = features[j:-1]\n",
    "            lbls = labels[j:-1]\n",
    "        else:\n",
    "            feat = features[j:j+batch_size]\n",
    "            lbls = labels[j:j+batch_size]\n",
    "        feat = feat.reshape(len(feat), feature_length)\n",
    "        acs = lbls.reshape(len(lbls), num_actions)\n",
    "        if j % 10000 == 0: \n",
    "            print('Processing val set indx: ', j )\n",
    "        loss, log_lik = sess.run([mdl.loss_cur_post, mdl.log_lik], \n",
    "                          feed_dict={mdl.states : feat, mdl.actions: acs, mdl.phase: 0})\n",
    "        total_loss += loss\n",
    "        total_log_lik += log_lik\n",
    "        if len(features) - j < batch_size:\n",
    "            j = len(features)\n",
    "        else:\n",
    "            j += batch_size\n",
    "    return total_loss, total_log_lik\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running default init\n",
      "Init done\n",
      "('Processing indx: ', 0)\n",
      "('Processing indx: ', 40000)\n",
      "('Processing indx: ', 80000)\n",
      "('Processing indx: ', 120000)\n",
      "('Processing indx: ', 160000)\n",
      "Average training loss per batch and loglik are 40.215041, -271119.982944 and E step is 0\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 38.029468, -35647.722462 and E step is 0\n",
      "Average training loss per batch and loglik are 37.699133, -237617.037292 and E step is 0\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 37.053789, -33770.177021 and E step is 0\n",
      "Average training loss per batch and loglik are 37.011223, -228809.151852 and E step is 0\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 36.628049, -32750.305145 and E step is 0\n",
      "Average training loss per batch and loglik are 36.630560, -223722.296700 and E step is 0\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 36.188618, -32071.475811 and E step is 0\n",
      "Average training loss per batch and loglik are 36.349317, -219864.817358 and E step is 0\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 35.922517, -31565.316431 and E step is 0\n",
      "('Processing indx: ', 0)\n",
      "('Processing indx: ', 40000)\n",
      "('Processing indx: ', 80000)\n",
      "('Processing indx: ', 120000)\n",
      "('Processing indx: ', 160000)\n",
      "Average training loss per batch and loglik are 35.712334, -215776.944336 and E step is 1\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 35.449607, -31203.483810 and E step is 1\n",
      "Average training loss per batch and loglik are 35.526434, -212982.508919 and E step is 1\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 35.295123, -30912.858370 and E step is 1\n",
      "Average training loss per batch and loglik are 35.412524, -211250.549543 and E step is 1\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 35.169367, -30431.402107 and E step is 1\n",
      "Average training loss per batch and loglik are 35.332012, -210027.473289 and E step is 1\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 34.989182, -30402.511818 and E step is 1\n",
      "Average training loss per batch and loglik are 35.252985, -208872.869070 and E step is 1\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 35.168673, -30512.847485 and E step is 1\n",
      "('Processing indx: ', 0)\n",
      "('Processing indx: ', 40000)\n",
      "('Processing indx: ', 80000)\n",
      "('Processing indx: ', 120000)\n",
      "('Processing indx: ', 160000)\n",
      "Average training loss per batch and loglik are 34.888473, -207416.732903 and E step is 2\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 34.687477, -29990.060787 and E step is 2\n",
      "Average training loss per batch and loglik are 34.806565, -206186.630192 and E step is 2\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 34.613811, -29893.679474 and E step is 2\n",
      "Average training loss per batch and loglik are 34.761744, -205498.341396 and E step is 2\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 34.793378, -29999.861374 and E step is 2\n",
      "Average training loss per batch and loglik are 34.720322, -204832.280666 and E step is 2\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 34.744225, -29844.027596 and E step is 2\n",
      "Average training loss per batch and loglik are 34.689291, -204345.248190 and E step is 2\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 34.522569, -29788.351147 and E step is 2\n",
      "('Processing indx: ', 0)\n",
      "('Processing indx: ', 40000)\n",
      "('Processing indx: ', 80000)\n",
      "('Processing indx: ', 120000)\n",
      "('Processing indx: ', 160000)\n",
      "Average training loss per batch and loglik are 34.331413, -203523.188450 and E step is 3\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 34.093250, -29616.500233 and E step is 3\n",
      "Average training loss per batch and loglik are 34.284700, -202779.354410 and E step is 3\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 34.202633, -29691.310564 and E step is 3\n",
      "Average training loss per batch and loglik are 34.254500, -202297.943825 and E step is 3\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 34.352509, -29586.654068 and E step is 3\n",
      "Average training loss per batch and loglik are 34.230306, -201944.220150 and E step is 3\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 34.136101, -29431.073673 and E step is 3\n",
      "Average training loss per batch and loglik are 34.206828, -201547.101254 and E step is 3\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 34.005885, -29393.639568 and E step is 3\n",
      "('Processing indx: ', 0)\n",
      "('Processing indx: ', 40000)\n",
      "('Processing indx: ', 80000)\n",
      "('Processing indx: ', 120000)\n",
      "('Processing indx: ', 160000)\n",
      "Average training loss per batch and loglik are 33.827103, -201064.889547 and E step is 4\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.665611, -29295.502876 and E step is 4\n",
      "Average training loss per batch and loglik are 33.782277, -200336.809677 and E step is 4\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.817611, -29450.847340 and E step is 4\n",
      "Average training loss per batch and loglik are 33.754222, -199922.923407 and E step is 4\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.908196, -29474.866013 and E step is 4\n",
      "Average training loss per batch and loglik are 33.739534, -199686.905006 and E step is 4\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.827452, -29287.106819 and E step is 4\n",
      "Average training loss per batch and loglik are 33.725204, -199399.283100 and E step is 4\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.895426, -29476.289196 and E step is 4\n",
      "('Processing indx: ', 0)\n",
      "('Processing indx: ', 40000)\n",
      "('Processing indx: ', 80000)\n",
      "('Processing indx: ', 120000)\n",
      "('Processing indx: ', 160000)\n",
      "Average training loss per batch and loglik are 33.517100, -198924.833187 and E step is 5\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.571742, -29347.935986 and E step is 5\n",
      "Average training loss per batch and loglik are 33.484958, -198439.600964 and E step is 5\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.473068, -29281.396305 and E step is 5\n",
      "Average training loss per batch and loglik are 33.464416, -198138.451315 and E step is 5\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.454901, -29040.927177 and E step is 5\n",
      "Average training loss per batch and loglik are 33.447014, -197859.302986 and E step is 5\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.344875, -29187.409618 and E step is 5\n",
      "Average training loss per batch and loglik are 33.430372, -197598.423672 and E step is 5\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.498180, -29202.646336 and E step is 5\n",
      "('Processing indx: ', 0)\n",
      "('Processing indx: ', 40000)\n",
      "('Processing indx: ', 80000)\n",
      "('Processing indx: ', 120000)\n",
      "('Processing indx: ', 160000)\n",
      "Average training loss per batch and loglik are 33.139219, -197325.281761 and E step is 6\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.368228, -29147.165707 and E step is 6\n",
      "Average training loss per batch and loglik are 33.108943, -196868.380711 and E step is 6\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.342903, -29231.003139 and E step is 6\n",
      "Average training loss per batch and loglik are 33.093975, -196601.028553 and E step is 6\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.273600, -29123.763603 and E step is 6\n",
      "Average training loss per batch and loglik are 33.075806, -196328.086130 and E step is 6\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.114868, -29041.593353 and E step is 6\n",
      "Average training loss per batch and loglik are 33.058630, -196075.954372 and E step is 6\n",
      "('Processing val set indx: ', 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg val loss and val loglik at epoch end are 33.250971, -29082.201439 and E step is 6\n",
      "('Processing indx: ', 0)\n",
      "('Processing indx: ', 40000)\n",
      "('Processing indx: ', 80000)\n",
      "('Processing indx: ', 120000)\n",
      "('Processing indx: ', 160000)\n",
      "Average training loss per batch and loglik are 32.838774, -195784.797380 and E step is 7\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.041856, -29083.277872 and E step is 7\n",
      "Average training loss per batch and loglik are 32.817958, -195473.019854 and E step is 7\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.037097, -29084.939278 and E step is 7\n",
      "Average training loss per batch and loglik are 32.796409, -195142.355051 and E step is 7\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 32.996942, -29044.203672 and E step is 7\n",
      "Average training loss per batch and loglik are 32.786659, -194932.043938 and E step is 7\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 32.959895, -29126.035458 and E step is 7\n",
      "Average training loss per batch and loglik are 32.774030, -194754.946648 and E step is 7\n",
      "('Processing val set indx: ', 0)\n",
      "Avg val loss and val loglik at epoch end are 33.008172, -29050.728466 and E step is 7\n",
      "('Processing indx: ', 0)\n",
      "('Processing indx: ', 0)\n",
      "('Processing indx: ', 40000)\n",
      "('Processing indx: ', 80000)\n",
      "('Processing indx: ', 120000)\n",
      "('Processing indx: ', 160000)\n",
      "('Processing indx: ', 0)\n",
      "('Processing indx: ', 40000)\n"
     ]
    }
   ],
   "source": [
    "# Idea when training:\n",
    "# Init posteriors (E step)\n",
    "# run M step for j epochs\n",
    "# re-estimate post (E)\n",
    "# run M step for j epochs\n",
    "# repeat until convergence on val set (?) \n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 5 # number of epochs to run M step for before re-estimating posteriors\n",
    "num_E_steps = 8 # how many times to re-estimate E\n",
    "load_model = False #Whether to load a saved model.\n",
    "save_dir = \"./MixNN_EM/\"\n",
    "save_path = \"./MixNN_EM/ckpt\"#The path to save our model to\n",
    "tf.reset_default_graph()\n",
    "mdl = MixNN()\n",
    "do_analysis = False\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    if load_model == True:\n",
    "        print('Trying to load model...')\n",
    "        try:\n",
    "            restorer = tf.train.import_meta_graph(save_path + '.meta')\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "            print \"Model restored\"\n",
    "        except IOError:\n",
    "            print \"No previous model found, running default init\"\n",
    "            sess.run(init)\n",
    "    else:\n",
    "        print(\"Running default init\")\n",
    "        sess.run(init)\n",
    "    print(\"Init done\")\n",
    "    \n",
    "\n",
    "    for e_step in range(num_E_steps):\n",
    "        if do_analysis:\n",
    "            print \"do analysis\"\n",
    "            marginal_lik, cond_lik, mixcomp_prob = get_probs_for_analysis('val', sess, mdl)\n",
    "            break\n",
    "        # start by initialising the posterior \n",
    "        posterior = get_posterior('train', sess, mdl)\n",
    "\n",
    "        for j in range(1,num_epochs+1):\n",
    "            net_loss = 0.0\n",
    "            net_loglik = 0.0\n",
    "            inds = np.random.permutation(train_feat.shape[0])\n",
    "            start_idx = 0\n",
    "            end_idx = 0\n",
    "            while start_idx < len(train_feat):\n",
    "                end_idx = min(len(train_feat), start_idx+batch_size)\n",
    "                batch_inds = inds[start_idx:end_idx]\n",
    "                x_batch = train_feat[batch_inds]\n",
    "                y_batch = train_labels[batch_inds]\n",
    "                post_batch = posterior[batch_inds]\n",
    "\n",
    "                # train using the batch\n",
    "                _,loss, batchtrain_loglik = sess.run([mdl.train_step,mdl.loss, mdl.log_lik], \\\n",
    "                feed_dict={mdl.states:x_batch,\n",
    "                           mdl.actions:y_batch,\n",
    "                           mdl.old_posterior:post_batch,\n",
    "                           mdl.phase:True})\n",
    "\n",
    "                # update net loss\n",
    "                net_loss += loss\n",
    "                \n",
    "                # update net loglik\n",
    "                net_loglik += batchtrain_loglik\n",
    "\n",
    "                # increment start index to inds\n",
    "                start_idx += batch_size\n",
    "\n",
    "#             saver.save(sess,save_path)\n",
    "#             print(\"Saved Model, epoch \" + str(j) + \" E step is \" + str(e_step))\n",
    "\n",
    "            av_loss = net_loss/(len(train_feat)/float(batch_size))\n",
    "            print(\"Average training loss per batch and loglik are %f, %f and E step is %d\"  % (av_loss, net_loglik, e_step))\n",
    "            net_loss = 0.0\n",
    "            net_loglik = 0.0\n",
    "\n",
    "            eval_loss, eval_loglik = run_eval(\"val\", sess, mdl)\n",
    "            \n",
    "            eval_loss = eval_loss/(len(val_feat)/float(batch_size))\n",
    "\n",
    "            print (\"Avg val loss and val loglik at epoch end are %f, %f and E step is %d\" % (eval_loss, eval_loglik, e_step))\n",
    "    \n",
    "    marginal_lik_val, cond_lik_val, mixcomp_prob_val = get_probs_for_analysis('val', sess, mdl)\n",
    "    marginal_lik_train, cond_lik_train, mixcomp_prob_train = get_probs_for_analysis('train', sess, mdl)\n",
    "    marginal_lik_test, cond_lik_test, mixcomp_prob_test = get_probs_for_analysis('test', sess, mdl)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24338, 25) (24338, 25, 5)\n",
      "(169495, 25) (169495, 25, 5)\n",
      "(48617, 25) (48617, 25, 5)\n"
     ]
    }
   ],
   "source": [
    "print marginal_lik_val.shape, cond_lik_val.shape\n",
    "print marginal_lik_train.shape, cond_lik_train.shape\n",
    "print marginal_lik_test.shape, cond_lik_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('./MixNN_EM/marginal_lik_val_dr.npy', marginal_lik_val)\n",
    "# np.save('./MixNN_EM/marginal_lik_train_dr.npy', marginal_lik_train)\n",
    "# np.save('./MixNN_EM/marginal_lik_test_dr.npy', marginal_lik_test)\n",
    "\n",
    "# np.save('./MixNN_EM/cond_lik_val_dr.npy', cond_lik_val)\n",
    "# np.save('./MixNN_EM/cond_lik_train_dr.npy', cond_lik_train)\n",
    "# np.save('./MixNN_EM/cond_lik_test_dr.npy',cond_lik_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('./MixNN_EM/marginal_lik_val.npy', marginal_lik)\n",
    "# np.save('./MixNN_EM/cond_lik_val.npy', cond_lik)\n",
    "# np.save('./MixNN_EM/mixcomp_prob.npy', mixcomp_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIRSTLY, ARGMAX PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the argmaxes for the first analysis\n",
    "actions_chosen = np.argmax(marginal_lik_val, axis=1)\n",
    "val_labels_asvec = np.argmax(val_labels, axis=1)\n",
    "mix_actions_chosen = np.argmax(cond_lik_val, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.604199194675\n"
     ]
    }
   ],
   "source": [
    "print sum(actions_chosen == val_labels_asvec)/float(len(val_labels_asvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24338, 25)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_comp_lik.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24338, 25)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_comp_lik.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6172.62290568\n",
      "-5932.4055236\n"
     ]
    }
   ],
   "source": [
    "# find where both are correct, where both are wrong\n",
    "# find what the probabilities are in both cases of the true classes\n",
    "# Add some of these up to understand what's been going wrong.\n",
    "correct_both = (val_labels_asvec == np.argmax(one_comp_lik, axis=1)) *  \\\n",
    "                    (val_labels_asvec == np.argmax(five_comp_lik, axis=1))\n",
    "one_comp_pred_both_correct = np.max(one_comp_lik, axis=1)[correct_both]\n",
    "five_comp_pred_both_correct = np.max(five_comp_lik, axis=1)[correct_both]\n",
    "print sum(np.log(one_comp_pred_both_correct))\n",
    "print sum(np.log(five_comp_pred_both_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong_both = (val_labels_asvec != np.argmax(one_comp_lik, axis=1)) *  \\\n",
    "                    (val_labels_asvec != np.argmax(five_comp_lik, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-21879.8962781\n",
      "-21802.6973631\n"
     ]
    }
   ],
   "source": [
    "one_comp_pred_both_wrong = one_comp_lik[range(len(one_comp_lik)), val_labels_asvec][wrong_both]\n",
    "five_comp_pred_both_wrong = five_comp_lik[range(len(one_comp_lik)), val_labels_asvec][wrong_both]\n",
    "print sum(np.log(one_comp_pred_both_wrong))\n",
    "print sum(np.log(five_comp_pred_both_wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find ones we got wrong with 1 comp and right with 5\n",
    "wrong_one_right_five = (val_labels_asvec != np.argmax(one_comp_lik, axis=1)) * \\\n",
    "                        (val_labels_asvec == np.argmax(five_comp_lik, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong_five_right_one = (val_labels_asvec == np.argmax(one_comp_lik, axis=1)) * \\\n",
    "                        (val_labels_asvec != np.argmax(five_comp_lik, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(wrong_five_right_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "637"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(wrong_one_right_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-881.509133399\n",
      "-619.70103617\n"
     ]
    }
   ],
   "source": [
    "one_comp_pred_w1r5 = one_comp_lik[range(len(one_comp_lik)), val_labels_asvec][wrong_one_right_five]\n",
    "five_comp_pred_w1r5 = five_comp_lik[range(len(one_comp_lik)), val_labels_asvec][wrong_one_right_five]\n",
    "print sum(np.log(one_comp_pred_w1r5))\n",
    "print sum(np.log(five_comp_pred_w1r5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-540.522264093\n",
      "-699.437117279\n"
     ]
    }
   ],
   "source": [
    "one_comp_pred_w5r1 = one_comp_lik[range(len(one_comp_lik)), val_labels_asvec][wrong_five_right_one]\n",
    "five_comp_pred_w5r1 = five_comp_lik[range(len(one_comp_lik)), val_labels_asvec][wrong_five_right_one]\n",
    "print sum(np.log(one_comp_pred_w5r1))\n",
    "print sum(np.log(five_comp_pred_w5r1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "240 + 77 + 270 - 159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3f85bab190>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmEPAzCxj/w9BXLBS9HVp7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f403c6df1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(actions_chosen).hist(bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4074629650>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkKSKGQKSVDFDQJIqZghIUsX+H5SXK6aGobe+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3fecf05dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(val_labels_asvec).hist(bins=25)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:my_root]",
   "language": "python",
   "name": "conda-env-my_root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
