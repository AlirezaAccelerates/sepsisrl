{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn the physician policy - ie, pi(a|s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/rl_train_data_final_cont.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_data = pd.read_csv('../data/rl_val_data_final_cont.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/rl_test_data_final_cont.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features (state vector) and labels (action taken) out of the dataframe for train \n",
    "# and val sets\n",
    "def preproc(df_in, iv_bins = 5):\n",
    "    df = df_in.copy()\n",
    "    actions_raw = df[['iv_input', 'vaso_input']].values\n",
    "    keep_arr = np.loadtxt('../data/state_features.txt', dtype=str)\n",
    "    df = df[keep_arr]\n",
    "    actions_proc = (iv_bins*actions_raw[:, 0] + actions_raw[:, 1]).astype(int)\n",
    "    hist = np.histogram(actions_proc, 25)\n",
    "    actions_proc = pd.get_dummies(actions_proc).values\n",
    "    #print(hist) just to check\n",
    "    return df.values, actions_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_sample(batch_size, features, labels):\n",
    "    idx = np.random.choice(np.arange(len(features)), batch_size)\n",
    "    return (np.vstack(features[idx]), np.vstack(labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feat, train_labels = preproc(train_data)\n",
    "val_feat, val_labels = preproc(val_data)\n",
    "test_feat, test_labels = preproc(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_length = len(train_feat[0])\n",
    "batch_size = 64\n",
    "num_actions = 25\n",
    "num_steps = 35000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# todo - reduce network size\n",
    "class PolicyModel():\n",
    "    def __init__(self):\n",
    "        self.input_feat = tf.placeholder(tf.float32, shape = [None, feature_length])\n",
    "        self.labels = tf.placeholder(tf.float32, shape = [None, num_actions])\n",
    "        self.phase = tf.placeholder(tf.bool)\n",
    "        \n",
    "        self.fc_1 = tf.contrib.layers.fully_connected(self.input_feat, 64, activation_fn=tf.nn.relu)\n",
    "        self.bn_1 = tf.contrib.layers.batch_norm(self.fc_1, center=True, scale=True, is_training=self.phase)\n",
    "#         self.fc_2 = tf.contrib.layers.fully_connected(self.bn_1 , 256, activation_fn=tf.nn.relu)    \n",
    "#         self.bn_2 = tf.contrib.layers.batch_norm(self.fc_2, center=True, scale=True, is_training=self.phase)\n",
    "#         self.fc_3 = tf.contrib.layers.fully_connected(self.bn_2 , 128, activation_fn=tf.nn.relu)\n",
    "#         self.bn_3 = tf.contrib.layers.batch_norm(self.fc_3, center=True, scale=True, is_training=self.phase)\n",
    "        self.fc_4 = tf.contrib.layers.fully_connected(self.bn_1 , 64, activation_fn=tf.nn.relu)\n",
    "        self.bn_4 = tf.contrib.layers.batch_norm(self.fc_4, center=True, scale=True, is_training=self.phase)\n",
    "        \n",
    "        self.logits = tf.contrib.layers.fully_connected(self.bn_4 , num_actions, activation_fn=None)\n",
    "        self.output = tf.nn.softmax(self.logits)\n",
    "        self.reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        self.reg_constant = 0.1 \n",
    "        \n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.labels, 1), tf.argmax(self.output, 1)),'float32'))\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.labels)) + self.reg_constant*sum(self.reg_losses)\n",
    "\n",
    "        \n",
    "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "            self.train_step = tf.train.AdamOptimizer().minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prints out accuracy on the relevant dataset and returns the policy. \n",
    "# This is the probability of taking each action in the action space from that state\n",
    "\n",
    "def get_policy(dataset,sess, mdl):\n",
    "\n",
    "    if dataset == 'train':\n",
    "        features, labels = train_feat,train_labels\n",
    "    elif dataset == 'val':\n",
    "        features, labels = val_feat,val_labels\n",
    "    elif dataset == 'test':\n",
    "        features, labels = test_feat,test_labels\n",
    "\n",
    "    \n",
    "    op = np.zeros((len(features), num_actions))\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    j = 0\n",
    "    while (j < len(features)):\n",
    "        feat = None\n",
    "        lbls = None\n",
    "        if len(features) - j < batch_size:\n",
    "            feat = features[j:-1]\n",
    "            lbls = labels[j:-1]\n",
    "        else:\n",
    "            feat = features[j:j+batch_size]\n",
    "            lbls = labels[j:j+batch_size]\n",
    "        feat = feat.reshape(len(feat), feature_length)\n",
    "        lbls = lbls.reshape(len(lbls), num_actions)\n",
    "        if j%10000 == 0: print('Processing val set indx: ', j )\n",
    "        softmax, accuracy, loss = sess.run([mdl.output, mdl.accuracy, mdl.loss], feed_dict={mdl.input_feat : feat, mdl.phase: 0, mdl.labels: lbls, mdl.phase: 0})\n",
    "        total_acc += accuracy\n",
    "        op[j:j+len(feat)] = softmax\n",
    "        if len(features) - j < batch_size:\n",
    "            j = len(features)\n",
    "        else: j+=batch_size\n",
    "        final_acc = total_acc/(len(op)/batch_size)\n",
    "        total_loss += loss\n",
    "    return op, final_acc, total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    tf.reset_default_graph()\n",
    "    mdl = PolicyModel()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True  # Don't use all GPUs \n",
    "    config.allow_soft_placement = True  # Enable manual control\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init)\n",
    "        net_loss = 0\n",
    "        net_accuracy = 0.0\n",
    "        print('Starting training!')\n",
    "        for i in range(num_steps):\n",
    "            feat, labels = batch_sample(batch_size, train_feat, train_labels)\n",
    "            \n",
    "            _, loss, accuracy = sess.run([mdl.train_step, mdl.loss, mdl.accuracy], feed_dict={mdl.input_feat : feat, mdl.labels: labels, mdl.phase: 1})\n",
    "            \n",
    "            net_loss += loss\n",
    "            net_accuracy += accuracy\n",
    "            if i % 1000 == 0 and i > 0:\n",
    "                av_loss = net_loss/1000.0\n",
    "                av_accuracy = net_accuracy/1000.0\n",
    "                print(\"Step: \", i, \"Average loss is: \", av_loss, \"Average accuracy is: \", av_accuracy)\n",
    "                net_loss = 0.0\n",
    "                net_accuracy = 0.0\n",
    "            \n",
    "            if i % 5000 == 0:\n",
    "                print \"Test on validation set\"\n",
    "                _, val_acc, val_loss = get_policy('val', sess, mdl)\n",
    "                print('Val set accuracy, loss: ', val_acc, val_loss)\n",
    "                \n",
    "        # Commented out for now\n",
    "        # train_policy, train_acc = get_policy('train')\n",
    "        print \"Finished, getting final accuracy\"\n",
    "        val_policy, val_acc, val_loss = get_policy('val', sess, mdl)\n",
    "        test_policy, _, _ = get_policy('test',sess, mdl)\n",
    "    print('Val set accuracy, loss: ', val_acc, val_loss)\n",
    "    return val_policy, test_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training!\n",
      "Test on validation set\n",
      "('Processing val set indx: ', 0)\n",
      "('Val set accuracy, loss: ', 0.023705474934036939, 1319.7603905200958)\n",
      "('Step: ', 1000, 'Average loss is: ', 2.079121185898781, 'Average accuracy is: ', 0.31293749999999998)\n",
      "('Step: ', 2000, 'Average loss is: ', 1.7933464502096177, 'Average accuracy is: ', 0.34654687499999998)\n",
      "('Step: ', 3000, 'Average loss is: ', 1.7625894849300385, 'Average accuracy is: ', 0.34957812500000002)\n",
      "('Step: ', 4000, 'Average loss is: ', 1.7442519917488097, 'Average accuracy is: ', 0.35295312499999998)\n",
      "('Step: ', 5000, 'Average loss is: ', 1.7291056674718857, 'Average accuracy is: ', 0.35209374999999998)\n",
      "Test on validation set\n",
      "('Processing val set indx: ', 0)\n",
      "('Val set accuracy, loss: ', 0.32086906332453824, 688.08596253395081)\n",
      "('Step: ', 6000, 'Average loss is: ', 1.7169677896499633, 'Average accuracy is: ', 0.35626562499999997)\n",
      "('Step: ', 7000, 'Average loss is: ', 1.7096919462680817, 'Average accuracy is: ', 0.35770312500000001)\n",
      "('Step: ', 8000, 'Average loss is: ', 1.69608054625988, 'Average accuracy is: ', 0.36060937500000001)\n",
      "('Step: ', 9000, 'Average loss is: ', 1.684339723944664, 'Average accuracy is: ', 0.36264062499999999)\n",
      "('Step: ', 10000, 'Average loss is: ', 1.6858431251049042, 'Average accuracy is: ', 0.36509374999999999)\n",
      "Test on validation set\n",
      "('Processing val set indx: ', 0)\n",
      "('Val set accuracy, loss: ', 0.32923812664907653, 693.06652855873108)\n",
      "('Step: ', 11000, 'Average loss is: ', 1.684468565106392, 'Average accuracy is: ', 0.36401562500000001)\n",
      "('Step: ', 12000, 'Average loss is: ', 1.6725080299377442, 'Average accuracy is: ', 0.36534375000000002)\n",
      "('Step: ', 13000, 'Average loss is: ', 1.6653573782444, 'Average accuracy is: ', 0.36784375000000002)\n",
      "('Step: ', 14000, 'Average loss is: ', 1.6602136299610137, 'Average accuracy is: ', 0.37135937499999999)\n",
      "('Step: ', 15000, 'Average loss is: ', 1.6646560591459274, 'Average accuracy is: ', 0.36690624999999999)\n",
      "Test on validation set\n",
      "('Processing val set indx: ', 0)\n",
      "('Val set accuracy, loss: ', 0.34069920844327178, 672.19870185852051)\n",
      "('Step: ', 16000, 'Average loss is: ', 1.6643627343177796, 'Average accuracy is: ', 0.3676875)\n",
      "('Step: ', 17000, 'Average loss is: ', 1.6533527481555939, 'Average accuracy is: ', 0.36889062500000003)\n",
      "('Step: ', 18000, 'Average loss is: ', 1.6594105679988862, 'Average accuracy is: ', 0.36635937499999999)\n",
      "('Step: ', 19000, 'Average loss is: ', 1.6439675900936126, 'Average accuracy is: ', 0.372640625)\n",
      "('Step: ', 20000, 'Average loss is: ', 1.6463266831636429, 'Average accuracy is: ', 0.37151562500000002)\n",
      "Test on validation set\n",
      "('Processing val set indx: ', 0)\n",
      "('Val set accuracy, loss: ', 0.34379122691292874, 663.45966827869415)\n",
      "('Step: ', 21000, 'Average loss is: ', 1.6398443295955658, 'Average accuracy is: ', 0.37471874999999999)\n",
      "('Step: ', 22000, 'Average loss is: ', 1.6368386305570601, 'Average accuracy is: ', 0.37442187500000002)\n",
      "('Step: ', 23000, 'Average loss is: ', 1.6427768769264222, 'Average accuracy is: ', 0.37429687499999997)\n",
      "('Step: ', 24000, 'Average loss is: ', 1.645944659113884, 'Average accuracy is: ', 0.3715)\n",
      "('Step: ', 25000, 'Average loss is: ', 1.6391052230596543, 'Average accuracy is: ', 0.37332812500000001)\n",
      "Test on validation set\n",
      "('Processing val set indx: ', 0)\n",
      "('Val set accuracy, loss: ', 0.34511048812664907, 661.36934185028076)\n",
      "('Step: ', 26000, 'Average loss is: ', 1.6303553236722945, 'Average accuracy is: ', 0.37375000000000003)\n",
      "('Step: ', 27000, 'Average loss is: ', 1.6430981500148774, 'Average accuracy is: ', 0.372046875)\n",
      "('Step: ', 28000, 'Average loss is: ', 1.6295419147014618, 'Average accuracy is: ', 0.37573437500000001)\n",
      "('Step: ', 29000, 'Average loss is: ', 1.6336285096406937, 'Average accuracy is: ', 0.37426562499999999)\n",
      "('Step: ', 30000, 'Average loss is: ', 1.6329574494361878, 'Average accuracy is: ', 0.37257812499999998)\n",
      "Test on validation set\n",
      "('Processing val set indx: ', 0)\n",
      "('Val set accuracy, loss: ', 0.34688324538258575, 653.86261451244354)\n",
      "('Step: ', 31000, 'Average loss is: ', 1.6238314838409424, 'Average accuracy is: ', 0.37557812499999998)\n",
      "('Step: ', 32000, 'Average loss is: ', 1.6201629285812378, 'Average accuracy is: ', 0.37753124999999998)\n",
      "('Step: ', 33000, 'Average loss is: ', 1.6257036677598953, 'Average accuracy is: ', 0.37571874999999999)\n",
      "('Step: ', 34000, 'Average loss is: ', 1.6216657589673995, 'Average accuracy is: ', 0.37714062500000001)\n",
      "Finished, getting final accuracy\n",
      "('Processing val set indx: ', 0)\n",
      "('Processing val set indx: ', 0)\n",
      "('Processing val set indx: ', 40000)\n",
      "('Val set accuracy, loss: ', 0.34824373350923482, 655.85515803098679)\n"
     ]
    }
   ],
   "source": [
    "val_policy, test_policy = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  save the learned policy as a numpy array with the columns as icustayid, bloc, iv input, vaso input,\n",
    "#  action index (of 25), and probability distribution over actions ( this is 25 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_data = val_data[['icustayid', 'bloc', 'iv_input', 'vaso_input']].values\n",
    "val_actions = (5*val_data['iv_input'].values + val_data['vaso_input']).values.astype(int)\n",
    "val_pickle = np.concatenate((v_data, val_actions.reshape(len(val_actions), 1), val_policy), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_data = test_data[['icustayid', 'bloc', 'iv_input', 'vaso_input']].values\n",
    "test_actions = (5*test_data['iv_input'].values + test_data['vaso_input']).values.astype(int)\n",
    "test_pickle = np.concatenate((t_data, test_actions.reshape(len(test_actions), 1), test_policy), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r\"val_policy.p\", \"wb\") as f:\n",
    "    pickle.dump(val_pickle, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r\"test_policy.p\", \"wb\") as f:\n",
    "    pickle.dump(test_pickle, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
